{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SC1095', 'NACA64A410']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6, 7\"\n",
    "\n",
    "data_raw = pd.read_csv('./expData_more_20240320.csv')\n",
    "data_index = data_raw.columns[52:52+52].append(data_raw.columns[[104, 106, 105]])\n",
    "X = torch.Tensor(np.array(data_raw[data_index]))\n",
    "\n",
    "af = data_raw['af']\n",
    "\n",
    "# Split \n",
    "af_unique = np.unique(af)\n",
    "train_afu, test_afu = train_test_split(af_unique, test_size = .1, random_state = 1) # 0\n",
    "# Manual override\n",
    "test_afu = ['SC1095', 'NACA64A410']\n",
    "train_afu = np.delete(af_unique, np.argwhere(af_unique==test_afu[0]))\n",
    "train_afu = np.delete(train_afu, np.argwhere(train_afu==test_afu[1]))\n",
    "\n",
    "\n",
    "train_ind = data_raw['af'].isin(train_afu).values\n",
    "test_ind = data_raw['af'].isin(test_afu).values\n",
    "\n",
    "# X = data\n",
    "# X = X - X.min(0)[0]\n",
    "# X = 2 * (X / X.max(0)[0]) - 1\n",
    "\n",
    "y = torch.Tensor(data_raw['Cp'])*100\n",
    "y_mean = torch.mean(y)\n",
    "y_std = torch.std(y)\n",
    "# y = (y-torch.mean(y))/torch.std(y)\n",
    "noise = (torch.Tensor(data_raw['noise'])*100)**2 # \n",
    "train_x = X[train_ind]\n",
    "test_x = X[test_ind]\n",
    "train_y = y[train_ind]\n",
    "test_y = y[test_ind]\n",
    "train_af = data_raw['af'][train_ind]\n",
    "test_af = data_raw['af'][test_ind]\n",
    "train_noise = noise[train_ind]\n",
    "test_noise = noise[test_ind]\n",
    "\n",
    "# plt.plot(noise)\n",
    "# del X, data_raw\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "\n",
    "print(test_afu)\n",
    "\n",
    "n_devices = torch.cuda.device_count()\n",
    "output_device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "nn_dims = [1000, 1000, 500, 50, 8] # 5[1000, 500, 50, 6]\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, nn_dims[0]))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(nn_dims[0], nn_dims[1]))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(nn_dims[1], nn_dims[2]))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(nn_dims[2], nn_dims[3]))\n",
    "        self.add_module('relu4', torch.nn.ReLU())\n",
    "        self.add_module('linear5', torch.nn.Linear(nn_dims[3], nn_dims[4]))\n",
    "feature_extractor = LargeFeatureExtractor()\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            \n",
    "            grid_size = 10#gpytorch.utils.grid.choose_grid_size(train_x)\n",
    "            # self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            #     gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=nn_dims[3])),\n",
    "            #     num_dims=nn_dims[3], grid_size=grid_size\n",
    "            # )\n",
    "            \n",
    "            base_covar_module = gpytorch.kernels.MaternKernel(nu=2.5,ard_num_dims=nn_dims[-1])\n",
    "            self.covar_module = gpytorch.kernels.MultiDeviceKernel(\n",
    "                base_covar_module, device_ids=range(n_devices),\n",
    "                output_device=output_device)\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "            # This module will scale the NN features so that they're nice values\n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1.0, 1.0)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\"\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        \n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(train_noise)#gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ceccb9fc71454ba17b151238e2e914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cuda:1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m output \u001b[38;5;241m=\u001b[39m model(train_x)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Calc loss and backprop derivatives\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Check if current loss is the best so far <- not sure if working\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m best_loss:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:64\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[0;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m     63\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood(function_dist, \u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 64\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_other_terms(res, params)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gpytorch/distributions/multivariate_normal.py:193\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[1;32m    192\u001b[0m covar \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39mevaluate_kernel()\n\u001b[0;32m--> 193\u001b[0m inv_quad, logdet \u001b[38;5;241m=\u001b[39m \u001b[43mcovar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_quad_logdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m([inv_quad, logdet, diff\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)])\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/linear_operator/operators/_linear_operator.py:1748\u001b[0m, in \u001b[0;36mLinearOperator.inv_quad_logdet\u001b[0;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inv_quad_rhs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1746\u001b[0m     args \u001b[38;5;241m=\u001b[39m [inv_quad_rhs] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[0;32m-> 1748\u001b[0m preconditioner, precond_lt, logdet_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preconditioner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precond_lt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01midentity_linear_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IdentityLinearOperator\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/linear_operator/operators/added_diag_linear_operator.py:126\u001b[0m, in \u001b[0;36mAddedDiagLinearOperator._preconditioner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_q_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     max_iter \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mmax_preconditioner_size\u001b[38;5;241m.\u001b[39mvalue()\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_piv_chol_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_linear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivoted_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many(torch\u001b[38;5;241m.\u001b[39misnan(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_piv_chol_self))\u001b[38;5;241m.\u001b[39mitem():\n\u001b[1;32m    128\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    129\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaNs encountered in preconditioner computation. Attempting to continue without preconditioning.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    130\u001b[0m             NumericalWarning,\n\u001b[1;32m    131\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/linear_operator/operators/_linear_operator.py:1965\u001b[0m, in \u001b[0;36mLinearOperator.pivoted_cholesky\u001b[0;34m(self, rank, error_tol, return_pivots)\u001b[0m\n\u001b[1;32m   1944\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1945\u001b[0m \u001b[38;5;124;03mPerforms a partial pivoted Cholesky factorization of the (positive definite) LinearOperator.\u001b[39;00m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;124;03m:math:`\\mathbf L \\mathbf L^\\top = \\mathbf K`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;124;03m    https://www.sciencedirect.com/science/article/pii/S0168927411001814\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m func \u001b[38;5;241m=\u001b[39m PivotedCholesky\u001b[38;5;241m.\u001b[39mapply\n\u001b[0;32m-> 1965\u001b[0m res, pivots \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_tol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_pivots:\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res, pivots\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/linear_operator/functions/_pivoted_cholesky.py:78\u001b[0m, in \u001b[0;36mPivotedCholesky.forward\u001b[0;34m(ctx, representation_tree, max_iter, error_tol, *matrix_args)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Populater L[... m:, m] with L[..., m:, m] * L[..., m, m].sqrt()\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m matrix_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Get next row of the permuted matrix\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[43mapply_permutation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi_m\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_permutation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     79\u001b[0m     pi_i \u001b[38;5;241m=\u001b[39m permutation[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     81\u001b[0m     L_m_new \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, pi_i)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/linear_operator/utils/permutation.py:80\u001b[0m, in \u001b[0;36mapply_permutation\u001b[0;34m(matrix, left_permutation, right_permutation)\u001b[0m\n\u001b[1;32m     76\u001b[0m     right_permutation \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(matrix\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39mmatrix\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Apply permutations\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_dense(\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mmatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[43mleft_permutation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m            \u001b[49m\u001b[43mright_permutation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/linear_operator/operators/_linear_operator.py:2847\u001b[0m, in \u001b[0;36mLinearOperator.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[38;5;66;03m# Convert all indices into tensor indices\u001b[39;00m\n\u001b[1;32m   2842\u001b[0m (\n\u001b[1;32m   2843\u001b[0m     \u001b[38;5;241m*\u001b[39mnew_batch_indices,\n\u001b[1;32m   2844\u001b[0m     new_row_index,\n\u001b[1;32m   2845\u001b[0m     new_col_index,\n\u001b[1;32m   2846\u001b[0m ) \u001b[38;5;241m=\u001b[39m _convert_indices_to_tensors(\u001b[38;5;28mself\u001b[39m, flattened_orig_indices)\n\u001b[0;32m-> 2847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_row_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_col_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_batch_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2848\u001b[0m \u001b[38;5;66;03m# Now un-flatten tensor indices\u001b[39;00m\n\u001b[1;32m   2849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor_index_shape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Do we need to unflatten?\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/linear_operator/operators/cat_linear_operator.py:210\u001b[0m, in \u001b[0;36mCatLinearOperator._get_indices\u001b[0;34m(self, row_index, col_index, *batch_indices)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m linear_op_idx, sub_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(linear_op_indices, sub_indices):\n\u001b[1;32m    208\u001b[0m     sub_index[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_dim] \u001b[38;5;241m=\u001b[39m sub_index[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_dim] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_dim_cum_sizes[linear_op_idx]\n\u001b[0;32m--> 210\u001b[0m res_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    211\u001b[0m     linear_op\u001b[38;5;241m.\u001b[39m_get_indices(sub_index[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], sub_index[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m*\u001b[39msub_index[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m linear_op, sub_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(linear_ops, sub_indices)\n\u001b[1;32m    213\u001b[0m ]\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mview(target_shape)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/linear_operator/operators/cat_linear_operator.py:211\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m linear_op_idx, sub_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(linear_op_indices, sub_indices):\n\u001b[1;32m    208\u001b[0m     sub_index[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_dim] \u001b[38;5;241m=\u001b[39m sub_index[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_dim] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_dim_cum_sizes[linear_op_idx]\n\u001b[1;32m    210\u001b[0m res_list \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 211\u001b[0m     \u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msub_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m linear_op, sub_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(linear_ops, sub_indices)\n\u001b[1;32m    213\u001b[0m ]\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mview(target_shape)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/linear_operator/operators/dense_linear_operator.py:50\u001b[0m, in \u001b[0;36mDenseLinearOperator._get_indices\u001b[0;34m(self, row_index, col_index, *batch_indices)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_indices\u001b[39m(\u001b[38;5;28mself\u001b[39m, row_index: IndexType, col_index: IndexType, \u001b[38;5;241m*\u001b[39mbatch_indices: IndexType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Perform the __getitem__\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[0;31mRuntimeError\u001b[0m: indices should be either on cpu or on the same device as the indexed tensor (cuda:1)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FixedNoiseGaussianLikelihood(\n",
       "  (noise_covar): FixedGaussianNoise()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_iterations = 1000#*3\n",
    "import tqdm.notebook as tn \n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "best_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "print(best_loss)\n",
    "def train(): \n",
    "    global best_loss, best_model_state_dict\n",
    "    iterator = tn.tqdm(range(training_iterations))\n",
    "    for i in iterator:\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_y)\n",
    "        # Check if current loss is the best so far <- not sure if working\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model_state_dict = model.state_dict()\n",
    "        loss.backward()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "        optimizer.step()\n",
    "        \n",
    "%time train()\n",
    "\n",
    "if best_model_state_dict is not None:\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    \n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "\n",
    "def train(train_x,\n",
    "          train_y,\n",
    "          n_devices,\n",
    "          output_device,\n",
    "          checkpoint_size,\n",
    "          preconditioner_size,\n",
    "          n_training_iter,\n",
    "):\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(output_device)\n",
    "    model = ExactGPModel(train_x, train_y, likelihood, n_devices).to(output_device)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "    ], lr=1e-2) # \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "    with gpytorch.beta_features.checkpoint_kernel(checkpoint_size), \\\n",
    "         gpytorch.settings.max_preconditioner_size(preconditioner_size):\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            return loss\n",
    "\n",
    "        loss = closure()\n",
    "        loss.backward()\n",
    "\n",
    "        for i in range(n_training_iter):\n",
    "            options = {'closure': closure, 'current_loss': loss, 'max_ls': 10}\n",
    "            loss, _, _, _, _, _, _, fail = optimizer.step(options)\n",
    "\n",
    "            print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                i + 1, n_training_iter, loss.item(),\n",
    "                model.covar_module.module.base_kernel.lengthscale.item(),\n",
    "                model.likelihood.noise.item()\n",
    "            ))\n",
    "\n",
    "            if fail:\n",
    "                print('Convergence reached!')\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    preds_test = model(test_x)\n",
    "    # preds_train = model(train_x)\n",
    "\n",
    "# print('Train MAE: {}'.format(torch.mean(torch.abs(preds_train.mean - train_y))))\n",
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds_test.mean - test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.plot(preds_train.mean.cpu())\n",
    "# plt.plot(train_y.cpu())\n",
    "\n",
    "plt.figure(figsize=(30,6))\n",
    "preds_std = np.sqrt(np.diag(preds_test.covariance_matrix.cpu()))\n",
    "\n",
    "dispNum = 32\n",
    "import random\n",
    "random.seed(30)\n",
    "# 2 \n",
    "# test_af[dispInds] + \n",
    "\n",
    "# dispInds = random.sample(range(1, test_x.cpu().shape[0]), dispNum)\n",
    "dispInds1 = random.sample(range(0, test_x.cpu().shape[0]), int(dispNum/2))\n",
    "dispInds2 = random.sample(range(0, 65), int(dispNum/2))\n",
    "dispInds = np.concatenate((dispInds1, dispInds2))\n",
    "# caseStr = \n",
    "\n",
    "afStr = np.array(test_af)[dispInds].flatten() # \n",
    "locStr = np.array(np.round(test_x.cpu(),3))[dispInds, -1].astype(str).flatten()\n",
    "AStr = np.array(test_x.cpu())[dispInds, -2].astype(str).flatten()\n",
    "\n",
    "\n",
    "finalStr = afStr + '\\n A=' + AStr + '\\n x/c=' + locStr\n",
    "\n",
    "\n",
    "plt.errorbar(np.arange(len(test_y[dispInds]))-0.2, test_y.cpu()[dispInds], yerr = np.sqrt(test_noise[dispInds]), capsize=2, label='Experimental $C_p \\pm \\sigma$', fmt='.', color = 'b', alpha =0.5)\n",
    "plt.bar(np.arange(len(test_y[dispInds]))-0.2, test_y.cpu()[dispInds], color='dodgerblue', width = 0.4)\n",
    "plt.errorbar(np.arange(len(test_y[dispInds]))+0.2, preds_test.mean.cpu()[dispInds],  yerr = preds_std[dispInds], label='Predicted $C_p \\pm \\sigma$', fmt='r.')\n",
    "plt.bar(np.arange(len(test_y[dispInds]))+0.2, preds_test.mean.cpu()[dispInds], color='crimson', width = 0.4, alpha =0.5, capsize=2)\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(0, dispNum), finalStr, fontsize=8);\n",
    "plt.ylabel('$C_p$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = [(6.0, test_afu[1], 0.6), (2.0, test_afu[0], 0.3), (18.0, test_afu[0], 0.52), (-5.0, test_afu[1], 0.88)]#, (4.0, test_afu[1])]\n",
    "print(test_afu[0])\n",
    "\n",
    "\n",
    "for i in np.arange(0, len(plotter)):#[6,7,8,9]:\n",
    "    plt.figure()\n",
    "    targetA = plotter[i][0]\n",
    "    targetAF = plotter[i][1]\n",
    "    targetM = plotter[i][2]\n",
    "    \n",
    "    tempA = torch.where(test_x[:, -3].cpu() == targetA)[0].numpy()\n",
    "    tempAF = np.where(test_af.values == targetAF)\n",
    "    tempM = torch.where(test_x[:, -2].cpu() == targetM)[0].numpy()\n",
    "\n",
    "    ind_search = np.intersect1d(np.intersect1d(tempA, tempAF), tempM)\n",
    "\n",
    "    sample_airfoil_temp = np.tile(test_x[ind_search[0],:-2].cpu(), (300,1))\n",
    "    desired_xc = np.linspace(-1, 1, 300).reshape((300,1))\n",
    "    desired_M = np.ones((300,1))*targetM\n",
    "    sample_airfoil = torch.Tensor(np.hstack((sample_airfoil_temp, desired_M, desired_xc))).cuda()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "        sample_airfoil_pred = model(sample_airfoil)\n",
    "    \n",
    "    sample_airfoil_std = np.sqrt(np.diag(sample_airfoil_pred.covariance_matrix.cpu()))\n",
    "    # newDist = torch.distributions.multivariate_normal.MultivariateNormal(sample_airfoil_pred.mean.cpu(), sample_airfoil_pred.covariance_matrix.cpu() + torch.eye(sample_airfoil_pred.covariance_matrix.cpu().shape[0])*1e-5)\n",
    "    \n",
    "    plt.errorbar(np.abs(test_x[ind_search, -1].cpu()), test_y[ind_search].cpu() + torch.mean(y), yerr=2*np.sqrt(noise[ind_search]), fmt='k.', capsize=2, label='Experiment, $C_p \\pm 2\\sigma$')\n",
    "    \n",
    "    # plt.plot(np.abs(sample_airfoil[:, -1].cpu()), sample_airfoil_pred.sample(sample_shape=1) + torch.mean(y))\n",
    "    plt.fill_between(-desired_xc[:150].flatten(), sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) + sample_airfoil_std[:150], sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) - sample_airfoil_std[:150], color = 'r', alpha = 0.3)\n",
    "    plt.fill_between(desired_xc[150:].flatten(), sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) + sample_airfoil_std[150:], sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) - sample_airfoil_std[150:], color = 'r', alpha = 0.3)\n",
    "    plt.fill_between(-desired_xc[:150].flatten(), sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) + 2*sample_airfoil_std[:150], sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) - 2*sample_airfoil_std[:150], color = 'r', alpha = 0.2)\n",
    "    plt.fill_between(desired_xc[150:].flatten(), sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) + 2*sample_airfoil_std[150:], sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) - 2*sample_airfoil_std[150:], color = 'r', alpha = 0.2)\n",
    "    # for j in np.arange(0, 5):\n",
    "    #     plt.plot(np.abs(sample_airfoil[:, -1].cpu()), newDist.sample()+ torch.mean(y), alpha = 0.2, color = 'r')\n",
    "    plt.plot(np.abs(sample_airfoil[:, -1].cpu()), sample_airfoil_pred.mean.cpu() + torch.mean(y),'r', label='DKL GP, $C_p \\pm 2\\sigma$')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('x/c')\n",
    "    plt.ylabel('$C_p$')\n",
    "    plt.legend()\n",
    "    # plt.title(targetAF +'\\n' + r'$\\alpha$ = ' + str(targetA) + r'$^\\circ,$' + r' $M_\\infty = 0.60$')\n",
    "# test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = sample_airfoil_pred.mean.cpu()\n",
    "cov = sample_airfoil_pred.covariance_matrix.cpu()\n",
    "newDist = torch.distributions.multivariate_normal.MultivariateNormal(mu, cov + torch.eye(cov.shape[0])*1e-5)\n",
    "plt.figure()\n",
    "plt.plot(newDist.sample(),'-')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(sample_airfoil_pred.covariance_matrix.cpu())#.sample()\n",
    "plt.title('Learned covariance matrix')\n",
    "plt.xlabel('Projected X')\n",
    "plt.ylabel('Projected X\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent variable plotter 1 - Latent variables vs each other \n",
    "extracted_values = model.feature_extractor(train_x)\n",
    "extracted_values = model.scale_to_bounds(extracted_values)\n",
    "extracted_values = extracted_values.cpu().detach().numpy()\n",
    "\n",
    "N_var = nn_dims[3]\n",
    "f, ax = plt.subplots(N_var, N_var-1, figsize = (18, 18))\n",
    "for i in np.arange(0, N_var):\n",
    "    for j in np.arange(0, i):\n",
    "        for jj in np.arange(0, train_afu.shape[0]):\n",
    "            plot_ind = np.argwhere(train_af.values == train_afu[jj])\n",
    "            ax[i,j].scatter(extracted_values[plot_ind, i], extracted_values[plot_ind, j], marker='.',s=8)\n",
    "        if j == 0:\n",
    "            ax[i,j].set_ylabel('Variable ' + str(i+1), rotation=0, fontsize=12, labelpad = 30)\n",
    "        if i == N_var-1:\n",
    "            ax[i,j].set_xlabel('Variable ' + str(j+1), rotation=0, fontsize=12)\n",
    "        \n",
    "    for k in np.arange(i, N_var-1):\n",
    "        f.delaxes(ax[i,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent variable plotter 1 - Latent variables vs each other \n",
    "extracted_values = model.feature_extractor(train_x)\n",
    "extracted_values = model.scale_to_bounds(extracted_values)\n",
    "extracted_values = extracted_values.cpu().detach().numpy()\n",
    "\n",
    "N_var = nn_dims[3]\n",
    "f, ax = plt.subplots(N_var, N_var-1, figsize = (18, 18))\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(extracted_values[:, 0], extracted_values[:, 1], c=train_y.cpu(), marker='.', s=8)\n",
    "\n",
    "for i in np.arange(0, N_var):\n",
    "    for j in np.arange(0, i):\n",
    "        ff = ax[i,j].scatter(extracted_values[:, i], extracted_values[:, j], c=train_y.cpu(), marker='.', s=8)\n",
    "        \n",
    "        if j == 0:\n",
    "            ax[i,j].set_ylabel('Variable ' + str(i+1), rotation=0, fontsize=12, labelpad = 30)\n",
    "        if i == N_var-1:\n",
    "            ax[i,j].set_xlabel('Variable ' + str(j+1), rotation=0, fontsize=12)\n",
    "        \n",
    "    for k in np.arange(i, N_var-1):\n",
    "        f.delaxes(ax[i,k])\n",
    "        \n",
    "cbar = plt.colorbar(ff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_values = model.feature_extractor(train_x)\n",
    "extracted_values = model.scale_to_bounds(extracted_values)\n",
    "extracted_values = extracted_values.cpu().detach().numpy()\n",
    "\n",
    "N_var = nn_dims[3]\n",
    "f, ax = plt.subplots(N_var, N_var-1, figsize = (18, 18))\n",
    "for i in np.arange(0, N_var):\n",
    "    for j in np.arange(0, i):\n",
    "        ax[i,j].scatter(extracted_values[:, i], extracted_values[:, j], c=preds_train.mean.cpu(), marker='.', s=8)\n",
    "        \n",
    "        if j == 0:\n",
    "            ax[i,j].set_ylabel('Variable ' + str(i+1), rotation=0, fontsize=12, labelpad = 30)\n",
    "        if i == N_var-1:\n",
    "            ax[i,j].set_xlabel('Variable ' + str(j+1), rotation=0, fontsize=12)\n",
    "        \n",
    "    for k in np.arange(i, N_var-1):\n",
    "        f.delaxes(ax[i,k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'full_backup2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('full_backup'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = [(10.0, train_afu[0]), (9.1, train_afu[-2])]#, (2.0, train_afu[6])]\n",
    "print(train_afu)\n",
    "for i in np.arange(0, len(plotter)):#[6,7,8,9]:\n",
    "    plt.figure()\n",
    "    targetA = plotter[i][0]\n",
    "\n",
    "    targetAF = plotter[i][1]\n",
    "    tempA = torch.where(train_x[:, -2].cpu() == targetA)[0].numpy()\n",
    "    tempAF = np.where(train_af.values == targetAF)\n",
    "    ind_search = np.intersect1d(tempA, tempAF)\n",
    "\n",
    "    sample_airfoil_temp = np.tile(train_x[ind_search[0],:-1].cpu(), (300,1))\n",
    "    desired_xc = np.linspace(-1, 1, 300).reshape((300,1))\n",
    "    sample_airfoil = torch.Tensor(np.hstack((sample_airfoil_temp, desired_xc))).cuda()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "        sample_airfoil_pred = model(sample_airfoil)\n",
    "    \n",
    "    sample_airfoil_std = np.sqrt(np.diag(sample_airfoil_pred.covariance_matrix.cpu()))\n",
    "    newDist = torch.distributions.multivariate_normal.MultivariateNormal(sample_airfoil_pred.mean.cpu(), sample_airfoil_pred.covariance_matrix.cpu() + torch.eye(sample_airfoil_pred.covariance_matrix.cpu().shape[0])*1e-5)\n",
    "    \n",
    "    plt.errorbar(np.abs(train_x[ind_search, -1].cpu()), train_y[ind_search].cpu() + torch.mean(y), yerr=2*np.sqrt(train_noise[ind_search]), fmt='k.', capsize=2, label='Experiment, $C_p \\pm 2\\sigma$')\n",
    "    \n",
    "    # plt.plot(np.abs(sample_airfoil[:, -1].cpu()), sample_airfoil_pred.sample(sample_shape=1) + torch.mean(y))\n",
    "    plt.fill_between(-desired_xc[:150].flatten(), sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) + sample_airfoil_std[:150], sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) - sample_airfoil_std[:150], color = 'r', alpha = 0.3)\n",
    "    plt.fill_between(desired_xc[150:].flatten(), sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) + sample_airfoil_std[150:], sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) - sample_airfoil_std[150:], color = 'r', alpha = 0.3)\n",
    "    plt.fill_between(-desired_xc[:150].flatten(), sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) + 2*sample_airfoil_std[:150], sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) - 2*sample_airfoil_std[:150], color = 'r', alpha = 0.2)\n",
    "    plt.fill_between(desired_xc[150:].flatten(), sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) + 2*sample_airfoil_std[150:], sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) - 2*sample_airfoil_std[150:], color = 'r', alpha = 0.2)\n",
    "    # for j in np.arange(0, 5):\n",
    "    #     plt.plot(np.abs(sample_airfoil[:, -1].cpu()), newDist.sample()+ torch.mean(y), alpha = 0.2, color = 'r')\n",
    "    plt.plot(np.abs(sample_airfoil[:, -1].cpu()), sample_airfoil_pred.mean.cpu() + torch.mean(y),'r', label='DKL GP, $C_p \\pm 2\\sigma$')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('x/c')\n",
    "    plt.ylabel('$C_p$')\n",
    "    plt.legend()\n",
    "    plt.title(targetAF +'\\n' + r'$\\alpha$ = ' + str(targetA) + r'$^\\circ,$' + r' $M_\\infty = 0.60$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Cl Cd calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manual_Cl = -np.trapz(y=(sample_airfoil_pred.mean.cpu()+torch.mean(y))[:150], x=desired_xc.flatten()[:150]) + np.trapz(y=(sample_airfoil_pred.mean.cpu()+torch.mean(y))[150:], x=desired_xc.flatten()[150:])\n",
    "print(np.trapz(y=(sample_airfoil_pred.mean.cpu())[:150]+torch.mean(y), x=desired_xc.flatten()[:150]) + np.trapz(y=(sample_airfoil_pred.mean.cpu())[150:]+torch.mean(y), x=desired_xc.flatten()[150:]))\n",
    "# torch.linalg.inv(temp_Kxx) @ train_y.cpu()\n",
    "# print(model.covar_module(train_x))\n",
    "\n",
    "print(sample_airfoil[0,-2])\n",
    "def lazy_Cl_Cd_Cm(x, cp, a):\n",
    "    N = np.trapz(cp[:150], x=np.abs(x.flatten())[:150]) + np.trapz(cp[150:], x=x.flatten()[150:])\n",
    "    Cl = N * np.cos(np.deg2rad(a))\n",
    "    Cd = N * np.sin(np.deg2rad(a))\n",
    "    Cm = np.trapz((np.abs(x.flatten())[:150]-0.25) * cp[:150], x=np.abs(x.flatten())[:150]) + np.trapz((x.flatten()[150:]-0.25) * cp[150:], x=x.flatten()[150:]) # <- double check\n",
    "    return Cl, Cd, Cm  \n",
    "\n",
    "def GP_Cl_Cd_Cm(model, train_x, test_data):\n",
    "    model = model.cpu()\n",
    "    train_x = train_x.cpu()\n",
    "    test_data = test_data.cpu()\n",
    "    \n",
    "    projected_xtrain = model.feature_extractor(train_x)\n",
    "    projected_xtrain = model.scale_to_bounds(projected_xtrain)\n",
    "    projected_xtest = model.feature_extractor(test_data)\n",
    "    projected_xtest = model.scale_to_bounds(projected_xtest)\n",
    "    jitter = 1e-5 * torch.eye(train_x.shape[0])\n",
    "    Kxx = (model.covar_module(projected_xtrain).evaluate().cpu() + train_noise * torch.eye(train_x.shape[0]) + jitter).detach().numpy()\n",
    "    Kxs = model.covar_module(projected_xtrain, projected_xtest).evaluate().cpu().detach().numpy()\n",
    "\n",
    "    KxsT = Kxs.T\n",
    "    temp_ind = np.argwhere(np.array(desired_xc).flatten() < 0.0)\n",
    "    KxsT[temp_ind, :] *= -1\n",
    "    i_Kxs = np.trapz(y=KxsT, x=np.array(desired_xc).flatten(), axis=0).reshape((-1,1)).T\n",
    "    N = i_Kxs @ np.linalg.inv(Kxx) @ train_y.cpu().numpy()\n",
    "    a = test_data[0, -2].numpy()\n",
    "    print(N)\n",
    "    Cl = N * np.cos(np.deg2rad(a))\n",
    "    Cd = N * np.sin(np.deg2rad(a))\n",
    "    # Cm = np.trapz((np.abs(x.flatten())[:150]-0.25) * cp[:150], x=np.abs(x.flatten())[:150]) + np.trapz((x.flatten()[150:]-0.25) * cp[150:], x=x.flatten()[150:]) # <- double check\n",
    "    return Cl, Cd  \n",
    "\n",
    "lazy_Cl_Cd_Cm(desired_xc.flatten(), (sample_airfoil_pred.mean.cpu()+torch.mean(y)).detach().numpy(), a = 9.1)\n",
    "GP_Cl_Cd_Cm(model, train_x, sample_airfoil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.abs(desired_xc.flatten()), asdf + torch.mean(y).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
