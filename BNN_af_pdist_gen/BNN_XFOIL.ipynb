{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries & set up env for GPU use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import vmap, jit   \n",
    "import os\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import (SVI, Trace_ELBO, autoguide, init_to_median, NUTS, MCMC, TraceMeanField_ELBO)\n",
    "import pandas as pd\n",
    "import pickle \n",
    "from numpyro.infer.util import Predictive, log_density \n",
    "import bezier\n",
    "\n",
    "#%% Environment and/or CUDA commands\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "os.environ['XLA_FLAGS']=\"--xla_gpu_force_compilation_parallelism=1\" # This is required until they update some things on PIBE\n",
    "# numpyro.enable_x64(use_x64=True)\n",
    "path = os.getcwd()\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = random.PRNGKey(0)\n",
    "\n",
    "\"\"\" Import data \"\"\"\n",
    "xdata = pd.read_csv('./data/XFOIL_BNN_train.csv',header=None)\n",
    "ydata = pd.read_csv('./data/XFOIL_BNN_values.csv',header=None)\n",
    "airfoilName = pd.read_csv('./data/XFOIL_BNN_ref.csv',header=None)\n",
    "airfoilName = np.array(airfoilName.values.reshape((-1,1)))\n",
    "x = xdata.values.T\n",
    "y = ydata.values.T\n",
    "\n",
    "\"\"\" Train-test split depending on the airfoil rather than individual cases (angles) \"\"\"\n",
    "def airfoil_train_test_split(X, Y, test_size, random_state, printChoice=False):\n",
    "    np.random.seed(random_state) \n",
    "    airfoilsNum = np.unique(airfoilName).shape[0]\n",
    "    airfoilsUnique = np.unique(airfoilName).flatten()\n",
    "    case_indexing = np.arange(0, airfoilsNum) # Create index of unique cases in X \n",
    "    test_draws_indexing = np.sort(np.random.choice(case_indexing, size=int(airfoilsNum*test_size)))\n",
    "    train_draws_indexing = np.sort(np.delete(case_indexing, test_draws_indexing))\n",
    "    \n",
    "    if printChoice:\n",
    "        print(test_draws_indexing)\n",
    "\n",
    "    for i in range(0, train_draws_indexing.shape[0]):\n",
    "        ind = np.argwhere(airfoilName.flatten()==airfoilsUnique[train_draws_indexing[i]]).flatten()\n",
    "        if i == 0:\n",
    "            x_train = X[ind, :]\n",
    "            y_train = Y[ind, :]\n",
    "        else :\n",
    "            x_train = np.vstack((x_train, X[ind, :]))\n",
    "            y_train = np.vstack((y_train, Y[ind, :]))\n",
    "    \n",
    "    for j in range(0, test_draws_indexing.shape[0]):\n",
    "        ind2 = np.argwhere(airfoilName.flatten()==airfoilsUnique[test_draws_indexing[j]]).flatten()\n",
    "        if j == 0:\n",
    "            x_test = X[ind2, :]\n",
    "            y_test = Y[ind2, :]\n",
    "        else :\n",
    "            x_test = np.vstack((x_test, X[ind2, :]))\n",
    "            y_test = np.vstack((y_test, Y[ind2, :]))\n",
    "        \n",
    "    return x_train, x_test, y_train, y_test \n",
    "\n",
    "trainPerc = 0.7 # Split percentage\n",
    "xTrain, xTest, yTrain, yTest = airfoil_train_test_split(x, y, test_size = 1-trainPerc, random_state=831, printChoice=False)\n",
    "afTrain, afTest, yTrain, yTest = airfoil_train_test_split(airfoilName, y, test_size = 1-trainPerc, random_state=831)\n",
    "\n",
    "\"\"\" Convert the coordinates data to Bezier fits \"\"\"\n",
    "from scipy.optimize import least_squares\n",
    "def convert_bezier(X, Y, deg):\n",
    "    degree_af = deg[0]\n",
    "    degree_p_u = deg[1]\n",
    "    degree_p_l = deg[2]\n",
    "    import scipy\n",
    "    # Convert X to airfoil information\n",
    "    new_data_x = np.zeros((X.shape[0], 2*(degree_af+2)+1))\n",
    "    new_data_Cp = np.zeros((X.shape[0], (degree_p_u+2) + (degree_p_l+2))) \n",
    "    \n",
    "        # Create design matrix \n",
    "    for i in range(X.shape[0]): # Loop through all cases \n",
    "        numVar = 1 # additional variables\n",
    "        cutoff = int((X.shape[1]-numVar)/2) #  usually 300\n",
    "        zeroLoc = np.argwhere(np.diff(X[i,:cutoff])<= 0.0)[-1][0]\n",
    "        # Obtain upper surface\n",
    "        xc_u = np.flip(X[i, :zeroLoc+1]) # airfoil x, spans from 0 to 1 \n",
    "        xc_l = X[i, zeroLoc+1:cutoff] \n",
    "        yc_u = np.flip(X[i, cutoff:cutoff+zeroLoc+1]) # airfoil thickness\n",
    "        yc_l = X[i, cutoff+zeroLoc+1:cutoff+cutoff]\n",
    "        Cp_u = np.flip(Y[i, :zeroLoc+1])\n",
    "        Cp_l = Y[i, zeroLoc+1:]\n",
    "\n",
    "        # Set up Bezier obj and optimize the control pts \n",
    "        fit_af_upper = bezier.Bezier(xc_u, yc_u, np.ones(degree_af)*np.max(yc_u), 'airfoil') # airfoil upper surf\n",
    "        fit_af_lower = bezier.Bezier(xc_l, yc_l, np.ones(degree_af)*np.min(yc_l), 'airfoil') # airfoil lower surf\n",
    "        fit_cp_upper = bezier.Bezier(xc_u, Cp_u, np.ones(degree_p_u)*-1, 'pressure') # pressure upper \n",
    "        fit_cp_lower = bezier.Bezier(xc_l, Cp_l, np.ones(degree_p_l)*1, 'pressure') # pressure lower\n",
    "        least_squares(fit_af_upper.update, np.ones(degree_af)*np.max(yc_u))\n",
    "        least_squares(fit_af_lower.update, np.ones(degree_af)*np.min(yc_l))\n",
    "        least_squares(fit_cp_upper.update, np.ones(degree_p_u)*-1)\n",
    "        least_squares(fit_cp_lower.update, np.ones(degree_p_l)*1)\n",
    "        \n",
    "        AoA = X[i, -1]\n",
    "        new_data_x[i,:] = np.hstack((fit_af_upper.ctrl_pts[:,1].flatten(), fit_af_lower.ctrl_pts[:,1].flatten(), AoA))\n",
    "        \n",
    "        new_data_Cp[i,:] = np.hstack((fit_cp_upper.ctrl_pts[:,1].flatten(), fit_cp_lower.ctrl_pts[:,1].flatten()))\n",
    "        print(i)\n",
    "    return new_data_x, new_data_Cp, [fit_af_upper.ctrl_pts[:,0], fit_af_lower.ctrl_pts[:,0], fit_cp_upper.ctrl_pts[:,0], fit_cp_lower.ctrl_pts[:,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertNew = False\n",
    "bezier_degs = [8,26,8]\n",
    "if convertNew:\n",
    "    xTrain, yTrain, xTrain_ctrl_pts = convert_bezier(xTrain, yTrain, bezier_degs)\n",
    "    xTest, yTest, xTest_ctrl_pts = convert_bezier(xTest, yTest, bezier_degs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './train_test_bz.pkl'\n",
    "\n",
    "# Use this to save the data \n",
    "# with open(file_path, 'wb') as file:\n",
    "#     save_dict = {'xTrain':xTrain, 'yTrain':yTrain, 'xTrain_ctrl_pts':xTrain_ctrl_pts, 'xTest':xTest, 'yTest':yTest, 'xTest_ctrl_pts':xTest_ctrl_pts}\n",
    "#     pickle.dump(save_dict, file)\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "\n",
    "xTrain = loaded_data['xTrain']    \n",
    "yTrain = loaded_data['yTrain']    \n",
    "xTrain_ctrl_pts = loaded_data['xTrain_ctrl_pts']    \n",
    "xTest = loaded_data['xTest']    \n",
    "yTest = loaded_data['yTest']    \n",
    "xTest_ctrl_pts = loaded_data['xTest_ctrl_pts']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-linear operation\n",
    "def nonlin(x):\n",
    "    return jnp.tanh(x)\n",
    "\n",
    "# Different model structure - effectively same as the one defined in airfoil_CP_predictor\n",
    "def model_bnn(X, Y=None, num_Neurons=[300, 300, 300], sigma_w=1):\n",
    "  D_X, D_Y = X.shape[1], yTrain.shape[1]\n",
    "  N = X.shape[0]\n",
    "  # sample first layer (we put unit normal priors on all weights)\n",
    "  w1 = numpyro.sample(\"w1\", dist.Normal(jnp.zeros((D_X, num_Neurons[0])), sigma_w*jnp.ones((D_X, num_Neurons[0]))))  \n",
    "  b1 = numpyro.sample(\"b1\", dist.Normal(jnp.zeros((num_Neurons[0], 1)), sigma_w*jnp.ones((num_Neurons[0], 1))))\n",
    "  z1 = nonlin(jnp.matmul(X, w1) + jnp.transpose(b1))   # N D_H  <= first layer of activations\n",
    "\n",
    "  # sample second layer\n",
    "  w2 = numpyro.sample(\"w2\", dist.Normal(jnp.zeros((num_Neurons[0], num_Neurons[1])), sigma_w*jnp.ones((num_Neurons[0], num_Neurons[1])))) \n",
    "  b2 = numpyro.sample(\"b2\", dist.Normal(jnp.zeros((num_Neurons[1], 1)), sigma_w*jnp.ones((num_Neurons[1], 1))))  # D_H 1\n",
    "  z2 = nonlin(jnp.matmul(z1, w2) + jnp.transpose(b2))  # N D_H  <= second layer of activations\n",
    "  \n",
    "  w3 = numpyro.sample(\"w3\", dist.Normal(jnp.zeros((num_Neurons[1], num_Neurons[2])), sigma_w*jnp.ones((num_Neurons[1], num_Neurons[2])))) \n",
    "  b3 = numpyro.sample(\"b3\", dist.Normal(jnp.zeros((num_Neurons[2], 1)), sigma_w*jnp.ones((num_Neurons[2], 1))))  # D_H 1\n",
    "  z3 = nonlin(jnp.matmul(z2, w3) + jnp.transpose(b3))  # N D_H  <= thrid layer of activations\n",
    "  \n",
    "  wf = numpyro.sample(\"wf\", dist.Normal(jnp.zeros((num_Neurons[-1], D_Y)), sigma_w*jnp.ones((num_Neurons[-1], D_Y)))) \n",
    "  bf = numpyro.sample(\"bf\", dist.Normal(jnp.zeros((D_Y, 1)), sigma_w*jnp.ones((D_Y, 1))))  # D_H 1\n",
    "  zf = jnp.matmul(z3, wf) + jnp.transpose(bf)  # N D_H  <= f layer of activations\n",
    " \n",
    "  with numpyro.plate(\"obs\", N):\n",
    "    # note we use to_event(1) because each observation has shape (1,)\n",
    "    numpyro.sample(\"Y\", dist.Normal(zf, 0.02).to_event(1), obs=Y) # 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNew = True\n",
    "\n",
    "if trainNew == True:\n",
    "    svi = None\n",
    "    svi_result = None\n",
    "    # Train new SVI model\n",
    "    total_iterations = 0\n",
    "    svi_init_state = None\n",
    "    guide = autoguide.AutoLaplaceApproximation(model=model_bnn) #AutoLaplaceApproximation(model=model_bnn, hessian_fn=lambda f, x: jax.hessian(f)(x) + 1e-3 * jnp.eye(x.shape[0]))\n",
    "    optimizer = numpyro.optim.Adam(step_size = 0.001) \n",
    "    svi = SVI(model_bnn, guide, optimizer, loss=TraceMeanField_ELBO())\n",
    "elif trainNew == False:\n",
    "    # Continue the save\n",
    "    print('Continuing from previous SVI state...') \n",
    "    svi.optimizer = autoguide.AutoLaplaceApproximation(model=model_bnn)\n",
    "    svi_init_state = svi_result.state\n",
    "else:\n",
    "     raise ValueError('Incorrect input')\n",
    "\n",
    "n_samples = 10000*3001\n",
    "# total_iterations += n_samples\n",
    "yTrainMean = np.mean(yTrain,axis=0)\n",
    "NN = [100, 50, 100]\n",
    "svi_result = svi.run(rng_key, n_samples, xTrain, yTrain-yTrainMean, NN, init_state=svi_init_state) \n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(svi_result.losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE AND LOAD \n",
    "file_path = './temp_BNN.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "svi_result = loaded_data['svi_result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Predictive from the trained model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = svi_result.params\n",
    "\n",
    "# file_path = './temp_BNN.pkl'\n",
    "# with open(file_path, 'wb') as file:\n",
    "#     save_dict = {'svi_result':svi_result}\n",
    "#     pickle.dump(save_dict, file)\n",
    "    \n",
    "# get posterior predictive (deterministics and likelihood)\n",
    "posterior_predictive = Predictive(\n",
    "    model=model_bnn, guide=guide, params=params, num_samples= 100\n",
    ")\n",
    "\n",
    "rng_key, rng_subkey = random.split(key=rng_key)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DEBUGGING HERE - DELET LATER \"\"\"\n",
    "state_ = svi_result.state\n",
    "params = svi_result.params\n",
    "print(params.keys())\n",
    "# print(params['auto_loc'].shape)\n",
    "def f(params):\n",
    "    new_params = unflat_info(params)\n",
    "    z1 = nonlin(jnp.matmul(xTrain[:2,:], new_params['w1_auto_loc']) + jnp.transpose(new_params['b1_auto_loc']))   # N D_H  <= first layer of activations\n",
    "    z2 = nonlin(jnp.matmul(z1, new_params['w2_auto_loc']) + jnp.transpose(new_params['b2_auto_loc']))  # N D_H  <= second layer of activations\n",
    "    z3 = nonlin(jnp.matmul(z2, new_params['w3_auto_loc']) + jnp.transpose(new_params['b3_auto_loc']))  # N D_H  <= thrid layer of activations\n",
    "    zf = jnp.matmul(z3, new_params['wf_auto_loc']) + jnp.transpose(new_params['bf_auto_loc'])  # N D_H  <= f layer of activations\n",
    "    lss = dist.Normal(zf, 0.02).to_event(1).log_prob(yTrain[:2] - yTrainMean)\n",
    "    return lss\n",
    "\n",
    "flat_params, unflat_info = jax.flatten_util.ravel_pytree(params)\n",
    "aaa = unflat_info(flat_params)\n",
    "\n",
    "H = jnp.squeeze(-jax.hessian(f)(flat_params))\n",
    "H += 1e-8 * jnp.eye(H.shape[0])\n",
    "\n",
    "plt.plot(H.flatten())\n",
    "H = (H + H.T)/2\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "scipy.linalg.cholesky(H)\n",
    "Hinv = scipy.linalg.inv(H)\n",
    "\n",
    "sampls = dist.MultivariateNormal(loc=flat_params, covariance_matrix = Hinv).sample(rng_key, (100,))\n",
    "\n",
    "plt.plot(sampls[:,0])\n",
    "# sampls.shape\n",
    "# sns.histplot(sampls[:,4], color='r', kde=True,stat='density')\n",
    "\n",
    "# plt.figure()\n",
    "# sns.histplot(sampls[:,0], color='r', kde=True,stat='density')\n",
    "\n",
    "# print(H1.shape)\n",
    "# ppd_sample = ppd.sample(rng_key, sample_shape=(14000,))\n",
    "# print(ppd)\n",
    "# print(ppd.mean)\n",
    "# print(ppd.variance)\n",
    "# print(ppd.mean.shape)\n",
    "# sns.histplot((ppd_sample[:,0].flatten()-ppd.mean[0])/np.sqrt(ppd.variance[0]), color='r', kde=True,stat='density')\n",
    "# plt.plot(qx, norm.pdf(qx), 'k--', lw=1, alpha=1.0, label='Normal dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Gaussianity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples from the posterior predictive\n",
    "runGaussianityCheck = True\n",
    "\"\"\" These are marginals \"\"\"\n",
    "if runGaussianityCheck: \n",
    "    numDraws = 15000\n",
    "    params = svi_result.params\n",
    "    prediction_sampler = Predictive(\n",
    "        model=model_bnn, params=params, guide=guide, num_samples=numDraws # , parallel=True\n",
    "    )\n",
    "    # print(prediction_sampler(rng_key, xTest[1:2,:]))\n",
    "    MLP_draws = np.squeeze(prediction_sampler(rng_key, xTest[1:2,:])['Y'])\n",
    "    \n",
    "    import scipy\n",
    "    from scipy.stats import norm\n",
    "    import pylab as py\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # print(sns.__version__)\n",
    "    f, ax = plt.subplots(MLP_draws.shape[1], 2, figsize=(10,90))\n",
    "    for i in range(0, MLP_draws.shape[1]): # MLP_draws.shape[1] \n",
    "        # Reference normal distribution\n",
    "        rv = norm.rvs(loc=0.0, scale=1.0, size=numDraws)\n",
    "        qx = np.linspace(norm.ppf(0.001), norm.ppf(0.999), 100)\n",
    "        # sns.histplot(rv, color='k', label='True Normal Distribution', kde=True, line_kws = dict(alpha=1.0), ax=ax[i,0])\n",
    "        \n",
    "        # \n",
    "        MLP_draws_case = np.array(MLP_draws[:,i]) # Convert to standard normal\n",
    "        MLP_draws_case = (MLP_draws_case - np.mean(MLP_draws_case))/np.std(MLP_draws_case)\n",
    "        sns.histplot(MLP_draws_case.flatten(), color='r', label='Samples', kde=True, line_kws = dict(alpha=1.0), ax=ax[i,0], bins=100, stat='density')\n",
    "        ax[i,0].text(-3.5, 0.39, str(i), horizontalalignment='center', verticalalignment='center')\n",
    "        \n",
    "        ax[i,0].plot(qx, norm.pdf(qx), 'k--', lw=1, alpha=1.0, label='Normal dist')\n",
    "        ax[i,0].legend(loc='upper right')\n",
    "        ax[0,0].title.set_text('Histogram')\n",
    "        # ax[i,0].set_xlim([-4,4])\n",
    "        # ax[i,0].set_ylim([0, 300])\n",
    "        \n",
    "        ax[i,0].plot()\n",
    "        scipy.stats.probplot(MLP_draws_case, dist='norm', plot=ax[i,1])\n",
    "        ax[0,1].title.set_text('QQ-plot')\n",
    "        test = scipy.stats.kstest(MLP_draws_case,\n",
    "                scipy.stats.norm.cdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Outdated- will update\"\"\"\n",
    "saveModel = False\n",
    "if saveModel:\n",
    "    architecture = 'MLP'\n",
    "    num_neurons = '300'\n",
    "    noise_value = 0.02\n",
    "    additional_notes = 'XFOIL prototype'\n",
    "    meta = [architecture, num_neurons, noise_value, n_samples, additional_notes]\n",
    "    data = [xTrain, xTest, afTrain, afTest, yTrain, yTest]\n",
    "    saveData = [svi_result, guide, data, meta]\n",
    "\n",
    "    saveData = {} \n",
    "    saveData['model'] = model_bnn\n",
    "    saveData['params'] = svi_result.params\n",
    "    saveData['guide'] = guide\n",
    "    saveData['data'] = data\n",
    "    saveData['meta'] = meta \n",
    "# with open('/home/hlee981/Documents/Lift Distribution/Onera M6/Prior Generator Model/quickload.pkl', 'wb') as file:\n",
    "#         pickle.dump(saveData, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation on NACA 4-digit series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions and convert to tangible curves - plotting samples, training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setNew = True\n",
    "if setNew == True:\n",
    "    post_pred_train = posterior_predictive(rng_subkey, xTrain)\n",
    "    mu = np.mean(np.squeeze(post_pred_train['Y']), axis=0) + yTrainMean\n",
    "    std = np.std(np.squeeze(post_pred_train['Y']), axis=0) # <- CHANGE THIS \n",
    "\n",
    "    post_pred_test = posterior_predictive(rng_subkey, xTest)\n",
    "    mu_t = np.mean(np.squeeze(post_pred_test['Y']), axis=0) + yTrainMean\n",
    "    std_t = np.std(np.squeeze(post_pred_test['Y']), axis=0)\n",
    "\n",
    "def verifyCpResults(indRange, og_curve, curve, std_u, std_l, names, ax, subplotInd):\n",
    "    ax[subplotInd].plot(curve[indRange]['cp_u'][0], curve[indRange]['cp_u'][1] ,'r', label =  'Mean prediction, $\\mu$')\n",
    "    ax[subplotInd].plot(curve[indRange]['cp_l'][0], curve[indRange]['cp_l'][1] ,'r--', label =  '')\n",
    "    ax[subplotInd].fill_between(std_u[indRange]['cp_u'][0], std_u[indRange]['cp_u'][1], std_l[indRange]['cp_u'][1], color='r', alpha = 0.4, label='$\\mu \\pm \\sigma$, 68% CI')\n",
    "    ax[subplotInd].fill_between(std_l[indRange]['cp_l'][0], std_u[indRange]['cp_l'][1], std_l[indRange]['cp_l'][1], color='r', alpha = 0.4, label='')\n",
    "    ax[subplotInd].plot(og_curve[indRange]['cp_u'][0], og_curve[indRange]['cp_u'][1] ,'k', label =  'Ground Truth')\n",
    "    ax[subplotInd].plot(og_curve[indRange]['cp_l'][0], og_curve[indRange]['cp_l'][1] ,'k--', label =  '')\n",
    "    ax[subplotInd].invert_yaxis() \n",
    "    ax[subplotInd].set_xlabel('x/c')\n",
    "    ax[subplotInd].set_ylabel('$C_p$')\n",
    "    titleStr = str(names[indRange][0]) + r\", $\\alpha$ = \" + str(xTrain[indRange,-1]) + ' deg'\n",
    "    ax[subplotInd].title.set_text(titleStr)\n",
    "\n",
    "\"\"\" Take bezier control point predictions and convert to corresponding curves \"\"\"\n",
    "def bezier2curve(x, y, ref_x, num_pts, degrees):\n",
    "    num_af   = degrees[0] + 2\n",
    "    num_cp_u = degrees[1] + 2\n",
    "    num_cp_l = degrees[2] + 2\n",
    "    xx = np.linspace(0, 1, num_pts)\n",
    "    \n",
    "    out_list = [] # create list of dictionaries\n",
    "    for i in range(0, x.shape[0]): # \n",
    "        af_u = bezier.bezier_curve(np.hstack((ref_x[0][:,None], x[i, :num_af][:,None])), xx)\n",
    "        af_l = bezier.bezier_curve(np.hstack((ref_x[1][:,None], x[i, num_af:-1][:,None])), xx)\n",
    "        cp_u = bezier.bezier_curve(np.hstack((ref_x[2][:,None], y[i, :num_cp_u][:,None])), xx)\n",
    "        cp_l = bezier.bezier_curve(np.hstack((ref_x[3][:,None], y[i, num_cp_u:][:,None])), xx)\n",
    "        caseDict = {'af_u':af_u, 'af_l':af_l, 'cp_u':cp_u, 'cp_l':cp_l}\n",
    "        out_list.append(caseDict)\n",
    "        \n",
    "    return out_list\n",
    "\n",
    "visualizeSamples = False\n",
    "if visualizeSamples:\n",
    "    plt.figure()\n",
    "    num_samples = 3\n",
    "    color_u = ['b', 'g', 'm']\n",
    "    color_l = ['b--', 'g--', 'm--']\n",
    "    label_u = ['Sample 1', 'Sample 2', 'Sample 3']\n",
    "    caseIndex = 80\n",
    "    \n",
    "    mean_curve = bezier2curve(xTrain[caseIndex, :].reshape((1,-1)), np.mean(np.squeeze(post_pred_train['Y'][:, caseIndex, :]),axis=0).reshape((1,-1)) + yTrainMean, xTest_ctrl_pts, 100, bezier_degs)\n",
    "    std_curve_u = bezier2curve(xTrain[caseIndex, :].reshape((1,-1)), np.mean(np.squeeze(post_pred_train['Y'][:, caseIndex, :]),axis=0).reshape((1,-1)) + yTrainMean + np.std(np.squeeze(post_pred_train['Y'][:, caseIndex, :]),axis=0).reshape((1,-1)), xTest_ctrl_pts, 100, bezier_degs)\n",
    "    std_curve_l = bezier2curve(xTrain[caseIndex, :].reshape((1,-1)), np.mean(np.squeeze(post_pred_train['Y'][:, caseIndex, :]),axis=0).reshape((1,-1)) + yTrainMean - np.std(np.squeeze(post_pred_train['Y'][:, caseIndex, :]),axis=0).reshape((1,-1)), xTest_ctrl_pts, 100, bezier_degs)\n",
    "    plt.plot(mean_curve[0]['cp_u'][0], mean_curve[0]['cp_u'][1], 'r-', label='Mean predicted $C_p$ $\\pm \\sigma$')\n",
    "    plt.plot(mean_curve[0]['cp_l'][0], mean_curve[0]['cp_l'][1], 'r--')\n",
    "    plt.fill_between(std_curve_u[0]['cp_u'][0], std_curve_u[0]['cp_u'][1], std_curve_l[0]['cp_u'][1], color='r', alpha=0.2, label = '')\n",
    "    \n",
    "    for i in range(0, num_samples):\n",
    "        sample_curve = bezier2curve(xTest[caseIndex, :].reshape((1,-1)), np.squeeze(post_pred_train['Y'][i, caseIndex, :]).reshape((1,-1)) + yTrainMean, xTest_ctrl_pts, 100, bezier_degs)\n",
    "        plt.plot(sample_curve[0]['cp_u'][0], sample_curve[0]['cp_u'][1], color_u[i], alpha =0.7, label=label_u[i])\n",
    "        plt.plot(sample_curve[0]['cp_l'][0], sample_curve[0]['cp_l'][1], color_l[i], alpha =0.7)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('x/c')\n",
    "    plt.ylabel('$C_p$')\n",
    "    plt.title('Example posterior distribution')\n",
    "\n",
    "train_curve = bezier2curve(xTrain, yTrain, xTrain_ctrl_pts, 100, bezier_degs)\n",
    "mean_ctrl_pts = np.mean(np.squeeze(post_pred_train['Y']),axis=0)\n",
    "std_ctrl_pts = np.std(np.squeeze(post_pred_train['Y']),axis=0)\n",
    "mean_curve_train = bezier2curve(xTrain, mean_ctrl_pts.reshape((-1,post_pred_train['Y'].shape[2])) + yTrainMean, xTrain_ctrl_pts, 100, bezier_degs)\n",
    "mean_curve_std_u = bezier2curve(xTrain, mean_ctrl_pts.reshape((-1,post_pred_train['Y'].shape[2])) + yTrainMean + std_ctrl_pts, xTrain_ctrl_pts, 100, bezier_degs)\n",
    "mean_curve_std_l = bezier2curve(xTrain, mean_ctrl_pts.reshape((-1,post_pred_train['Y'].shape[2])) + yTrainMean - std_ctrl_pts, xTrain_ctrl_pts, 100, bezier_degs)\n",
    "\n",
    "# Verify fits on the test data \n",
    "train_samples = np.random.choice(np.arange(0, xTrain.shape[0]), size=6)\n",
    "fig, ax = plt.subplots(1,6, figsize=(32,4))\n",
    "verifyCpResults(train_samples[0], train_curve, mean_curve_train, mean_curve_std_u, mean_curve_std_l, afTrain, ax, 0)\n",
    "verifyCpResults(train_samples[1], train_curve, mean_curve_train, mean_curve_std_u, mean_curve_std_l, afTrain, ax, 1)\n",
    "verifyCpResults(train_samples[2], train_curve, mean_curve_train, mean_curve_std_u, mean_curve_std_l, afTrain, ax, 2)\n",
    "verifyCpResults(train_samples[3], train_curve, mean_curve_train, mean_curve_std_u, mean_curve_std_l, afTrain, ax, 3)\n",
    "verifyCpResults(train_samples[4], train_curve, mean_curve_train, mean_curve_std_u, mean_curve_std_l, afTrain, ax, 4)\n",
    "verifyCpResults(train_samples[5], train_curve, mean_curve_train, mean_curve_std_u, mean_curve_std_l, afTrain, ax, 5)\n",
    "ax[0].legend()\n",
    "fig.suptitle('Checking predictions on training data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseIndex = 2\n",
    "\n",
    "trainPerc = 0.7 # Split percentage\n",
    "oxTrain, oxTest, oyTrain, oyTest = airfoil_train_test_split(x, y, test_size = 1-trainPerc, random_state=831, printChoice=False)\n",
    "\n",
    "test_curve = bezier2curve(xTest, yTest, xTest_ctrl_pts, 100, bezier_degs)\n",
    "mean_ctrl_pts = np.mean(np.squeeze(post_pred_test['Y']),axis=0)\n",
    "std_ctrl_pts = np.std(np.squeeze(post_pred_test['Y']),axis=0)\n",
    "mean_curve_test = bezier2curve(xTest, mean_ctrl_pts.reshape((-1,post_pred_test['Y'].shape[2])) + yTrainMean, xTest_ctrl_pts, 100, bezier_degs)\n",
    "mean_curve_std_u = bezier2curve(xTest, mean_ctrl_pts.reshape((-1,post_pred_test['Y'].shape[2])) + yTrainMean + std_ctrl_pts, xTest_ctrl_pts, 100, bezier_degs)\n",
    "mean_curve_std_l = bezier2curve(xTest, mean_ctrl_pts.reshape((-1,post_pred_test['Y'].shape[2])) + yTrainMean - std_ctrl_pts, xTest_ctrl_pts, 100, bezier_degs)\n",
    "\n",
    "# Verify fits on the test data \n",
    "validation_samples = np.random.choice(np.arange(0, xTest.shape[0]), size=6)\n",
    "fig, ax = plt.subplots(1,6, figsize=(32,4))\n",
    "verifyCpResults(validation_samples[0], test_curve, mean_curve_test, mean_curve_std_u, mean_curve_std_l, afTrain, ax, 0)\n",
    "verifyCpResults(validation_samples[1], test_curve, mean_curve_test, mean_curve_std_u, mean_curve_std_l, afTrain, ax, 1)\n",
    "verifyCpResults(validation_samples[2], test_curve, mean_curve_test, mean_curve_std_u, mean_curve_std_l, afTrain, ax, 2)\n",
    "verifyCpResults(validation_samples[3], test_curve, mean_curve_test, mean_curve_std_u, mean_curve_std_l, afTrain, ax, 3)\n",
    "verifyCpResults(validation_samples[4], test_curve, mean_curve_test, mean_curve_std_u, mean_curve_std_l, afTrain, ax, 4)\n",
    "verifyCpResults(validation_samples[5], test_curve, mean_curve_test, mean_curve_std_u, mean_curve_std_l, afTrain, ax, 5)\n",
    "ax[0].legend()\n",
    "fig.suptitle('Checking predictions on test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation on other types of airfoils - incomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to do this, only the XFOIL the data is plotted as of now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1: Clark Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runNew = True\n",
    "# Read Clark Y Coordinates\n",
    "if runNew:\n",
    "    fileLoc = './data/validation_ClarkY/'\n",
    "    clark_y = np.loadtxt(fileLoc+\"clark_y.txt\", delimiter=' ')\n",
    "    num_pts = clark_y.shape[0]\n",
    "    clark_y_rs = np.hstack((clark_y.reshape((1,-1), order='F'), np.array([1])[:,None])) # Append a single num for the function\n",
    "    case1, dummy_y, new_ctrl_pts = convert_bezier(clark_y_rs, np.zeros((1, num_pts)), bezier_degs)\n",
    "\n",
    "plt.figure()\n",
    "clark_y_profile = bezier2curve(case1, dummy_y, new_ctrl_pts, 100, bezier_degs)\n",
    "plt.plot(clark_y_profile[0]['af_u'][0][::5], clark_y_profile[0]['af_u'][1][::5],'ro', label='Optimized Bezier fit')\n",
    "plt.plot(clark_y_profile[0]['af_l'][0][::5], clark_y_profile[0]['af_l'][1][::5],'ro')\n",
    "plt.plot(clark_y[::4,0], clark_y[::4,1], 'k-', label = 'Original Clark Y')\n",
    "plt.xlabel('x/c')\n",
    "plt.ylabel('z/c')\n",
    "plt.axis('scaled')\n",
    "plt.ylim([-0.15,0.15])\n",
    "plt.legend()\n",
    "\n",
    "# Case a) a = 0 deg\n",
    "f_case1, ax_case1 = plt.subplots(2,3,figsize=(14,8))\n",
    "case1[-1] = 0 \n",
    "case1a_results = posterior_predictive(rng_subkey, case1)\n",
    "case1a_results_mu = np.mean(np.squeeze(case1a_results['Y']), axis=0) + yTrainMean\n",
    "case1a_results_curve = bezier2curve(case1, case1a_results_mu.reshape((1,-1)), new_ctrl_pts, 100, bezier_degs)\n",
    "ax_case1[0,0].plot(case1a_results_curve[0]['cp_u'][0], case1a_results_curve[0]['cp_u'][1])\n",
    "ax_case1[0,0].plot(case1a_results_curve[0]['cp_l'][0], case1a_results_curve[0]['cp_l'][1])\n",
    "\n",
    "    # Validation data \n",
    "case1a_val = np.loadtxt(fileLoc+\"clarkY_0.txt\", delimiter=' ')\n",
    "ax_case1[0,0].plot(case1a_val[:,0], case1a_val[:,2],'k--', label='XFOIL')\n",
    "ax_case1[0,0].set_xlabel('x/c [1]')\n",
    "ax_case1[0,0].set_ylabel('$C_p$ [1]')\n",
    "ax_case1[0,0].title.set_text('0 deg')\n",
    "ax_case1[0,0].set_ylim([-1.6, 1.1])\n",
    "ax_case1[0,0].invert_yaxis()\n",
    "\n",
    "# Case b) a = 1 deg\n",
    "case1[-1] = 1 \n",
    "case1b_results = posterior_predictive(rng_subkey, case1)\n",
    "case1b_results_mu = np.mean(np.squeeze(case1b_results['Y']), axis=0) + yTrainMean\n",
    "case1b_results_curve = bezier2curve(case1, case1b_results_mu.reshape((1,-1)), new_ctrl_pts, 100, bezier_degs)\n",
    "ax_case1[0,1].plot(case1b_results_curve[0]['cp_u'][0], case1b_results_curve[0]['cp_u'][1])\n",
    "ax_case1[0,1].plot(case1b_results_curve[0]['cp_l'][0], case1b_results_curve[0]['cp_l'][1])\n",
    "    # Validation data \n",
    "case1b_val = np.loadtxt(fileLoc+\"clarkY_1.txt\", delimiter=' ')\n",
    "ax_case1[0,1].plot(case1b_val[:,0], case1b_val[:,2],'k--', label='XFOIL')\n",
    "ax_case1[0,1].set_xlabel('x/c [1]')\n",
    "ax_case1[0,1].set_ylabel('$C_p$ [1]')\n",
    "ax_case1[0,1].title.set_text('1 deg')\n",
    "ax_case1[0,1].set_ylim([-1.6, 1.1])\n",
    "ax_case1[0,1].invert_yaxis()\n",
    "\n",
    "# Case c) a = 2 deg\n",
    "    # Validation data \n",
    "case1c_val = np.loadtxt(fileLoc+\"clarkY_2.txt\", delimiter=' ')\n",
    "ax_case1[0,2].plot(case1c_val[:,0], case1c_val[:,2],'k--', label='XFOIL')\n",
    "ax_case1[0,2].set_xlabel('x/c [1]')\n",
    "ax_case1[0,2].set_ylabel('$C_p$ [1]')\n",
    "ax_case1[0,2].title.set_text('2 deg')\n",
    "ax_case1[0,2].set_ylim([-1.6, 1.1])\n",
    "ax_case1[0,2].invert_yaxis()\n",
    "\n",
    "# Case d) a = 3 deg\n",
    "    # Validation data \n",
    "case1d_val = np.loadtxt(fileLoc+\"clarkY_3.txt\", delimiter=' ')\n",
    "ax_case1[1,0].plot(case1d_val[:,0], case1d_val[:,2],'k--', label='XFOIL')\n",
    "ax_case1[1,0].set_xlabel('x/c [1]')\n",
    "ax_case1[1,0].set_ylabel('$C_p$ [1]')\n",
    "ax_case1[1,0].title.set_text('3 deg')\n",
    "ax_case1[1,0].set_ylim([-1.6, 1.1])\n",
    "ax_case1[1,0].invert_yaxis()\n",
    "\n",
    "# Case e) a = 4 deg\n",
    "    # Validation data \n",
    "case1e_val = np.loadtxt(fileLoc+\"clarkY_4.txt\", delimiter=' ')\n",
    "ax_case1[1,1].plot(case1e_val[:,0], case1e_val[:,2],'k--', label='XFOIL')\n",
    "ax_case1[1,1].set_xlabel('x/c [1]')\n",
    "ax_case1[1,1].set_ylabel('$C_p$ [1]')\n",
    "ax_case1[1,1].title.set_text('4 deg')\n",
    "ax_case1[1,1].set_ylim([-1.6, 1.1])\n",
    "ax_case1[1,1].invert_yaxis()\n",
    "\n",
    "# Case f) a = 5 deg\n",
    "    # Validation data \n",
    "case1f_val = np.loadtxt(fileLoc+\"clarkY_5.txt\", delimiter=' ')\n",
    "ax_case1[1,2].plot(case1f_val[:,0], case1f_val[:,2],'k--', label='XFOIL')\n",
    "ax_case1[1,2].set_xlabel('x/c [1]')\n",
    "ax_case1[1,2].set_ylabel('$C_p$ [1]')\n",
    "ax_case1[1,2].title.set_text('5 deg')\n",
    "ax_case1[1,2].set_ylim([-1.6, 1.1])\n",
    "ax_case1[1,2].invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2: Onera D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runNew = True\n",
    "# Read Onera D Coordinates\n",
    "if runNew:\n",
    "    fileLoc = './data/validation_OneraD/'\n",
    "    onera_D = np.loadtxt(fileLoc+\"oneraD.txt\", delimiter=' ')\n",
    "    num_pts = onera_D.shape[0]\n",
    "    onera_D_rs = np.hstack((onera_D.reshape((1,-1), order='F'), np.array([1])[:,None])) # Append a single num for the function\n",
    "    case2, dummy_y, new_ctrl_pts = convert_bezier(onera_D_rs, np.zeros((1, num_pts)), bezier_degs)\n",
    "\n",
    "plt.figure()\n",
    "onera_D_profile = bezier2curve(case2, dummy_y, new_ctrl_pts, 100, bezier_degs)\n",
    "plt.plot(onera_D_profile[0]['af_u'][0][::5], onera_D_profile[0]['af_u'][1][::5],'ro', label='Optimized Bezier fit')\n",
    "plt.plot(onera_D_profile[0]['af_l'][0][::5], onera_D_profile[0]['af_l'][1][::5],'ro')\n",
    "plt.plot(onera_D[::4,0], onera_D[::4,1], 'k-', label = 'Original Onera D')\n",
    "plt.xlabel('x/c')\n",
    "plt.ylabel('z/c')\n",
    "plt.axis('scaled')\n",
    "plt.ylim([-0.15,0.15])\n",
    "plt.legend()\n",
    "\n",
    "# Case a) a = 0 deg\n",
    "f_case2, ax_case2 = plt.subplots(2,3,figsize=(14,8))\n",
    "case2[-1] = 0 \n",
    "case2a_results = posterior_predictive(rng_subkey, case2)\n",
    "case2a_results_mu = np.mean(np.squeeze(case2a_results['Y']), axis=0) + yTrainMean\n",
    "case2a_results_curve = bezier2curve(case2, case2a_results_mu.reshape((1,-1)), new_ctrl_pts, 100, bezier_degs)\n",
    "ax_case2[0,0].plot(case2a_results_curve[0]['cp_u'][0], case2a_results_curve[0]['cp_u'][1])\n",
    "ax_case2[0,0].plot(case2a_results_curve[0]['cp_l'][0], case2a_results_curve[0]['cp_l'][1])\n",
    "\n",
    "    # Validation data \n",
    "case2a_val = np.loadtxt(fileLoc+\"oneraD_0.txt\", delimiter=' ')\n",
    "ax_case2[0,0].plot(case2a_val[:,0], case2a_val[:,2],'k--', label='XFOIL')\n",
    "ax_case2[0,0].set_xlabel('x/c [1]')\n",
    "ax_case2[0,0].set_ylabel('$C_p$ [1]')\n",
    "ax_case2[0,0].title.set_text('0 deg')\n",
    "ax_case2[0,0].set_ylim([-2.2, 1.1])\n",
    "ax_case2[0,0].invert_yaxis()\n",
    "\n",
    "# Case b) a = 1 deg\n",
    "case2[-1] = 1 \n",
    "case2b_results = posterior_predictive(rng_subkey, case2)\n",
    "case2b_results_mu = np.mean(np.squeeze(case2b_results['Y']), axis=0) + yTrainMean\n",
    "case2b_results_curve = bezier2curve(case2, case2b_results_mu.reshape((1,-1)), new_ctrl_pts, 100, bezier_degs)\n",
    "ax_case2[0,1].plot(case2b_results_curve[0]['cp_u'][0], case2b_results_curve[0]['cp_u'][1])\n",
    "ax_case2[0,1].plot(case2b_results_curve[0]['cp_l'][0], case2b_results_curve[0]['cp_l'][1])\n",
    "    # Validation data \n",
    "case2b_val = np.loadtxt(fileLoc+\"oneraD_1.txt\", delimiter=' ')\n",
    "ax_case2[0,1].plot(case2b_val[:,0], case2b_val[:,2],'k--', label='XFOIL')\n",
    "ax_case2[0,1].set_xlabel('x/c [1]')\n",
    "ax_case2[0,1].set_ylabel('$C_p$ [1]')\n",
    "ax_case2[0,1].title.set_text('1 deg')\n",
    "ax_case2[0,1].set_ylim([-2.2, 1.1])\n",
    "ax_case2[0,1].invert_yaxis()\n",
    "\n",
    "# Case c) a = 2 deg\n",
    "    # Validation data \n",
    "case2c_val = np.loadtxt(fileLoc+\"oneraD_2.txt\", delimiter=' ')\n",
    "ax_case2[0,2].plot(case2c_val[:,0], case2c_val[:,2],'k--', label='XFOIL')\n",
    "ax_case2[0,2].set_xlabel('x/c [1]')\n",
    "ax_case2[0,2].set_ylabel('$C_p$ [1]')\n",
    "ax_case2[0,2].title.set_text('2 deg')\n",
    "ax_case2[0,2].set_ylim([-2.2, 1.1])\n",
    "ax_case2[0,2].invert_yaxis()\n",
    "\n",
    "# Case d) a = 3 deg\n",
    "    # Validation data \n",
    "case2d_val = np.loadtxt(fileLoc+\"oneraD_3.txt\", delimiter=' ')\n",
    "ax_case2[1,0].plot(case2d_val[:,0], case2d_val[:,2],'k--', label='XFOIL')\n",
    "ax_case2[1,0].set_xlabel('x/c [1]')\n",
    "ax_case2[1,0].set_ylabel('$C_p$ [1]')\n",
    "ax_case2[1,0].title.set_text('3 deg')\n",
    "ax_case2[1,0].set_ylim([-2.2, 1.1])\n",
    "ax_case2[1,0].invert_yaxis()\n",
    "\n",
    "# Case e) a = 4 deg\n",
    "    # Validation data \n",
    "case2e_val = np.loadtxt(fileLoc+\"oneraD_4.txt\", delimiter=' ')\n",
    "ax_case2[1,1].plot(case2e_val[:,0], case2e_val[:,2],'k--', label='XFOIL')\n",
    "ax_case2[1,1].set_xlabel('x/c [1]')\n",
    "ax_case2[1,1].set_ylabel('$C_p$ [1]')\n",
    "ax_case2[1,1].title.set_text('4 deg')\n",
    "ax_case2[1,1].set_ylim([-2.2, 1.1])\n",
    "ax_case2[1,1].invert_yaxis()\n",
    "\n",
    "# Case f) a = 5 deg\n",
    "    # Validation data \n",
    "case2f_val = np.loadtxt(fileLoc+\"oneraD_5.txt\", delimiter=' ')\n",
    "ax_case2[1,2].plot(case2f_val[:,0], case2f_val[:,2],'k--', label='XFOIL')\n",
    "ax_case2[1,2].set_xlabel('x/c [1]')\n",
    "ax_case2[1,2].set_ylabel('$C_p$ [1]')\n",
    "ax_case2[1,2].title.set_text('5 deg')\n",
    "ax_case2[1,2].set_ylim([-2.2, 1.1])\n",
    "ax_case2[1,2].invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
