{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.model_selection import train_test_split \n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "# Set up GPU usage \n",
    "useGPU = True\n",
    "if useGPU == True: # 6 is for personal use \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "    n_devices = torch.cuda.device_count()\n",
    "    output_device = torch.device('cuda:0')\n",
    "elif useGPU == False: \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    output_device = torch.device('cpu')\n",
    "else: \n",
    "    raise ValueError(\"Incorrect value of useGPU variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, history, iters, fileName):\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'history': history,\n",
    "                'iterations': iters\n",
    "                },\n",
    "            fileName)\n",
    "    \n",
    "# Diagnostics \n",
    "def alpha_vs_Mach(x_af, y_pred_af, y_true_af, titleStr='MAE: ' + r'$\\alpha$' + ' vs. M'):\n",
    "    x_af = x_af.cpu().detach().numpy()\n",
    "    x_af[:, -3] = np.round(np.rad2deg(x_af[:, -3]), 2)\n",
    "    y_pred_af = y_pred_af.cpu().detach().numpy()/100\n",
    "    y_true_af = y_true_af.cpu().detach().numpy()/100\n",
    "    \n",
    "    M_list = np.unique(x_af[:, -2])\n",
    "    A_list = np.unique(x_af[:, -3])\n",
    "    MAE_arr = np.empty((M_list.shape[0], A_list.shape[0]))\n",
    "    MAE_arr[:] = np.nan\n",
    "\n",
    "    for i in range(0, M_list.shape[0]):\n",
    "        for j in range(0, A_list.shape[0]):\n",
    "            ind1 = np.argwhere(x_af[:, -2]==M_list[i])\n",
    "            ind2 = np.argwhere(x_af[:, -3]==A_list[j])\n",
    "            search_ind = np.intersect1d(ind1, ind2)\n",
    "            \n",
    "            # Grab all Cp along x/c and obtain average MAE\n",
    "            MAE_arr[-1-i, j] = np.mean(np.abs(y_pred_af[search_ind] - y_true_af[search_ind]))\n",
    "    \n",
    "    # Create plot \n",
    "    f = plt.figure()\n",
    "    ax = f.gca()\n",
    "    im = ax.imshow(MAE_arr, cmap='Oranges')\n",
    "    ax.set_yticks(np.arange(0, M_list.shape[0]), np.flip(M_list))\n",
    "    ax.set_xticks(np.arange(0, A_list.shape[0]), A_list)\n",
    "    f.colorbar(im, label='MAE')\n",
    "    ax.title.set_text(titleStr)\n",
    "    ax.set_xlabel('Angle of attack, ' + r'$\\alpha$ [deg]')\n",
    "    ax.set_ylabel('Mach Number, M')\n",
    "    ax.set_aspect('auto')\n",
    "    return MAE_arr, (f, ax)\n",
    "\n",
    "def xc_vs_airfoil(x_af, y_pred_af, y_true_af, titleStr='MAE: ' + r'$\\alpha$' + ' vs. M'):\n",
    "    x_af = x_af.cpu().detach().numpy()\n",
    "    x_af[:, -3] = np.round(np.rad2deg(x_af[:, -3]), 2)\n",
    "    sort_ind = np.lexsort((x_af[:,-3], x_af[:,-2]))\n",
    "    x_af = x_af[sort_ind]\n",
    "    y_pred_af = y_pred_af.cpu().detach().numpy()/100\n",
    "    y_pred_af = y_pred_af[sort_ind]\n",
    "    y_true_af = y_true_af.cpu().detach().numpy()/100\n",
    "    y_true_af = y_true_af[sort_ind]\n",
    "    \n",
    "    xc_list = np.unique(x_af[:, -1])\n",
    "    inds = np.where(np.abs(np.diff(x_af[:,-1])) > 1)[0]+1\n",
    "\n",
    "    N = inds.shape[0]\n",
    "    MAE_arr = np.empty((N, xc_list.shape[0]))\n",
    "    MAE_arr[:] = np.nan\n",
    "    \n",
    "    prev_pt = 0\n",
    "    tick_label = []\n",
    "    for i in range(0, N):\n",
    "        # Make tick label str \n",
    "        tick_label.append('M = ' + str(x_af[inds[i]-1, -2]) + r', $\\alpha$ = ' + str(x_af[inds[i]-1, -3]))\n",
    "        for j in range(prev_pt, inds[i]):\n",
    "            new_ind = np.argwhere(xc_list == x_af[j, -1])\n",
    "            MAE_arr[-1-i, new_ind] = np.abs(y_pred_af[j] - y_true_af[j])\n",
    "        prev_pt = inds[i]\n",
    "        \n",
    "    f = plt.figure()\n",
    "    ax = f.gca()\n",
    "    im = ax.imshow(MAE_arr, cmap='Oranges')\n",
    "    ax.set_aspect('auto')\n",
    "    f.colorbar(im, label='MAE')\n",
    "    ax.title.set_text(titleStr)\n",
    "    ax.set_xlabel('Unraveled x/c')\n",
    "    ax.set_ylabel('Case')\n",
    "    ax.set_yticks(np.arange(0, N), np.flip(tick_label))\n",
    "    ax.set_xticks(np.linspace(0, xc_list.shape[0]-1, 8), np.round(xc_list[np.linspace(0, xc_list.shape[0]-1, 8, dtype=int)], 2))\n",
    "    return MAE_arr, (f, ax)\n",
    "\n",
    "def data_breakdown(data_raw): # Redo this \n",
    "    infos = data_raw[['af', 'symmetry', 'supercritical', 'M' ,'alpha']]\n",
    "    symm = infos[infos['symmetry'] == 'symmetric']\n",
    "    camb = infos[infos['symmetry'] == 'cambered']\n",
    "    symm_supercrit = symm[symm['supercritical']=='supercritical']\n",
    "    symm_classic = symm[symm['supercritical'] != 'supercritical']\n",
    "    camb_supercrit = camb[camb['supercritical']=='supercritical']\n",
    "    camb_classic = camb[camb['supercritical'] != 'supercritical']\n",
    "    result1 = np.array([[len(symm_classic), len(symm_supercrit)], \n",
    "                        [len(camb_classic), len(camb_supercrit)]])\n",
    "\n",
    "    \n",
    "    f1, ax1 = plt.subplots()\n",
    "    im1 = ax1.imshow(result1, cmap='Blues')\n",
    "    ax1.set_yticks([0, 1], labels=['Symmetric', 'Cambered'])\n",
    "    ax1.set_xticks([0, 1], labels=['Classic', 'Supercritical'])\n",
    "    for i in range(0, 2):\n",
    "        for j in range(0, 2):\n",
    "            text = ax1.text(j, i, result1[i, j],\n",
    "                        ha=\"center\", va=\"center\", color=\"k\")\n",
    "    f1.colorbar(im1, ax=ax1)        \n",
    "    ax1.title.set_text('Breakdown of available data'  + '\\n Total: ' + str(np.sum(np.sum(result1))))\n",
    "    plt.show()\n",
    "    \n",
    "    # Category vs. Number at varying M \n",
    "    M_range = np.arange(0.0+0.05, 1.0, 0.1)\n",
    "    result2 = np.zeros((4, M_range.shape[0]))\n",
    "    iter_list_x = [symm_classic, camb_classic, symm_supercrit, camb_supercrit]\n",
    "    # iter_list_y1 = [symm_classic_true, camb_classic_true, symm_supercr_true, camb_supercr_true]\n",
    "    for i in range(0, len(iter_list_x)):\n",
    "        for j in range(0, M_range.shape[0]):\n",
    "            new_ind = np.abs(iter_list_x[i]['M']-M_range[j]) < 0.05\n",
    "            result2[i, j] = len(iter_list_x[i]['M'][new_ind])\n",
    "    \n",
    "    f2, ax2 = plt.subplots(figsize=(8,2))\n",
    "    im2 = ax2.imshow(result2, cmap='Blues')\n",
    "    ax2.set_yticks([0, 1, 2, 3], labels=['Symmetric, classic', 'Cambered, classic', 'Symmetric, supercritical', 'Cambered, supercritical'])\n",
    "    ax2.set_xticks(np.arange(0, M_range.shape[0]), labels=np.round(M_range-0.05, 2))\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, M_range.shape[0]):\n",
    "            text = ax2.text(j, i, int(result2[i, j]) if not np.isnan(result2[i,j]) else '',\n",
    "                        ha=\"center\", va=\"center\", color=\"k\")\n",
    "    f2.colorbar(im2, ax=ax2, label='MAE')        \n",
    "    ax2.title.set_text('Number of available training data vs. M' + '\\n Total: ' + str(int(np.sum(np.sum(result2)))))\n",
    "    ax2.set_xlabel('Mach number')\n",
    "    ax2.set_aspect('auto')\n",
    "    plt.show()\n",
    "    \n",
    "    # Category vs. Number at varying alpha \n",
    "    A_range = np.arange(-16.0, 31.0, 1.0)\n",
    "    result3 = np.zeros((4, A_range.shape[0]))\n",
    "    iter_list_x = [symm_classic, camb_classic, symm_supercrit, camb_supercrit]\n",
    "    for i in range(0, len(iter_list_x)):\n",
    "        for j in range(0, A_range.shape[0]):\n",
    "            new_ind = (iter_list_x[i]['alpha']-A_range[j] < 0.5) & (iter_list_x[i]['alpha']-A_range[j] >= -0.5)\n",
    "            result3[i, j] = len(iter_list_x[i]['alpha'][new_ind])\n",
    "            \n",
    "    f3, ax3 = plt.subplots(figsize=(27,2))\n",
    "    im3 = ax3.imshow(result3, cmap='Blues')\n",
    "    ax3.set_yticks([0, 1, 2, 3], labels=['Symmetric, classic', 'Cambered, classic', 'Symmetric, supercritical', 'Cambered, supercritical'])\n",
    "    ax3.set_xticks(np.arange(0, A_range.shape[0]), labels=np.round(A_range, 2))\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, A_range.shape[0]):\n",
    "            text = ax3.text(j, i, int(result3[i, j]) if not np.isnan(result3[i,j]) else '',\n",
    "                        ha=\"center\", va=\"center\", color=\"k\")\n",
    "    f3.colorbar(im3, ax=ax3, label='MAE')        \n",
    "    ax3.title.set_text('Number of available training data vs. ' + r'$\\alpha$' + '\\n Total: ' + str(int(np.sum(np.sum(result3)))))\n",
    "    ax3.set_aspect('auto')\n",
    "    ax3.set_xlabel('Angle of attack [deg] ')\n",
    "    plt.show()\n",
    "    \n",
    "    # By airfoil \n",
    "    f4, ax4 = plt.subplots()\n",
    "    result4 = np.array([[len(pd.unique(symm_classic['af'])), len(pd.unique(symm_supercrit['af']))], \n",
    "                        [len(pd.unique(camb_classic['af'])), len(pd.unique(camb_supercrit['af']))]])\n",
    "    im4 = ax4.imshow(result4, cmap='Blues')\n",
    "    ax4.set_yticks([0, 1], labels=['Symmetric', 'Cambered'])\n",
    "    ax4.set_xticks([0, 1], labels=['Classic', 'Supercritical'])\n",
    "    for i in range(0, 2):\n",
    "        for j in range(0, 2):\n",
    "            text = ax4.text(j, i, result4[i, j],\n",
    "                        ha=\"center\", va=\"center\", color=\"k\")\n",
    "    f4.colorbar(im4, ax=ax4)        \n",
    "    ax4.title.set_text('Breakdown of available data'  + '\\n Total airfoils: ' + str(np.sum(np.sum(result4))))\n",
    "    plt.show()\n",
    "    \n",
    "    return (f1, ax1), (f2, ax2), (f3, ax3), (f4, ax4)\n",
    "\n",
    "def accuracy_breakdown(model, train_y, train_x, train_category):\n",
    "    # Category vs. MAE\n",
    "    with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "        preds_model = model(train_x)\n",
    "    preds = preds_model.mean.cpu().detach().numpy()\n",
    "    train_y = train_y.cpu().detach().numpy()\n",
    "    train_x = train_x.cpu().detach().numpy()\n",
    "        # Create array \n",
    "    symm = preds[train_category['symmetry']=='symmetric']\n",
    "    symm_true = train_y[train_category['symmetry']=='symmetric']\n",
    "    symm_cat = train_category[train_category['symmetry']=='symmetric']\n",
    "    symm_x = train_x[train_category['symmetry']=='symmetric']\n",
    "    \n",
    "    camb = preds[train_category['symmetry']=='cambered']\n",
    "    camb_true = train_y[train_category['symmetry']=='cambered']\n",
    "    camb_cat = train_category[train_category['symmetry']=='cambered']\n",
    "    camb_x = train_x[train_category['symmetry']=='cambered']\n",
    "    \n",
    "    symm_supercr = symm[symm_cat['supercritical']=='supercritical']\n",
    "    symm_supercr_true = symm_true[symm_cat['supercritical']=='supercritical']\n",
    "    symm_classic = symm[symm_cat['supercritical']!='supercritical']\n",
    "    symm_classic_true = symm_true[symm_cat['supercritical']!='supercritical']\n",
    "    symm_supercr_x = symm_x[symm_cat['supercritical']=='supercritical']\n",
    "    symm_classic_x = symm_x[symm_cat['supercritical']!='supercritical']\n",
    "    \n",
    "    camb_supercr = camb[camb_cat['supercritical']=='supercritical']\n",
    "    camb_supercr_true = camb_true[camb_cat['supercritical']=='supercritical']\n",
    "    camb_classic = camb[camb_cat['supercritical']!='supercritical']\n",
    "    camb_classic_true = camb_true[camb_cat['supercritical']!='supercritical']\n",
    "    camb_supercr_x = camb_x[camb_cat['supercritical']=='supercritical']\n",
    "    camb_classic_x = camb_x[camb_cat['supercritical']!='supercritical']\n",
    "    \n",
    "    result1 = np.array([[np.mean(np.abs(symm_classic - symm_classic_true)), np.mean(np.abs(symm_supercr - symm_supercr_true))],\n",
    "                        [np.mean(np.abs(camb_classic - camb_classic_true)), np.mean(np.abs(camb_supercr - camb_supercr_true))]])\n",
    "    \n",
    "    f1, ax1 = plt.subplots()\n",
    "    im1 = ax1.imshow(result1, cmap='Reds')\n",
    "    ax1.set_yticks([0, 1], labels=['Symmetric', 'Cambered'])\n",
    "    ax1.set_xticks([0, 1], labels=['Classic', 'Supercritical'])\n",
    "    for i in range(0, 2):\n",
    "        for j in range(0, 2):\n",
    "            text = ax1.text(j, i, result1[i, j] if not np.isnan(result1[i,j]) else '',\n",
    "                        ha=\"center\", va=\"center\", color=\"k\")\n",
    "    f1.colorbar(im1, ax=ax1, label='MAE [100 $C_p$]')        \n",
    "    ax1.title.set_text('MAE of predictions on data')\n",
    "    plt.show()\n",
    "    \n",
    "    # Category vs. MAE at varying M \n",
    "    M_range = np.arange(0.0+0.05, 1.0, 0.1)\n",
    "    result2 = np.zeros((4, M_range.shape[0]))\n",
    "    iter_list_x = [symm_classic_x, camb_classic_x, symm_supercr_x, camb_supercr_x]\n",
    "    iter_list_y1 = [symm_classic_true, camb_classic_true, symm_supercr_true, camb_supercr_true]\n",
    "    iter_list_y2 = [symm_classic, camb_classic, symm_supercr, camb_supercr]\n",
    "    for i in range(0, len(iter_list_x)):\n",
    "        for j in range(0, M_range.shape[0]):\n",
    "            new_ind = (iter_list_x[i][:,-2]-M_range[j] < 0.05) & (iter_list_x[i][:,-2]-M_range[j] >= -0.05)#np.abs(iter_list_x[i][:,-2]-M_range[j]) <= 0.05\n",
    "            result2[i, j] = np.mean(np.abs(iter_list_y1[i][new_ind] - iter_list_y2[i][new_ind]))\n",
    "            \n",
    "    f2, ax2 = plt.subplots(figsize=(8,2))\n",
    "    im2 = ax2.imshow(result2, cmap='Reds')\n",
    "    ax2.set_yticks([0, 1, 2, 3], labels=['Symmetric, classic', 'Cambered, classic', 'Symmetric, supercritical', 'Cambered, supercritical'])\n",
    "    ax2.set_xticks(np.arange(0, M_range.shape[0]), labels=np.round(M_range-0.05, 2))\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, M_range.shape[0]):\n",
    "            text = ax2.text(j, i, np.round(result2[i, j], 2) if not np.isnan(result2[i,j]) else '',\n",
    "                        ha=\"center\", va=\"center\", color=\"k\")\n",
    "    f2.colorbar(im2, ax=ax2, label='MAE [100 $C_p$]')        \n",
    "    ax2.title.set_text('MAE of predictions on data vs. M')\n",
    "    ax2.set_xlabel('Mach number')\n",
    "    ax2.set_aspect('auto')\n",
    "    plt.show()\n",
    "    \n",
    "    # Category vs. MAE at varying alpha \n",
    "    A_range = np.arange(-16.0, 31.0, 1.0)\n",
    "    result3 = np.zeros((4, A_range.shape[0]))\n",
    "    iter_list_x = [symm_classic_x, camb_classic_x, symm_supercr_x, camb_supercr_x]\n",
    "    iter_list_y1 = [symm_classic_true, camb_classic_true, symm_supercr_true, camb_supercr_true]\n",
    "    iter_list_y2 = [symm_classic, camb_classic, symm_supercr, camb_supercr]\n",
    "    for i in range(0, len(iter_list_x)):\n",
    "        for j in range(0, A_range.shape[0]):\n",
    "            new_ind = (iter_list_x[i][:,-3]-A_range[j] < 0.5) & (iter_list_x[i][:,-3]-A_range[j] >= -0.5) #  np.abs(iter_list_x[i][:,-3]-A_range[j]) <= 0.5\n",
    "            result3[i, j] = np.mean(np.abs(iter_list_y1[i][new_ind] - iter_list_y2[i][new_ind]))\n",
    "            \n",
    "    f3, ax3 = plt.subplots(figsize=(27,2))\n",
    "    im3 = ax3.imshow(result3, cmap='Reds')\n",
    "    ax3.set_yticks([0, 1, 2, 3], labels=['Symmetric, classic', 'Cambered, classic', 'Symmetric, supercritical', 'Cambered, supercritical'])\n",
    "    ax3.set_xticks(np.arange(0, A_range.shape[0]), labels=np.round(A_range, 2))\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, A_range.shape[0]):\n",
    "            text = ax3.text(j, i, np.round(result3[i, j], 2) if not np.isnan(result3[i,j]) else '',\n",
    "                        ha=\"center\", va=\"center\", color=\"k\")\n",
    "    f3.colorbar(im3, ax=ax3, label='MAE [100 $C_p$]')        \n",
    "    ax3.title.set_text('MAE of predictions on data vs. ' + r'$\\alpha$')\n",
    "    ax3.set_aspect('auto')\n",
    "    ax3.set_xlabel('Angle of attack [deg] ')\n",
    "    plt.show()\n",
    "    \n",
    "    return (f1, ax1),  (f2, ax2),  (f3, ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3629229/3301568176.py:6: DtypeWarning: Columns (110) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_raw = pd.read_csv('./expData_20240415_removed.csv') # Read csv\n"
     ]
    }
   ],
   "source": [
    "# Useful variables \n",
    "random_test = False\n",
    "y_scale = 100\n",
    "\n",
    "# Read data \n",
    "data_raw = pd.read_csv('./expData_20240415_removed.csv') # Read csv \n",
    "num_xc = 26*2 # Number of points in x/c, upper and lower\n",
    "data_index = data_raw.columns[num_xc:num_xc+num_xc].append(data_raw.columns[[2*num_xc, 2*num_xc+2, 2*num_xc+1]])#data_raw.columns[52:52+52].append(data_raw.columns[[104, 106, 105]]) # Choose columns for training data \n",
    "af = data_raw['af']\n",
    "cat = data_raw[['symmetry', 'supercritical']]\n",
    "\n",
    "X = torch.Tensor(np.array(data_raw[data_index]))\n",
    "X[:, -3] = torch.deg2rad(X[:, -3])\n",
    "noise = (torch.Tensor(data_raw['noise'])*y_scale)**2 \n",
    "\n",
    "# Train - test split for airfoils \n",
    "af_unique = np.unique(af)\n",
    "if random_test: \n",
    "    # Randomly select test set \n",
    "    train_afu, test_afu = train_test_split(af_unique, test_size = .1, random_state = 1) \n",
    "elif random_test == False:\n",
    "    # Manual override of the test set \n",
    "    test_afu = ['RAE2822', 'SC1095','NACA0015']\n",
    "\n",
    "train_afu = np.delete(af_unique, np.argwhere(af_unique==test_afu[0]))\n",
    "train_afu = np.delete(train_afu, np.argwhere(train_afu==test_afu[1])) \n",
    "train_afu = np.delete(train_afu, np.argwhere(train_afu==test_afu[2])) \n",
    "\n",
    "train_ind = data_raw['af'].isin(train_afu).values\n",
    "test_ind = data_raw['af'].isin(test_afu).values\n",
    "\n",
    "# process y \n",
    "y = torch.Tensor(data_raw['Cp'])*y_scale # Scale up for numerical stability \n",
    "y_mean = torch.mean(y)\n",
    "y -= y_mean\n",
    "y_std = torch.std(y)\n",
    "\n",
    "# train - test split \n",
    "train_x, test_x, train_y, test_y, train_af, test_af = X[train_ind], X[test_ind], y[train_ind], y[test_ind], data_raw['af'][train_ind], data_raw['af'][test_ind]\n",
    "mean_x = torch.mean(train_x, axis = 0)\n",
    "std_x = torch.max(torch.abs(train_x), axis = 0)[0]\n",
    "\n",
    "train_noise, test_noise, train_cat, test_cat = noise[train_ind], noise[test_ind], cat[train_ind], cat[test_ind]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_x, train_y, test_x, test_y = train_x.to(output_device), train_y.to(output_device), test_x.to(output_device), test_y.to(output_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP Regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "nn_dims = [1000, 500, 50, 8] \n",
    "import math\n",
    "from gpytorch.priors import NormalPrior\n",
    "from gpytorch.constraints import Positive, GreaterThan\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, nn_dims[0]))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(nn_dims[0], nn_dims[1]))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(nn_dims[1], nn_dims[2]))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(nn_dims[2], nn_dims[3]))\n",
    "        if len(nn_dims) > 4:\n",
    "            self.add_module('relu4', torch.nn.ReLU())\n",
    "            self.add_module('linear5', torch.nn.Linear(nn_dims[3], nn_dims[4]))\n",
    "            \n",
    "from gpytorch.variational import CholeskyVariationalDistribution, DeltaVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "np_ind = np.random.choice(np.arange(0, train_x.shape[0]), size = 1000)\n",
    "# inducing_points = torch.Tensor(np.random.uniform(-1.0, 1.0, size=(800, num_xc+3)))\n",
    "inducing_points = train_x[np_ind]\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ApproximateGP):\n",
    "        def __init__(self, inducing_points):\n",
    "            variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "            variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True, jitter_val = 1e-4)\n",
    "            \n",
    "            super().__init__(variational_strategy)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=nn_dims[-1],\n",
    "                lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(\n",
    "                    math.exp(-1), math.exp(1), sigma=0.1, transform=torch.exp)\n",
    "            )\n",
    "            , outputscale_prior= NormalPrior(0.0, 10.0), outputscale_constraint=GreaterThan(1e-4) # <- changing scale but not having an effect?\n",
    "            )\n",
    " \n",
    "            self.feature_extractor = LargeFeatureExtractor()\n",
    "\n",
    "            # This module will scale the NN features so that they're nice values\n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1.0, 1.0)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\"\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Define model & likelihood  \n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=train_noise, learn_additional_noise=True) # <- adding additional noise for now\n",
    "# likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda() #  \n",
    "model = GPRegressionModel(inducing_points)\n",
    "\n",
    "# Push to CUDA \n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()\n",
    "\n",
    "# Define Optimizer  \n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=1e-2, weight_decay=1e-5) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoint or set up meta-information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint = True # Set this to true if you want to load the best weights\n",
    "if load_checkpoint:\n",
    "    loaded_checkpoint = torch.load('./checkpoints/20240415_checkpoint_40_loss_6.432229995727539_trainMSE_64.93795776367188_testMSE_529.1762084960938')\n",
    "    model.load_state_dict(loaded_checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(loaded_checkpoint['optimizer_state_dict'])\n",
    "else: \n",
    "    best_loss = float('inf') # loss\n",
    "    best_model_state_dict = None\n",
    "    iters = 0\n",
    "    iters_checkpt = 0\n",
    "    if 'history_values' in locals(): # history\n",
    "        del history_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.state_dict()['param_groups'][0]['lr'] = 1e-3\n",
    "# lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.9, last_epoch=-1)\n",
    "\n",
    "# Set up training iterations\n",
    "training_iterations = 1000*5#5000*5#4#*20\n",
    "interval_checkpt = 100\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Set up hyperparameters history\n",
    "if 'history_values' in locals():\n",
    "    history_values['train_MSE'] = np.concatenate((history_values['train_MSE'], np.zeros((training_iterations//interval_checkpt, ))), axis = None)\n",
    "    history_values['test_MSE'] = np.concatenate((history_values['test_MSE'], np.zeros((training_iterations//interval_checkpt, ))), axis = None)\n",
    "    history_values['mll'] = np.concatenate((history_values['mll'], np.zeros((1, training_iterations))), axis = None)\n",
    "else: \n",
    "    history_values = {\n",
    "        'test_MSE': np.zeros((int(training_iterations/interval_checkpt), )),\n",
    "        'train_MSE': np.zeros((int(training_iterations/interval_checkpt), )), \n",
    "        'mll': np.zeros((training_iterations, ))\n",
    "    }\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.shape[0]) \n",
    "\n",
    "# print(best_loss)\n",
    "import tqdm.notebook as tn \n",
    "def train_and_save_checkpoints(): \n",
    "    global best_loss, best_model_state_dict, best_optim_state_dict, iters, iters_checkpt\n",
    "    iterator = tn.tqdm(range(training_iterations))\n",
    "    for i in iterator:\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        \n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "     \n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        \n",
    "        # Calc loss and backprop derivatives\n",
    "        #loss_mse = torch.nn.MSELoss()\n",
    "        \n",
    "        loss = -mll(output, train_y)# + 10000*loss_mse(output.mean, train_y)\n",
    "        loss.backward()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "        history_values['mll'][iters] = loss.cpu().detach().numpy() \n",
    "        \n",
    "        # Save checkpoint \n",
    "        if (i+1)%interval_checkpt == 0:\n",
    "            model.eval()\n",
    "            likelihood.eval()\n",
    "            with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "                # Calculate MAE \n",
    "                validation_results = model(test_x)\n",
    "                train_results = model(train_x)\n",
    "                history_values['test_MSE'][iters_checkpt] = torch.mean((torch.abs(validation_results.mean - test_y))**2)\n",
    "                history_values['train_MSE'][iters_checkpt] = torch.mean((torch.abs(train_results.mean - train_y))**2)\n",
    "                \n",
    "                # Print Losses and MAE \n",
    "                print('Loss: {}'.format(history_values['mll'][iters]) + \n",
    "                    ' / Train MSE: {}'.format(history_values['train_MSE'][iters_checkpt]) + \n",
    "                    ' / Test MSE: {}'.format(history_values['test_MSE'][iters_checkpt]))\n",
    "                \n",
    "                # Save \n",
    "                fileDir = \"/home/hlee981/Documents/GP-Aero/BNN_af_pdist_gen/checkpoints/\" + \"20240415_checkpoint_\" + str(iters_checkpt) + \"_loss_\" + str(history_values['mll'][iters]) + '_trainMSE_' + str(history_values['train_MSE'][iters_checkpt]) + \\\n",
    "                    '_testMSE_' + str(history_values['test_MSE'][iters_checkpt])\n",
    "                    \n",
    "                save_checkpoint(model=model, optimizer=optimizer, history=history_values, fileName=fileDir, iters=iters)\n",
    "                iters_checkpt += 1 \n",
    "        optimizer.step()\n",
    "        iters += 1 \n",
    "    \n",
    "    validation_results = model(test_x)\n",
    "    train_results = model(train_x)\n",
    "    final_train_values = torch.mean(torch.abs(validation_results.mean - test_y))\n",
    "    final_test_values = torch.mean(torch.abs(train_results.mean - train_y))\n",
    "    fileDir = \"/home/hlee981/Documents/GP-Aero/BNN_af_pdist_gen/checkpoints/\" + \"20240415_checkpoint_\" + str(iters_checkpt) + \"_loss_\" + str(history_values['mll'][iters]) + '_trainMSE_' + str(final_train_values) + \\\n",
    "                    '_testMSE_' + str(final_test_values)\n",
    "    save_checkpoint(model=model, optimizer=optimizer, history=history_values, fileName=fileDir, iters=iters)\n",
    "\n",
    "%time train_and_save_checkpoints()\n",
    "\n",
    "\"\"\" BATCH TRAINING CODE \"\"\"\n",
    "# training_set = torch.utils.data.TensorDataset(train_x, train_y, train_noise)\n",
    "# test_set = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "# train_loader = torch.utils.data.DataLoader(training_set, batch_size=1024, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_set, batch_size=2, shuffle=False)\n",
    "# from tqdm.auto import tqdm\n",
    "# train_loader\n",
    "# def train(epoch):\n",
    "#     # model.train()\n",
    "#     # likelihood.train()\n",
    "\n",
    "#     minibatch_iter = tqdm(train_loader, desc=f\"(Epoch {epoch}) Minibatch\", leave=False, disable=True)\n",
    "#     with gpytorch.settings.num_likelihood_samples(8):\n",
    "       \n",
    "#         for data, target, noise_val in minibatch_iter:\n",
    "#             if torch.cuda.is_available():\n",
    "#                 data, target, noise_val = data.cuda(), target.cuda(), noise_val.cuda()\n",
    "#             likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise_val)\n",
    "#             mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=1024) \n",
    "#             model.train()\n",
    "#             likelihood.train()\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data)\n",
    "#             loss = -mll(output, target)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             minibatch_iter.set_postfix(loss=loss.item(), refresh=True)\n",
    "            \n",
    "#     return loss, minibatch_iter\n",
    "    \n",
    "# # %time train()\n",
    "\n",
    "# for epoch in range(1, training_iterations):\n",
    "#     with gpytorch.settings.use_toeplitz(False):\n",
    "#         loss, prog_bar = train(epoch)\n",
    "#         history_values['mll'][iters] = loss\n",
    "#         # Save checkpoint \n",
    "#         if (epoch+1)%interval_checkpt == 0:\n",
    "#             model.eval()\n",
    "#             likelihood.eval()\n",
    "#             with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "#                 # Calculate MAE \n",
    "#                 validation_results = model(test_x)\n",
    "#                 train_results = model(train_x)\n",
    "#                 history_values['test_MSE'][iters_checkpt] = torch.mean((torch.abs(validation_results.mean - test_y))**2)\n",
    "#                 history_values['train_MSE'][iters_checkpt] = torch.mean((torch.abs(train_results.mean - train_y))**2)\n",
    "                \n",
    "#                 # Print Losses and MAE \n",
    "#                 print('Loss: {}'.format(history_values['mll'][iters]) + \n",
    "#                     ' / Train MSE: {}'.format(history_values['train_MSE'][iters_checkpt]) + \n",
    "#                     ' / Test MSE: {}'.format(history_values['test_MSE'][iters_checkpt]))\n",
    "                \n",
    "#                 # Save \n",
    "#                 fileDir = \"/home/hlee981/Documents/GP-Aero/BNN_af_pdist_gen/checkpoints/\" + \"20240412_checkpoint_\" + str(iters_checkpt) + \"_loss_\" + str(history_values['mll'][iters]) + '_trainMSE_' + str(history_values['train_MSE'][iters_checkpt]) + \\\n",
    "#                     '_testMSE_' + str(history_values['test_MSE'][iters_checkpt])\n",
    "                    \n",
    "#                 save_checkpoint(model=model, optimizer=optimizer, history=history_values, fileName=fileDir, iters=iters)\n",
    "#                 iters_checkpt += 1 \n",
    "#         optimizer.step()\n",
    "#         iters += 1 \n",
    "        \n",
    "model.eval()\n",
    "likelihood.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histories of loss and test data MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,2, figsize=(12, 4))\n",
    "ax[0].semilogy(history_values['mll'][:training_iterations])\n",
    "ax[0].set_xlabel('Iterations')\n",
    "ax[0].set_ylabel('Marginal Log Likelihood')\n",
    "ax[0].title.set_text('Loss vs. Iterations')\n",
    "\n",
    "ax[1].plot(np.arange(0, training_iterations, interval_checkpt), history_values['test_MSE'][:],'o--', label='Test Data')\n",
    "ax[1].plot(np.arange(0, training_iterations, interval_checkpt), history_values['train_MSE'][:],'o--', label='Training Data')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Iterations')\n",
    "ax[1].set_ylabel('MAE')\n",
    "ax[1].title.set_text('MAE vs. Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.covar_module.outputscale.item())\n",
    "# print(model.covar_module.base_kernel.lengthscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions & obtain MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileDir = \"/home/hlee981/Documents/GP-Aero/BNN_af_pdist_gen/checkpoints/\" + \"checkpoint_\" + str(iters_checkpt) + \"_loss_\" + str(history_values['mll'][iters]) + '_trainMAE_' + str(history_values['train_MAE'][iters_checkpt]) + \\\n",
    "#                     '_testMAE_' + str(torch.mean(torch.abs(preds_test.mean - test_y)))\n",
    "# save_checkpoint(model=model, optimizer=optimizer, history=history_values, fileName=fileDir, iters=iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "subtest = True\n",
    "test_num = 1\n",
    "subtest_af = [test_afu[test_num]]\n",
    "\n",
    "subtest_x = test_x[test_af.isin(subtest_af).values]\n",
    "subtest_y = test_y[test_af.isin(subtest_af).values]\n",
    "\n",
    "if subtest:\n",
    "    with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "        preds_subtest = model(subtest_x)\n",
    "    print(subtest_af)\n",
    "    print('Subtest MAE: {}'.format(torch.mean(torch.abs(preds_subtest.mean - subtest_y))))\n",
    "else:   \n",
    "    with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "        preds_test = model(test_x)\n",
    "    print('Test MAE: {}'.format(torch.mean(torch.abs(preds_test.mean - test_y))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = alpha_vs_Mach(subtest_x, preds_subtest.mean, subtest_y, titleStr='MAE at various A, M')\n",
    "b = xc_vs_airfoil(subtest_x, preds_subtest.mean, subtest_y, titleStr='MAE at x/c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Cp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotter = [(6.0, test_afu[1], 0.60), (12.2, test_afu[0], 0.31), (0.0, test_afu[1], 0.81), (4.0, test_afu[1], 0.51)]#, (4.0, test_afu[1])]\n",
    "\n",
    "num_cases_visualize = 1\n",
    "\n",
    "seeds = np.arange(0, 10)\n",
    "for i in np.arange(0, 1): # \n",
    "    targetAF = test_afu[test_num]\n",
    "    \n",
    "    for j in np.arange(0, num_cases_visualize):\n",
    "        plt.figure()\n",
    "        temp_ind_af = np.where(test_af.values == targetAF)[0]\n",
    "        temp_arr = subtest_x#test_x[temp_ind_af, :]\n",
    "\n",
    "        # Get angle \n",
    "        A_list = torch.unique(temp_arr[:, -3])\n",
    "        if len(A_list) > 1:\n",
    "            targetA = A_list[torch.randint(low=0, high=len(A_list), size=(1,))][0].cpu()\n",
    "        else: \n",
    "            targetA = A_list[0]\n",
    "        temp_ind = temp_ind_af[torch.where(temp_arr[:, -3].cpu() == targetA)[0].numpy()]\n",
    "        temp_arr = temp_arr[torch.where(temp_arr[:, -3].cpu() == targetA)[0].numpy(), :]\n",
    "        \n",
    "        # Get Mach \n",
    "        M_list = torch.unique(temp_arr[:, -2]).cpu() \n",
    "        # print(M_list)\n",
    "        if len(M_list) > 1:\n",
    "            targetM = M_list[torch.randint(low=0, high=len(M_list), size=(1,))]\n",
    "        else: \n",
    "            targetM = M_list[0]\n",
    "        temp_ind = temp_ind[torch.where(temp_arr[:, -2].cpu() == targetM)[0].numpy()]\n",
    "        temp_arr = temp_arr[torch.where(temp_arr[:, -2].cpu() == targetM)[0].numpy(), :]\n",
    "    \n",
    "        \n",
    "        sample_airfoil_temp = np.tile(temp_arr[0,:-2].cpu(), (600,1))\n",
    "        desired_xc = np.linspace(-1, 1, 600).reshape((600,1))\n",
    "        desired_M = np.ones((600,1))*targetM.detach().numpy()\n",
    "        sample_airfoil = torch.Tensor(np.hstack((sample_airfoil_temp, desired_M, desired_xc))).cuda()\n",
    "    \n",
    "        # Posterior Predictive \n",
    "        with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "            sample_airfoil_pred = model(sample_airfoil)\n",
    "        sample_airfoil_std = np.sqrt(sample_airfoil_pred.variance.cpu())\n",
    "        sample_airfoil_mean = sample_airfoil_pred.mean.cpu() \n",
    "        # print(sample_airfoil_std)\n",
    "        # Plot experimental data \n",
    "        plt.errorbar(np.abs(test_x[temp_ind, -1].cpu()), (test_y[temp_ind].cpu() + y_mean)/y_scale, yerr=2*np.sqrt(noise[temp_ind])/y_scale, fmt='k.--', capsize=2, label='Experiment, $C_p \\pm 2\\sigma$')\n",
    "        \n",
    "        # plt.plot(np.abs(sample_airfoil[:, -1].cpu()), sample_airfoil_pred.sample(sample_shape=1) + torch.mean(y))\n",
    "        plt.fill_between(-desired_xc[:300].flatten(), (sample_airfoil_mean[:300]+ y_mean + sample_airfoil_std[:300])/y_scale, \n",
    "                         (sample_airfoil_mean[:300] + y_mean - sample_airfoil_std[:300])/y_scale, color = 'r', alpha = 0.3)\n",
    "        plt.fill_between(desired_xc[300:].flatten(), (sample_airfoil_mean[300:]+ y_mean + sample_airfoil_std[300:])/y_scale,\n",
    "                         (sample_airfoil_mean[300:]+ y_mean - sample_airfoil_std[300:])/y_scale, color = 'r', alpha = 0.3)\n",
    "        plt.fill_between(-desired_xc[:300].flatten(), (sample_airfoil_mean[:300] + y_mean + 2*sample_airfoil_std[:300])/y_scale, \n",
    "                         (sample_airfoil_mean[:300]+ y_mean - 2*sample_airfoil_std[:300])/y_scale, color = 'r', alpha = 0.2)\n",
    "        plt.fill_between(desired_xc[300:].flatten(), (sample_airfoil_mean[300:] + y_mean + 2*sample_airfoil_std[300:])/y_scale, \n",
    "                         (sample_airfoil_mean[300:]+ y_mean - 2*sample_airfoil_std[300:])/y_scale, color = 'r', alpha = 0.2)\n",
    "        # for j in np.arange(0, 5):\n",
    "        #     plt.plot(np.abs(sample_airfoil[:, -1].cpu()), newDist.sample()+ torch.mean(y), alpha = 0.2, color = 'r')\n",
    "        plt.plot(np.abs(sample_airfoil[:300+1, -1].cpu()), (sample_airfoil_mean + y_mean)[:300+1]/y_scale,'r', label='DKL GP, suction side $C_p \\pm 2\\sigma$')\n",
    "        plt.plot(np.abs(sample_airfoil[300:, -1].cpu()), (sample_airfoil_mean + y_mean)[300:]/y_scale,'r--', label='DKL GP, pressure side $C_p \\pm 2\\sigma$')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.xlabel('x/c')\n",
    "        plt.ylabel('$C_p$')\n",
    "        plt.legend()\n",
    "        plt.title(targetAF +'\\n' + r'$\\alpha$ = ' + str(np.round(np.rad2deg(targetA.detach().numpy()),2)) + r'$^\\circ,$' + r' $M_\\infty = $' + str(targetM.detach().numpy().flatten()[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cases_visualize = 5\n",
    "\n",
    "for i in np.arange(0, 1): # len(test_afu)\n",
    "    targetAF = train_afu[0]\n",
    "    \n",
    "    for j in np.arange(0, num_cases_visualize):\n",
    "        plt.figure()\n",
    "        temp_ind_af = np.where(train_af.values == targetAF)[0]\n",
    "        temp_arr = train_x[temp_ind_af, :]\n",
    "\n",
    "        # Get angle \n",
    "        A_list = torch.unique(temp_arr[:, -3])\n",
    "        targetA = A_list[torch.randint(low=0, high=len(A_list)-1, size=(1,))][0].cpu()\n",
    "        temp_ind = temp_ind_af[torch.where(temp_arr[:, -3].cpu() == targetA)[0].numpy()]\n",
    "        temp_arr = temp_arr[torch.where(temp_arr[:, -3].cpu() == targetA)[0].numpy(), :]\n",
    "        \n",
    "        # Get Mach \n",
    "        M_list = torch.unique(temp_arr[:, -2]).cpu() \n",
    "        if len(M_list) > 1:\n",
    "            targetM = M_list[torch.randint(low=0, high=len(M_list)-1, size=(1,))]\n",
    "        else:\n",
    "            targetM = M_list[0]\n",
    "    \n",
    "        temp_ind = temp_ind[torch.where(temp_arr[:, -2].cpu() == targetM)[0].numpy()]\n",
    "        temp_arr = temp_arr[torch.where(temp_arr[:, -2].cpu() == targetM)[0].numpy(), :]\n",
    "    \n",
    "        num_pts = 600\n",
    "        half_pt = int(num_pts/2)\n",
    "        sample_airfoil_temp = np.tile(temp_arr[0,:-2].cpu(), (num_pts,1))\n",
    "        desired_xc = np.linspace(-1, 1, num_pts).reshape((num_pts,1))\n",
    "        desired_M = np.ones((num_pts,1))*targetM.detach().numpy()\n",
    "        sample_airfoil = torch.Tensor(np.hstack((sample_airfoil_temp, desired_M, desired_xc))).cuda()\n",
    "\n",
    "        # Posterior Predictive \n",
    "        with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "            sample_airfoil_pred = model(sample_airfoil)\n",
    "        sample_airfoil_std = np.sqrt(sample_airfoil_pred.variance.cpu())\n",
    "        sample_airfoil_mean = sample_airfoil_pred.mean.cpu() \n",
    "        \n",
    "        # Plot experimental data \n",
    "        plt.errorbar(np.abs(train_x[temp_ind, -1].cpu()), (train_y[temp_ind].cpu() + torch.mean(y))/y_scale, yerr=2*np.sqrt(noise[temp_ind])/y_scale, fmt='k.--', capsize=2, label='Experiment, $C_p \\pm 2\\sigma$')\n",
    "        \n",
    "        # plt.plot(np.abs(sample_airfoil[:, -1].cpu()), sample_airfoil_pred.sample(sample_shape=1) + torch.mean(y))\n",
    "        plt.fill_between(-desired_xc[:half_pt].flatten(), (sample_airfoil_mean[:half_pt]+ torch.mean(y) + sample_airfoil_std[:half_pt])/y_scale, \n",
    "                         (sample_airfoil_mean[:half_pt]+ torch.mean(y) - sample_airfoil_std[:half_pt])/y_scale, color = 'r', alpha = 0.3)\n",
    "        plt.fill_between(desired_xc[half_pt:].flatten(), (sample_airfoil_mean[half_pt:]+ torch.mean(y) + sample_airfoil_std[half_pt:])/y_scale,\n",
    "                         (sample_airfoil_mean[half_pt:]+ torch.mean(y) - sample_airfoil_std[half_pt:])/y_scale, color = 'r', alpha = 0.3)\n",
    "        plt.fill_between(-desired_xc[:half_pt].flatten(), (sample_airfoil_mean[:half_pt]+ torch.mean(y) + 2*sample_airfoil_std[:half_pt])/y_scale, \n",
    "                         (sample_airfoil_mean[:half_pt]+ torch.mean(y) - 2*sample_airfoil_std[:half_pt])/y_scale, color = 'r', alpha = 0.2)\n",
    "        plt.fill_between(desired_xc[half_pt:].flatten(), (sample_airfoil_mean[half_pt:]+ torch.mean(y) + 2*sample_airfoil_std[half_pt:])/y_scale, \n",
    "                         (sample_airfoil_mean[half_pt:]+ torch.mean(y) - 2*sample_airfoil_std[half_pt:])/y_scale, color = 'r', alpha = 0.2)\n",
    "        # for j in np.arange(0, 5):\n",
    "        #     plt.plot(np.abs(sample_airfoil[:, -1].cpu()), newDist.sample()+ torch.mean(y), alpha = 0.2, color = 'r')\n",
    "        plt.plot(np.abs(sample_airfoil[:half_pt+1, -1].cpu()), (sample_airfoil_mean + torch.mean(y))[:half_pt+1]/y_scale,'r', label='DKL GP, suction side $C_p \\pm 2\\sigma$')\n",
    "        plt.plot(np.abs(sample_airfoil[half_pt:, -1].cpu()), (sample_airfoil_mean + torch.mean(y))[half_pt:]/y_scale,'r--', label='DKL GP, pressure side $C_p \\pm 2\\sigma$')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.xlabel('x/c')\n",
    "        plt.ylabel('$C_p$')\n",
    "        plt.legend()\n",
    "        plt.title(targetAF +'\\n' + r'$\\alpha$ = ' + str(np.round(np.rad2deg(targetA.detach().numpy()), 2)) + r'$^\\circ,$' + r' $M_\\infty = $' + str(targetM.detach().numpy().flatten()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent variables - dont use this for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent variable plotter 1 - Latent variables vs each other \n",
    "extracted_values = model.feature_extractor(train_x)\n",
    "extracted_values = model.scale_to_bounds(extracted_values)\n",
    "extracted_values = extracted_values.cpu().detach().numpy()\n",
    "\n",
    "N_var = nn_dims[3]\n",
    "f, ax = plt.subplots(N_var, N_var-1, figsize = (18, 18))\n",
    "for i in np.arange(0, N_var):\n",
    "    for j in np.arange(0, i):\n",
    "        for jj in np.arange(0, train_afu.shape[0]):\n",
    "            plot_ind = np.argwhere(train_af.values == train_afu[jj])\n",
    "            ax[i,j].scatter(extracted_values[plot_ind, i], extracted_values[plot_ind, j], marker='.',s=8)\n",
    "        if j == 0:\n",
    "            ax[i,j].set_ylabel('Variable ' + str(i+1), rotation=0, fontsize=12, labelpad = 30)\n",
    "        if i == N_var-1:\n",
    "            ax[i,j].set_xlabel('Variable ' + str(j+1), rotation=0, fontsize=12)\n",
    "        \n",
    "    for k in np.arange(i, N_var-1):\n",
    "        f.delaxes(ax[i,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent variable plotter 1 - Latent variables vs each other \n",
    "extracted_values = model.feature_extractor(train_x)\n",
    "extracted_values = model.scale_to_bounds(extracted_values)\n",
    "extracted_values = extracted_values.cpu().detach().numpy()\n",
    "\n",
    "N_var = nn_dims[3]\n",
    "f, ax = plt.subplots(N_var, N_var-1, figsize = (18, 18))\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(extracted_values[:, 0], extracted_values[:, 1], c=train_y.cpu(), marker='.', s=8)\n",
    "\n",
    "for i in np.arange(0, N_var):\n",
    "    for j in np.arange(0, i):\n",
    "        ff = ax[i,j].scatter(extracted_values[:, i], extracted_values[:, j], c=train_y.cpu(), marker='.', s=8)\n",
    "        \n",
    "        if j == 0:\n",
    "            ax[i,j].set_ylabel('Variable ' + str(i+1), rotation=0, fontsize=12, labelpad = 30)\n",
    "        if i == N_var-1:\n",
    "            ax[i,j].set_xlabel('Variable ' + str(j+1), rotation=0, fontsize=12)\n",
    "        \n",
    "    for k in np.arange(i, N_var-1):\n",
    "        f.delaxes(ax[i,k])\n",
    "        \n",
    "cbar = plt.colorbar(ff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_values = model.feature_extractor(train_x)\n",
    "extracted_values = model.scale_to_bounds(extracted_values)\n",
    "extracted_values = extracted_values.cpu().detach().numpy()\n",
    "\n",
    "N_var = nn_dims[3]\n",
    "f, ax = plt.subplots(N_var, N_var-1, figsize = (18, 18))\n",
    "for i in np.arange(0, N_var):\n",
    "    for j in np.arange(0, i):\n",
    "        ax[i,j].scatter(extracted_values[:, i], extracted_values[:, j], c=preds_train.mean.cpu(), marker='.', s=8)\n",
    "        \n",
    "        if j == 0:\n",
    "            ax[i,j].set_ylabel('Variable ' + str(i+1), rotation=0, fontsize=12, labelpad = 30)\n",
    "        if i == N_var-1:\n",
    "            ax[i,j].set_xlabel('Variable ' + str(j+1), rotation=0, fontsize=12)\n",
    "        \n",
    "    for k in np.arange(i, N_var-1):\n",
    "        f.delaxes(ax[i,k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cl Cd calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manual_Cl = -np.trapz(y=(sample_airfoil_pred.mean.cpu()+torch.mean(y))[:150], x=desired_xc.flatten()[:150]) + np.trapz(y=(sample_airfoil_pred.mean.cpu()+torch.mean(y))[150:], x=desired_xc.flatten()[150:])\n",
    "print(np.trapz(y=(sample_airfoil_pred.mean.cpu())[:150]+torch.mean(y), x=desired_xc.flatten()[:150]) + np.trapz(y=(sample_airfoil_pred.mean.cpu())[150:]+torch.mean(y), x=desired_xc.flatten()[150:]))\n",
    "# torch.linalg.inv(temp_Kxx) @ train_y.cpu()\n",
    "# print(model.covar_module(train_x))\n",
    "\n",
    "print(sample_airfoil[0,-2])\n",
    "def lazy_Cl_Cd_Cm(x, cp, a):\n",
    "    N = np.trapz(cp[:150], x=np.abs(x.flatten())[:150]) + np.trapz(cp[150:], x=x.flatten()[150:])\n",
    "    Cl = N * np.cos(np.deg2rad(a))\n",
    "    Cd = N * np.sin(np.deg2rad(a))\n",
    "    Cm = np.trapz((np.abs(x.flatten())[:150]-0.25) * cp[:150], x=np.abs(x.flatten())[:150]) + np.trapz((x.flatten()[150:]-0.25) * cp[150:], x=x.flatten()[150:]) # <- double check\n",
    "    return Cl, Cd, Cm  \n",
    "\n",
    "def GP_Cl_Cd_Cm(model, train_x, test_data):\n",
    "    model = model.cpu()\n",
    "    train_x = train_x.cpu()\n",
    "    test_data = test_data.cpu()\n",
    "    \n",
    "    projected_xtrain = model.feature_extractor(train_x)\n",
    "    projected_xtrain = model.scale_to_bounds(projected_xtrain)\n",
    "    projected_xtest = model.feature_extractor(test_data)\n",
    "    projected_xtest = model.scale_to_bounds(projected_xtest)\n",
    "    jitter = 1e-5 * torch.eye(train_x.shape[0])\n",
    "    Kxx = (model.covar_module(projected_xtrain).evaluate().cpu() + train_noise * torch.eye(train_x.shape[0]) + jitter).detach().numpy()\n",
    "    Kxs = model.covar_module(projected_xtrain, projected_xtest).evaluate().cpu().detach().numpy()\n",
    "\n",
    "    KxsT = Kxs.T\n",
    "    temp_ind = np.argwhere(np.array(desired_xc).flatten() < 0.0)\n",
    "    KxsT[temp_ind, :] *= -1\n",
    "    i_Kxs = np.trapz(y=KxsT, x=np.array(desired_xc).flatten(), axis=0).reshape((-1,1)).T\n",
    "    N = i_Kxs @ np.linalg.inv(Kxx) @ train_y.cpu().numpy()\n",
    "    a = test_data[0, -2].numpy()\n",
    "    print(N)\n",
    "    Cl = N * np.cos(np.deg2rad(a))\n",
    "    Cd = N * np.sin(np.deg2rad(a))\n",
    "    # Cm = np.trapz((np.abs(x.flatten())[:150]-0.25) * cp[:150], x=np.abs(x.flatten())[:150]) + np.trapz((x.flatten()[150:]-0.25) * cp[150:], x=x.flatten()[150:]) # <- double check\n",
    "    return Cl, Cd  \n",
    "\n",
    "lazy_Cl_Cd_Cm(desired_xc.flatten(), (sample_airfoil_pred.mean.cpu()+torch.mean(y)).detach().numpy(), a = 9.1)\n",
    "GP_Cl_Cd_Cm(model, train_x, sample_airfoil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_x_U = data_raw[data_raw.columns[:num_xc//2]].values[0]#np.flip(1-np.cos(np.linspace(0,np.pi/2,30)))\n",
    "ref_x_L = data_raw[data_raw.columns[num_xc//2:num_xc]].values[0]#1-np.cos(np.linspace(0,np.pi/2,30))\n",
    "ref_x = np.hstack((ref_x_U, ref_x_L))\n",
    "\n",
    "print(np.unique(af.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# af = ['NACA0012']\n",
    "import copy \n",
    "temp_x = copy.deepcopy(test_x.cpu()).detach().numpy()\n",
    "# plt.plot(ref_x, temp_x[0,:60],'.-')\n",
    "for i in range(0, len(test_afu)):\n",
    "    temp_ind_af = np.where(test_af.values == test_afu[i])\n",
    "    temp_arr = temp_x[temp_ind_af[0][0], :]\n",
    "    \n",
    "    plt.figure(figsize=(12,3))\n",
    "    plt.plot(ref_x, temp_arr[:num_xc] ,'.-')\n",
    "    plt.title(test_afu[i])\n",
    "    plt.ylim([-.1, .1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_noise)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
