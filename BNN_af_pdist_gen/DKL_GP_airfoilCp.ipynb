{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful variables \n",
    "random_test = False\n",
    "y_scale = 100\n",
    "\n",
    "# Set up GPU usage \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "n_devices = torch.cuda.device_count()\n",
    "output_device = torch.device('cuda:0')\n",
    "\n",
    "# Read data \n",
    "data_raw = pd.read_csv('./expData_more_20240320.csv') # Read csv \n",
    "data_index = data_raw.columns[52:52+52].append(data_raw.columns[[104, 106, 105]]) # Choose columns for training data \n",
    "af = data_raw['af']\n",
    "\n",
    "X = torch.Tensor(np.array(data_raw[data_index]))\n",
    "noise = (torch.Tensor(data_raw['noise'])*100)**2 # \n",
    "\n",
    "# Train - test split for airfoils \n",
    "af_unique = np.unique(af)\n",
    "if random_test: \n",
    "    # Randomly select test set \n",
    "    train_afu, test_afu = train_test_split(af_unique, test_size = .1, random_state = 1) \n",
    "elif random_test == False:\n",
    "    # Manual override of the test set \n",
    "    test_afu = ['NACA64A010', 'NACA64A406']\n",
    "print(test_afu) \n",
    "\n",
    "train_afu = np.delete(af_unique, np.argwhere(af_unique==test_afu[0]))\n",
    "train_afu = np.delete(train_afu, np.argwhere(train_afu==test_afu[1])) \n",
    "\n",
    "train_ind = data_raw['af'].isin(train_afu).values\n",
    "test_ind = data_raw['af'].isin(test_afu).values\n",
    "\n",
    "# process y \n",
    "y = torch.Tensor(data_raw['Cp'])*y_scale # Scale up for numerical stability \n",
    "y_mean = torch.mean(y)\n",
    "y_std = torch.std(y)\n",
    "\n",
    "# train - test split \n",
    "train_x = X[train_ind]\n",
    "test_x = X[test_ind]\n",
    "train_y = y[train_ind]\n",
    "test_y = y[test_ind]\n",
    "train_af = data_raw['af'][train_ind]\n",
    "test_af = data_raw['af'][test_ind]\n",
    "train_noise = noise[train_ind]\n",
    "test_noise = noise[test_ind]\n",
    "\n",
    "# push to cuda \n",
    "if torch.cuda.is_available():\n",
    "    train_x, train_y, test_x, test_y = train_x.to(output_device), train_y.to(output_device), test_x.to(output_device), test_y.to(output_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DKL GP model\n",
    "### NN Feature Extractor for dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "nn_dims = [1000, 500, 100, 8] \n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, nn_dims[0]))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(nn_dims[0], nn_dims[1]))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(nn_dims[1], nn_dims[2]))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(nn_dims[2], nn_dims[3]))\n",
    "        if len(nn_dims) > 4:\n",
    "            self.add_module('relu4', torch.nn.ReLU())\n",
    "            self.add_module('linear5', torch.nn.Linear(nn_dims[3], nn_dims[4]))\n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP Regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            \n",
    "            grid_size = 100#gpytorch.utils.grid.choose_grid_size(train_x)\n",
    "            # self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            #     gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=nn_dims[-1])),\n",
    "            #     num_dims=nn_dims[-1], grid_size=grid_size\n",
    "            # )\n",
    "            \n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.keops.MaternKernel(nu=0.5, ard_num_dims=nn_dims[-1]))\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "            # This module will scale the NN features so that they're nice values\n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1.0, 1.0)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\"\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Define model & likelihood \n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(train_noise)\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "\n",
    "# Push to CUDA \n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()\n",
    "\n",
    "# Define Optimizer \n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "], lr=1e-2) # \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load check or set up meta-information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint = False\n",
    "if load_checkpoint:\n",
    "    checkpoint = torch.load('1000_500_50_8_new')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    history_GP = checkpoint['hyperparams_trace']\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    iters = checkpoint['iterations']\n",
    "else: \n",
    "    best_loss = float('inf') # loss\n",
    "    best_model_state_dict = None\n",
    "    iters = 0\n",
    "\n",
    "    if 'history_GP' in locals(): # history/trace of GP hyperparams\n",
    "        del history_GP\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr'] = 1e-3\n",
    "optimizer.state_dict()['param_groups'][0]['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.state_dict()['param_groups'][0]['lr'] = 1e-3\n",
    "\n",
    "# Set up training iterations\n",
    "training_iterations = 500\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Set up hyperparameters history\n",
    "if 'history_GP' in locals():\n",
    "    history_GP = np.hstack((history_GP, np.zeros((nn_dims[-1], training_iterations))))\n",
    "else: \n",
    "    history_GP = np.zeros((nn_dims[-1], training_iterations))\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "print(best_loss)\n",
    "import tqdm.notebook as tn \n",
    "def train(): \n",
    "    global best_loss, best_model_state_dict, iters\n",
    "    iterator = tn.tqdm(range(training_iterations))\n",
    "    for i in iterator:\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        \n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_y)\n",
    "        \n",
    "        model.train()\n",
    "        # Check if current loss is the best so far <- not sure if working\n",
    "        \n",
    "        loss.backward()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "        \n",
    "        history_GP[:, iters] = optimizer.param_groups[1]['params'][1][0].cpu().detach().numpy() # place holder \n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model_state_dict = model.state_dict()\n",
    "            \n",
    "        iters += 1 \n",
    "        \n",
    "%time train()\n",
    "\n",
    "if best_model_state_dict is not None:\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    \n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Show trace\n",
    "for i in range(0, nn_dims[-1]):\n",
    "    plt.plot(history_GP[i, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions & obtain MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    preds_test = model(test_x)\n",
    "    \n",
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds_test.mean - test_y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveCheckpoint = False\n",
    "if saveCheckpoint:\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'hyperparams_trace': history_GP,\n",
    "                'best_loss': best_loss,\n",
    "                'iterations': iters\n",
    "                },\n",
    "            '1000_500_50_8_newFormat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,6))\n",
    "preds_std = np.sqrt(np.diag(preds_test.covariance_matrix.cpu()))\n",
    "\n",
    "dispNum = 32\n",
    "import random\n",
    "random.seed(30)\n",
    "# 2 \n",
    "# test_af[dispInds] + \n",
    "\n",
    "# dispInds = random.sample(range(1, test_x.cpu().shape[0]), dispNum)\n",
    "dispInds1 = random.sample(range(0, test_x.cpu().shape[0]), int(dispNum/2))\n",
    "dispInds2 = random.sample(range(0, 65), int(dispNum/2))\n",
    "dispInds = np.concatenate((dispInds1, dispInds2))\n",
    "# caseStr = \n",
    "\n",
    "afStr = np.array(test_af)[dispInds].flatten() # \n",
    "locStr = np.array(np.round(test_x.cpu(),3))[dispInds, -1].astype(str).flatten()\n",
    "AStr = np.array(test_x.cpu())[dispInds, -2].astype(str).flatten()\n",
    "\n",
    "\n",
    "finalStr = afStr + '\\n A=' + AStr + '\\n x/c=' + locStr\n",
    "\n",
    "\n",
    "plt.errorbar(np.arange(len(test_y[dispInds]))-0.2, test_y.cpu()[dispInds], yerr = np.sqrt(test_noise[dispInds]), capsize=2, label='Experimental $C_p \\pm \\sigma$', fmt='.', color = 'b', alpha =0.5)\n",
    "plt.bar(np.arange(len(test_y[dispInds]))-0.2, test_y.cpu()[dispInds], color='dodgerblue', width = 0.4)\n",
    "plt.errorbar(np.arange(len(test_y[dispInds]))+0.2, preds_test.mean.cpu()[dispInds],  yerr = preds_std[dispInds], label='Predicted $C_p \\pm \\sigma$', fmt='r.')\n",
    "plt.bar(np.arange(len(test_y[dispInds]))+0.2, preds_test.mean.cpu()[dispInds], color='crimson', width = 0.4, alpha =0.5, capsize=2)\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(0, dispNum), finalStr, fontsize=8);\n",
    "plt.ylabel('$C_p$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Cp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = [(6.0, test_afu[1], 0.6), (0.0, test_afu[1], 0.31), (16.2, test_afu[0], 0.52), (-1.8, test_afu[0], 0.81)]#, (4.0, test_afu[1])]\n",
    "print(test_afu)\n",
    "\n",
    "\n",
    "for i in np.arange(0, len(plotter)):#[6,7,8,9]:\n",
    "    plt.figure()\n",
    "    targetA = plotter[i][0]\n",
    "    targetAF = plotter[i][1]\n",
    "    targetM = plotter[i][2]\n",
    "    \n",
    "    tempA = torch.where(test_x[:, -3].cpu() == targetA)[0].numpy()\n",
    "    tempAF = np.where(test_af.values == targetAF)\n",
    "    tempM = torch.where(test_x[:, -2].cpu() == targetM)[0].numpy()\n",
    "\n",
    "    ind_search = np.intersect1d(np.intersect1d(tempA, tempAF), tempM)\n",
    "\n",
    "    sample_airfoil_temp = np.tile(test_x[ind_search[0],:-2].cpu(), (300,1))\n",
    "    desired_xc = np.linspace(-1, 1, 300).reshape((300,1))\n",
    "    desired_M = np.ones((300,1))*targetM\n",
    "    sample_airfoil = torch.Tensor(np.hstack((sample_airfoil_temp, desired_M, desired_xc))).cuda()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "        sample_airfoil_pred = model(sample_airfoil)\n",
    "    \n",
    "    sample_airfoil_std = np.sqrt(np.diag(sample_airfoil_pred.covariance_matrix.cpu()))\n",
    "    # newDist = torch.distributions.multivariate_normal.MultivariateNormal(sample_airfoil_pred.mean.cpu(), sample_airfoil_pred.covariance_matrix.cpu() + torch.eye(sample_airfoil_pred.covariance_matrix.cpu().shape[0])*1e-5)\n",
    "    \n",
    "    plt.errorbar(np.abs(test_x[ind_search, -1].cpu()), test_y[ind_search].cpu() + torch.mean(y), yerr=2*np.sqrt(noise[ind_search]), fmt='k.', capsize=2, label='Experiment, $C_p \\pm 2\\sigma$')\n",
    "    \n",
    "    # plt.plot(np.abs(sample_airfoil[:, -1].cpu()), sample_airfoil_pred.sample(sample_shape=1) + torch.mean(y))\n",
    "    plt.fill_between(-desired_xc[:150].flatten(), sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) + sample_airfoil_std[:150], sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) - sample_airfoil_std[:150], color = 'r', alpha = 0.3)\n",
    "    plt.fill_between(desired_xc[150:].flatten(), sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) + sample_airfoil_std[150:], sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) - sample_airfoil_std[150:], color = 'r', alpha = 0.3)\n",
    "    plt.fill_between(-desired_xc[:150].flatten(), sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) + 2*sample_airfoil_std[:150], sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) - 2*sample_airfoil_std[:150], color = 'r', alpha = 0.2)\n",
    "    plt.fill_between(desired_xc[150:].flatten(), sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) + 2*sample_airfoil_std[150:], sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) - 2*sample_airfoil_std[150:], color = 'r', alpha = 0.2)\n",
    "    # for j in np.arange(0, 5):\n",
    "    #     plt.plot(np.abs(sample_airfoil[:, -1].cpu()), newDist.sample()+ torch.mean(y), alpha = 0.2, color = 'r')\n",
    "    plt.plot(np.abs(sample_airfoil[:151, -1].cpu()), (sample_airfoil_pred.mean.cpu() + torch.mean(y))[:151],'r', label='DKL GP, suction side $C_p \\pm 2\\sigma$')\n",
    "    plt.plot(np.abs(sample_airfoil[150:, -1].cpu()), (sample_airfoil_pred.mean.cpu() + torch.mean(y))[150:],'r--', label='DKL GP, pressure side $C_p \\pm 2\\sigma$')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('x/c')\n",
    "    plt.ylabel('$C_p$')\n",
    "    plt.legend()\n",
    "    # plt.title(targetAF +'\\n' + r'$\\alpha$ = ' + str(targetA) + r'$^\\circ,$' + r' $M_\\infty = 0.60$')\n",
    "# test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent variables - dont use this for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent variable plotter 1 - Latent variables vs each other \n",
    "extracted_values = model.feature_extractor(train_x)\n",
    "extracted_values = model.scale_to_bounds(extracted_values)\n",
    "extracted_values = extracted_values.cpu().detach().numpy()\n",
    "\n",
    "N_var = nn_dims[3]\n",
    "f, ax = plt.subplots(N_var, N_var-1, figsize = (18, 18))\n",
    "for i in np.arange(0, N_var):\n",
    "    for j in np.arange(0, i):\n",
    "        for jj in np.arange(0, train_afu.shape[0]):\n",
    "            plot_ind = np.argwhere(train_af.values == train_afu[jj])\n",
    "            ax[i,j].scatter(extracted_values[plot_ind, i], extracted_values[plot_ind, j], marker='.',s=8)\n",
    "        if j == 0:\n",
    "            ax[i,j].set_ylabel('Variable ' + str(i+1), rotation=0, fontsize=12, labelpad = 30)\n",
    "        if i == N_var-1:\n",
    "            ax[i,j].set_xlabel('Variable ' + str(j+1), rotation=0, fontsize=12)\n",
    "        \n",
    "    for k in np.arange(i, N_var-1):\n",
    "        f.delaxes(ax[i,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent variable plotter 1 - Latent variables vs each other \n",
    "extracted_values = model.feature_extractor(train_x)\n",
    "extracted_values = model.scale_to_bounds(extracted_values)\n",
    "extracted_values = extracted_values.cpu().detach().numpy()\n",
    "\n",
    "N_var = nn_dims[3]\n",
    "f, ax = plt.subplots(N_var, N_var-1, figsize = (18, 18))\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(extracted_values[:, 0], extracted_values[:, 1], c=train_y.cpu(), marker='.', s=8)\n",
    "\n",
    "for i in np.arange(0, N_var):\n",
    "    for j in np.arange(0, i):\n",
    "        ff = ax[i,j].scatter(extracted_values[:, i], extracted_values[:, j], c=train_y.cpu(), marker='.', s=8)\n",
    "        \n",
    "        if j == 0:\n",
    "            ax[i,j].set_ylabel('Variable ' + str(i+1), rotation=0, fontsize=12, labelpad = 30)\n",
    "        if i == N_var-1:\n",
    "            ax[i,j].set_xlabel('Variable ' + str(j+1), rotation=0, fontsize=12)\n",
    "        \n",
    "    for k in np.arange(i, N_var-1):\n",
    "        f.delaxes(ax[i,k])\n",
    "        \n",
    "cbar = plt.colorbar(ff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_values = model.feature_extractor(train_x)\n",
    "extracted_values = model.scale_to_bounds(extracted_values)\n",
    "extracted_values = extracted_values.cpu().detach().numpy()\n",
    "\n",
    "N_var = nn_dims[3]\n",
    "f, ax = plt.subplots(N_var, N_var-1, figsize = (18, 18))\n",
    "for i in np.arange(0, N_var):\n",
    "    for j in np.arange(0, i):\n",
    "        ax[i,j].scatter(extracted_values[:, i], extracted_values[:, j], c=preds_train.mean.cpu(), marker='.', s=8)\n",
    "        \n",
    "        if j == 0:\n",
    "            ax[i,j].set_ylabel('Variable ' + str(i+1), rotation=0, fontsize=12, labelpad = 30)\n",
    "        if i == N_var-1:\n",
    "            ax[i,j].set_xlabel('Variable ' + str(j+1), rotation=0, fontsize=12)\n",
    "        \n",
    "    for k in np.arange(i, N_var-1):\n",
    "        f.delaxes(ax[i,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = [(10.0, train_afu[0]), (9.1, train_afu[-2])]#, (2.0, train_afu[6])]\n",
    "print(train_afu)\n",
    "for i in np.arange(0, len(plotter)):#[6,7,8,9]:\n",
    "    plt.figure()\n",
    "    targetA = plotter[i][0]\n",
    "\n",
    "    targetAF = plotter[i][1]\n",
    "    tempA = torch.where(train_x[:, -2].cpu() == targetA)[0].numpy()\n",
    "    tempAF = np.where(train_af.values == targetAF)\n",
    "    ind_search = np.intersect1d(tempA, tempAF)\n",
    "\n",
    "    sample_airfoil_temp = np.tile(train_x[ind_search[0],:-1].cpu(), (300,1))\n",
    "    desired_xc = np.linspace(-1, 1, 300).reshape((300,1))\n",
    "    sample_airfoil = torch.Tensor(np.hstack((sample_airfoil_temp, desired_xc))).cuda()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "        sample_airfoil_pred = model(sample_airfoil)\n",
    "    \n",
    "    sample_airfoil_std = np.sqrt(np.diag(sample_airfoil_pred.covariance_matrix.cpu()))\n",
    "    newDist = torch.distributions.multivariate_normal.MultivariateNormal(sample_airfoil_pred.mean.cpu(), sample_airfoil_pred.covariance_matrix.cpu() + torch.eye(sample_airfoil_pred.covariance_matrix.cpu().shape[0])*1e-5)\n",
    "    \n",
    "    plt.errorbar(np.abs(train_x[ind_search, -1].cpu()), train_y[ind_search].cpu() + torch.mean(y), yerr=2*np.sqrt(train_noise[ind_search]), fmt='k.', capsize=2, label='Experiment, $C_p \\pm 2\\sigma$')\n",
    "    \n",
    "    # plt.plot(np.abs(sample_airfoil[:, -1].cpu()), sample_airfoil_pred.sample(sample_shape=1) + torch.mean(y))\n",
    "    plt.fill_between(-desired_xc[:150].flatten(), sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) + sample_airfoil_std[:150], sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) - sample_airfoil_std[:150], color = 'r', alpha = 0.3)\n",
    "    plt.fill_between(desired_xc[150:].flatten(), sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) + sample_airfoil_std[150:], sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) - sample_airfoil_std[150:], color = 'r', alpha = 0.3)\n",
    "    plt.fill_between(-desired_xc[:150].flatten(), sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) + 2*sample_airfoil_std[:150], sample_airfoil_pred.mean.cpu()[:150]+ torch.mean(y) - 2*sample_airfoil_std[:150], color = 'r', alpha = 0.2)\n",
    "    plt.fill_between(desired_xc[150:].flatten(), sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) + 2*sample_airfoil_std[150:], sample_airfoil_pred.mean.cpu()[150:]+ torch.mean(y) - 2*sample_airfoil_std[150:], color = 'r', alpha = 0.2)\n",
    "    # for j in np.arange(0, 5):\n",
    "    #     plt.plot(np.abs(sample_airfoil[:, -1].cpu()), newDist.sample()+ torch.mean(y), alpha = 0.2, color = 'r')\n",
    "    plt.plot(np.abs(sample_airfoil[:, -1].cpu()), sample_airfoil_pred.mean.cpu() + torch.mean(y),'r', label='DKL GP, $C_p \\pm 2\\sigma$')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('x/c')\n",
    "    plt.ylabel('$C_p$')\n",
    "    plt.legend()\n",
    "    plt.title(targetAF +'\\n' + r'$\\alpha$ = ' + str(targetA) + r'$^\\circ,$' + r' $M_\\infty = 0.60$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cl Cd calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manual_Cl = -np.trapz(y=(sample_airfoil_pred.mean.cpu()+torch.mean(y))[:150], x=desired_xc.flatten()[:150]) + np.trapz(y=(sample_airfoil_pred.mean.cpu()+torch.mean(y))[150:], x=desired_xc.flatten()[150:])\n",
    "print(np.trapz(y=(sample_airfoil_pred.mean.cpu())[:150]+torch.mean(y), x=desired_xc.flatten()[:150]) + np.trapz(y=(sample_airfoil_pred.mean.cpu())[150:]+torch.mean(y), x=desired_xc.flatten()[150:]))\n",
    "# torch.linalg.inv(temp_Kxx) @ train_y.cpu()\n",
    "# print(model.covar_module(train_x))\n",
    "\n",
    "print(sample_airfoil[0,-2])\n",
    "def lazy_Cl_Cd_Cm(x, cp, a):\n",
    "    N = np.trapz(cp[:150], x=np.abs(x.flatten())[:150]) + np.trapz(cp[150:], x=x.flatten()[150:])\n",
    "    Cl = N * np.cos(np.deg2rad(a))\n",
    "    Cd = N * np.sin(np.deg2rad(a))\n",
    "    Cm = np.trapz((np.abs(x.flatten())[:150]-0.25) * cp[:150], x=np.abs(x.flatten())[:150]) + np.trapz((x.flatten()[150:]-0.25) * cp[150:], x=x.flatten()[150:]) # <- double check\n",
    "    return Cl, Cd, Cm  \n",
    "\n",
    "def GP_Cl_Cd_Cm(model, train_x, test_data):\n",
    "    model = model.cpu()\n",
    "    train_x = train_x.cpu()\n",
    "    test_data = test_data.cpu()\n",
    "    \n",
    "    projected_xtrain = model.feature_extractor(train_x)\n",
    "    projected_xtrain = model.scale_to_bounds(projected_xtrain)\n",
    "    projected_xtest = model.feature_extractor(test_data)\n",
    "    projected_xtest = model.scale_to_bounds(projected_xtest)\n",
    "    jitter = 1e-5 * torch.eye(train_x.shape[0])\n",
    "    Kxx = (model.covar_module(projected_xtrain).evaluate().cpu() + train_noise * torch.eye(train_x.shape[0]) + jitter).detach().numpy()\n",
    "    Kxs = model.covar_module(projected_xtrain, projected_xtest).evaluate().cpu().detach().numpy()\n",
    "\n",
    "    KxsT = Kxs.T\n",
    "    temp_ind = np.argwhere(np.array(desired_xc).flatten() < 0.0)\n",
    "    KxsT[temp_ind, :] *= -1\n",
    "    i_Kxs = np.trapz(y=KxsT, x=np.array(desired_xc).flatten(), axis=0).reshape((-1,1)).T\n",
    "    N = i_Kxs @ np.linalg.inv(Kxx) @ train_y.cpu().numpy()\n",
    "    a = test_data[0, -2].numpy()\n",
    "    print(N)\n",
    "    Cl = N * np.cos(np.deg2rad(a))\n",
    "    Cd = N * np.sin(np.deg2rad(a))\n",
    "    # Cm = np.trapz((np.abs(x.flatten())[:150]-0.25) * cp[:150], x=np.abs(x.flatten())[:150]) + np.trapz((x.flatten()[150:]-0.25) * cp[150:], x=x.flatten()[150:]) # <- double check\n",
    "    return Cl, Cd  \n",
    "\n",
    "lazy_Cl_Cd_Cm(desired_xc.flatten(), (sample_airfoil_pred.mean.cpu()+torch.mean(y)).detach().numpy(), a = 9.1)\n",
    "GP_Cl_Cd_Cm(model, train_x, sample_airfoil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.abs(desired_xc.flatten()), asdf + torch.mean(y).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
