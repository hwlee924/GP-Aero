{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt \n",
    "import os \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import r_pca # Credit to dganguli (https://github.com/dganguli/robust-pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useGPU = True # Use GPU?\n",
    "random_test = False # Leave this for now \n",
    "\n",
    "# For Model Training\n",
    "train_new = False # Training new, False if you want to load existing vars\n",
    "training_iterations = 5000 # Total training iterations\n",
    "swa_startpt = 3000 # When to start saving weights for Stochastic weight averaging \n",
    "interval_checkpt = 50 # Interval for internal MAE tracking \n",
    "swa_load_num = 2 # How many weights for SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GPU usage , 60 GB requiresd\n",
    "if useGPU == True: # 6 is for personal use \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    output_device = torch.device('cuda:0')\n",
    "    n_devices = torch.cuda.device_count()\n",
    "    output_device = torch.device('cuda:0')\n",
    "    print('Planning to run on {} GPUs'.format(n_devices))\n",
    "    \n",
    "elif useGPU == False: \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    output_device = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "else: \n",
    "    raise ValueError(\"Incorrect value of useGPU variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, history, iters, fileName):\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'history': history,\n",
    "                'iterations': iters\n",
    "                },\n",
    "            fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful variables \n",
    "random_test = False\n",
    "y_scale = 10\n",
    "\n",
    "# Read data \n",
    "\"\"\" Make sure to unpack the rar file\"\"\"\n",
    "data_raw = pd.read_csv('./data/ASPIRE_subsample_data.csv', low_memory=False) # Read csv \n",
    "\n",
    "# Pre-process data \n",
    "def detect_num(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "## Identify end of coordinates  \n",
    "for num_idx in range(data_raw.columns.shape[0]):\n",
    "    out_ = detect_num(data_raw.columns[num_idx])\n",
    "    if out_ == False:\n",
    "        num_xc_str = data_raw.columns[num_idx-1]\n",
    "        num_xc = int(num_xc_str.split('_')[-1])\n",
    "        break\n",
    "## Extract column var names for airfoil geometry (z)\n",
    "zu_str = []\n",
    "zl_str = []\n",
    "for i in range(1, num_xc+1):\n",
    "    zu_str.append('z_u_' + str(i))\n",
    "    zl_str.append('z_l_' + str(i))\n",
    "## Define remaining variables to be extracted\n",
    "rem_str = ['alpha', 'M', 'xc', 'yc'] # angle of attach, Mach, conformal x, conformal y\n",
    "input_idx = zu_str + zl_str + rem_str\n",
    "\n",
    "# Convert input data to tensor\n",
    "input_df = data_raw[input_idx]\n",
    "X = torch.Tensor(input_df.values)\n",
    "# X[:,-2] = torch.arcsin(X[:,-2] )\n",
    "X[:, -4] = torch.deg2rad(X[:, -4]) # Convert AoA to radians\n",
    "X[:, :-4] *= y_scale\n",
    "meanX = torch.mean(X, axis=0)\n",
    "meanX[:] = 0 # unnormalized in this case \n",
    "X -= meanX\n",
    "\n",
    "noise = (torch.Tensor(data_raw['noise'])*y_scale)**2 # Define noise values from experiments\n",
    "\n",
    "# Extract supplementary data\n",
    "af = data_raw['af']\n",
    "cat = data_raw[['symmetry', 'supercritical']]\n",
    "af_unique = np.unique(af)\n",
    "\n",
    "# Train - test split per airfoil \n",
    "if random_test: \n",
    "    # Randomly select test set \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_afu, test_afu = train_test_split(af_unique, test_size = .1, random_state = 1) # fix this later\n",
    "elif random_test == False:\n",
    "    # Manual override of the test set \n",
    "    test_afu = ['SC 1095','Supercritical airfoil 9a', 'NACA 63-415'] # 'RISO-A1-21'\n",
    "## Remove manually chosen af from af list\n",
    "train_afu = np.delete(af_unique, np.argwhere(af_unique==test_afu[0]))\n",
    "for i in range(1, len(test_afu)):\n",
    "    train_afu = np.delete(train_afu, np.argwhere(train_afu==test_afu[i])) \n",
    "## Identify corresponding indices\n",
    "train_idx = af.isin(train_afu).values\n",
    "test_idx = af.isin(test_afu).values\n",
    "## Only use subset?\n",
    "def get_subset(train_indices, test_indices, subset_bounds):\n",
    "    M_bounds, A_bounds = subset_bounds[0], subset_bounds[1]\n",
    "    if subset_bounds[0] is not None:\n",
    "        train_idx = np.logical_and(input_df['M'] <= M_bounds[1], train_indices)\n",
    "        train_idx = np.logical_and(input_df['M'] >= M_bounds[0], train_idx)\n",
    "        test_idx =  np.logical_and(input_df['M'] <= M_bounds[1], test_indices)\n",
    "        test_idx =  np.logical_and(input_df['M'] >= M_bounds[0], test_idx)\n",
    "    else:\n",
    "        train_idx = train_indices\n",
    "        test_idx = train_indices\n",
    "    if subset_bounds[1] is not None:\n",
    "        train_idx = np.logical_and(input_df['alpha'] <= A_bounds[1], train_idx)\n",
    "        train_idx = np.logical_and(input_df['alpha'] >= A_bounds[0], train_idx)\n",
    "        test_idx =  np.logical_and(input_df['alpha'] <= A_bounds[1], test_idx)\n",
    "        test_idx =  np.logical_and(input_df['alpha'] >= A_bounds[0], test_idx)\n",
    "    return train_idx, test_idx\n",
    "train_idx, test_idx = get_subset(train_idx, test_idx, [(0.0, 0.73), (-4.0, 12.0)])\n",
    "\n",
    "# Pre-process training targets \n",
    "y = torch.Tensor(data_raw['Cp'].values) * y_scale\n",
    "y_mean = torch.mean(y)\n",
    "y -= y_mean\n",
    "\n",
    "# Train - test split \n",
    "train_x, test_x, train_y, test_y, train_af, test_af = X[train_idx], X[test_idx], y[train_idx], y[test_idx], data_raw['af'][train_idx], data_raw['af'][test_idx]\n",
    "train_noise, test_noise, train_cat, test_cat = noise[train_idx], noise[test_idx], cat[train_idx], cat[test_idx]\n",
    "## push to output device \n",
    "train_x, train_y  = train_x.to(output_device), train_y.to(output_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DKL GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.priors import NormalPrior\n",
    "\n",
    "data_dim = train_x.size(-1)\n",
    "nn_dims = [1000, 1000, 500, 50, 10] \n",
    "\n",
    "# Define model \n",
    "## Main DKL Model\n",
    "class DKL_GP(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(DKL_GP, self).__init__(train_x, train_y, likelihood)\n",
    "            \n",
    "            # Mean Module\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            \n",
    "            # Covariance module \n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.keops.MaternKernel(nu=5/2, ard_num_dims=nn_dims[-1]),\n",
    "                )\n",
    "\n",
    "            # NN Feature Extractor Module\n",
    "            self.feature_extractor = LargeFeatureExtractor()\n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1.0, 1.0) # scale the feature extractor outputs \n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            projected_x = self.feature_extractor(x) # .to(output_device)\n",
    "            projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\" .to(output_device)\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "## Feature Extractor \n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, nn_dims[0])) # __\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('dropout1', torch.nn.Dropout(0.2))\n",
    "        self.add_module('linear2', torch.nn.Linear(nn_dims[0], nn_dims[1]))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('dropout2', torch.nn.Dropout(0.2))\n",
    "        self.add_module('linear3', torch.nn.Linear(nn_dims[1], nn_dims[2]))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('dropout3', torch.nn.Dropout(0.2))\n",
    "        self.add_module('linear4', torch.nn.Linear(nn_dims[2], nn_dims[3]))\n",
    "        if len(nn_dims) > 4:\n",
    "            self.add_module('relu4', torch.nn.ReLU())\n",
    "            self.add_module('dropout4', torch.nn.Dropout(0.2))\n",
    "            self.add_module('linear5', torch.nn.Linear(nn_dims[3], nn_dims[4]))\n",
    "        \n",
    "# Define model & likelihood  \n",
    "noise_prior = NormalPrior(0.0, 1.0)\n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=train_noise, learn_additional_noise=True, noise_prior=noise_prior) \n",
    "likelihood.second_noise = 0.3\n",
    "model = DKL_GP(train_x, train_y, likelihood)\n",
    "## Push to output device \n",
    "model = model.to(output_device)\n",
    "model.feature_extractor = model.feature_extractor.to(output_device)\n",
    "likelihood = likelihood.to(output_device)\n",
    "\n",
    "# Define Optimizer  \n",
    "lr = 1e-3\n",
    "decay = 1e-4 \n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "], lr=lr) #  , weight_decay=decay\n",
    "from torch.optim.lr_scheduler import StepLR, ConstantLR\n",
    "scheduler = StepLR(optimizer, step_size=1000, gamma = 0.5)#ConstantLR(optimizer, factor=1.0, total_iters=800)\n",
    "\n",
    "# Set up run or load checkpoint \n",
    "load_checkpoint_bool = False # Set this to true if you want to load the best weights\n",
    "\n",
    "def load_checkpoint(file_path, model, optimizer):\n",
    "    checkpt = torch.load(file_path, map_location=output_device) \n",
    "    model.load_state_dict(checkpt['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpt['optimizer_state_dict'])\n",
    "    history_values = checkpt['history']\n",
    "    return history_values\n",
    "\n",
    "def calc_err(true_val, pred_val, err_type = 'MAE'):\n",
    "        if err_type == 'MAE':\n",
    "            err = torch.mean(torch.abs(true_val - pred_val))\n",
    "        if err_type == 'MSE':\n",
    "            # not implemented yet \n",
    "            err = 1\n",
    "        return err\n",
    "    \n",
    "best_loss = float('inf') # loss\n",
    "best_model_state_dict = None\n",
    "iters = 0\n",
    "iters_checkpt = 0\n",
    "if 'history_values' in locals(): # history\n",
    "    del history_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the very large covariance matrix, approx 60 GB GPU is required to train the model fully.\n",
    "import tqdm.notebook as tn \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Set up hyperparameters history\n",
    "if 'history_values' not in locals():\n",
    "    history_values = {\n",
    "        'test_err': np.array([]),\n",
    "        'train_err': np.array([]),\n",
    "        'mll': np.array([]),\n",
    "        'mll_all': np.array([]),\n",
    "        'n': np.array([])\n",
    "    }\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model) \n",
    "save_tag = 'output'\n",
    "\n",
    "mem = []\n",
    "mem_model = []\n",
    "save_pt_mll = []\n",
    "save_pt_test = []\n",
    "save_pt_train = []\n",
    "save_pt_iter = []\n",
    "\n",
    "best_loss = float('inf') # loss\n",
    "best_model_state_dict = None\n",
    "iters = 0\n",
    "iters_checkpt = 0\n",
    "\n",
    "def train_and_save_checkpoints(begin_cycle): \n",
    "    global best_loss, best_model_state_dict, best_optim_state_dict, iters, iters_checkpt, mem, scheduler\n",
    "    sub_iter = 0\n",
    "    trpz = []\n",
    "    iterator = tn.tqdm(range(training_iterations))\n",
    "\n",
    "    for i in iterator:\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        \n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "     \n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        \n",
    "        # Calculate loss \n",
    "        loss = -mll(output, train_y)  \n",
    "        loss.backward()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "        history_values['mll_all'] = np.append(history_values['mll_all'], loss.cpu().detach().numpy())\n",
    "        \n",
    "        # Save checkpoint \n",
    "        if (i+1)%interval_checkpt == 0:\n",
    "            model.eval()\n",
    "            likelihood.eval()\n",
    "            with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "                # Calculate MAE \n",
    "                validation_results = model(test_x.to(output_device)) # temp_model(temp_test)#\n",
    "                train_results = model(train_x) #  temp_model(temp_train)#\n",
    "                history_values['mll'] = np.append(history_values['mll'], loss.cpu().detach().numpy())\n",
    "                history_values['test_err'] = np.append(history_values['test_err'], calc_err(validation_results.mean.cpu(), test_y)) # Force cpu \n",
    "                history_values['train_err'] = np.append(history_values['train_err'], calc_err(train_results.mean.cpu(), train_y.cpu())) \n",
    "                history_values['n'] = np.append(history_values['n'], iters)\n",
    "                \n",
    "                # Print Losses and MAE \n",
    "                print('Loss: {}'.format(history_values['mll'][iters_checkpt]) + \n",
    "                    ' / Train MAE: {}'.format(history_values['train_err'][iters_checkpt]) + \n",
    "                    ' / Test MAE: {}'.format(history_values['test_err'][iters_checkpt]) + \n",
    "                    ' / Noise: {}'.format(np.sqrt(likelihood.second_noise_covar.noise.item())))\n",
    "                mem.append(history_values['test_err'][iters_checkpt])\n",
    "                mem_model.append([model, optimizer, history_values, iters])\n",
    "                trpz.append(torch.abs(torch.trapz(validation_results.mean.cpu()) - torch.trapz(test_y)))\n",
    "                # Continuously update minimum \n",
    "                \n",
    "                min_idx = np.argmin(mem)\n",
    "                min_idx_sub = len(mem)-1-np.argmin(mem)\n",
    "                fileDir = './' + gen_save_string(save_tag, lr, 0.0, nn_dims, (history_values['mll'][iters_checkpt-min_idx_sub], history_values['train_err'][iters_checkpt-min_idx_sub], \n",
    "                                                                                            history_values['test_err'][iters_checkpt-min_idx_sub]), iters_checkpt-min_idx_sub)\n",
    "                \n",
    "                if sub_iter >= 199:\n",
    "                    save_pt_mll.append(history_values['mll'][iters_checkpt-min_idx_sub])\n",
    "                    save_pt_train.append(history_values['train_err'][iters_checkpt-min_idx_sub])\n",
    "                    save_pt_test.append(history_values['test_err'][iters_checkpt-min_idx_sub])\n",
    "                    save_pt_iter.append(iters_checkpt-min_idx_sub)\n",
    "                    save_checkpoint(model=mem_model[min_idx][0], optimizer=mem_model[min_idx][1], history=mem_model[min_idx][2], fileName=fileDir, iters=mem_model[min_idx][3])\n",
    "                    optimizer.param_groups[0]['lr'] = 1e-3\n",
    "                    scheduler = StepLR(optimizer, step_size=500, gamma=0.999)\n",
    "                    mem, trpz = [], []\n",
    "                    sub_iter = 0 \n",
    "                    print('New cycle')\n",
    "\n",
    "                iters_checkpt += 1 \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i >= begin_cycle:\n",
    "            sub_iter += 1 \n",
    "        iters += 1 \n",
    "\n",
    "if train_new:\n",
    "    # Run Training \n",
    "    train_and_save_checkpoints(swa_startpt)\n",
    "\n",
    "    # Plot Results \n",
    "    f, ax = plt.subplots(1,2, figsize=(12, 4))\n",
    "    # Loss vs iterations\n",
    "    ax[0].semilogy(history_values['mll_all'])\n",
    "    ax[0].set_xlabel('Iterations')\n",
    "    ax[0].set_ylabel('Marginal Log Likelihood')\n",
    "    ax[0].title.set_text('Loss vs. Iterations')\n",
    "\n",
    "    # Validation error vs iterations\n",
    "    ax[1].semilogy(np.arange(interval_checkpt, iters+1, interval_checkpt), np.array(history_values['test_err'][:]),'--', label='Test Data')\n",
    "    ax[1].legend()\n",
    "\n",
    "    # Labels\n",
    "    ax[1].set_xlabel('Iterations')\n",
    "    ax[1].set_ylabel('MAE')\n",
    "    ax[1].title.set_text('MAE vs. Iterations')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis sub-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_data(airfoil, alpha, mach, num_points = 601, manual_point = None, out_x=False):\n",
    "    global af, data_raw, meanX\n",
    "    if manual_point is not None:\n",
    "        num_points = manual_point.shape[0]\n",
    "\n",
    "    if isinstance(airfoil, str):\n",
    "        # if string, read airfoil info from training data\n",
    "        target_af_idx = np.argwhere(af.isin([airfoil]).values)[0]\n",
    "        target_af_geom = torch.Tensor(data_raw[zu_str+zl_str].values[target_af_idx, :])*y_scale - meanX[:28*2]\n",
    "        target_af_geom = target_af_geom.tile((num_points,1))\n",
    "        \n",
    "    else:\n",
    "        # manual loading, implement later\n",
    "        target_xloc_u = torch.Tensor(airfoil[0]).reshape((1,-1)) #np.flip([0, 0.25, 0.75, 1.0, 1.5, 2.0, 2.5, 5.0, 7.5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100])/100\n",
    "        target_xloc_l = torch.Tensor(airfoil[1]).reshape((1,-1))#np.array([0, 0.25, 0.75, 1.0, 1.5, 2.0, 2.5, 5.0, 7.5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]/100)\n",
    "        target_af_geom = torch.hstack((target_xloc_u, target_xloc_l))*y_scale - meanX[:28*2]\n",
    "        target_af_geom = target_af_geom.tile((num_points,1))\n",
    "    # Final output \n",
    "    if manual_point is None:\n",
    "        # Define others\n",
    "        xc_u = torch.linspace(-1, 1, num_points//2+1).reshape((num_points//2+1,1)) #torch.linspace(-1.0, 1.0, num_points//2).reshape((num_points//2,1)) # raw x/c\n",
    "        xc_l = torch.linspace((1.0 - (xc_u[1].item()-xc_u[0].item())), -1, num_points//2).reshape((num_points//2,1))\n",
    "        # theta = torch.arccos(xc)#torch.arccos(2*torch.abs(xc)-1) # conformal mapping angle\n",
    "        yc_u = torch.sin(torch.arccos(xc_u))#**2 # y/c\n",
    "        yc_l = -torch.sin(torch.arccos(xc_l))#**2# y/c\n",
    "        a = torch.ones((num_points, 1))*alpha# angle of attack\n",
    "        m = torch.ones((num_points, 1))*mach# mach\n",
    "    \n",
    "        test_array = torch.hstack((target_af_geom, a, m, torch.vstack((xc_u, xc_l)), torch.vstack((yc_u, yc_l)))).to(output_device) # torch.vstack((xc, xc))\n",
    "    else: \n",
    "        a = torch.ones((num_points, 1))*alpha # angle of attack\n",
    "        m = torch.ones((num_points, 1))*mach # mach\n",
    "        test_array = torch.hstack((target_af_geom, a, m, manual_point[:, 0].reshape((-1,1)), manual_point[:, 1].reshape((-1,1)))).to(output_device)\n",
    "    \n",
    "    if out_x == False:\n",
    "        return test_array \n",
    "    elif out_x == True:\n",
    "        return test_array, (xc_u, xc_l)\n",
    "\n",
    "def plot_posterior(xc, posterior_predictive, scale = 1, mean = 0, cutoff = None, color='r'):\n",
    "    f, ax = plt.subplots(1,1)\n",
    "    \n",
    "    # obtain cutoff \n",
    "    if cutoff is None:\n",
    "        cutoff = xc.shape[0]//2\n",
    "    \n",
    "    xc_u, xc_l = (xc[:cutoff].cpu()+1)/2, (xc[cutoff:].cpu()+1)/2#xc[cutoff:].cpu()\n",
    "    \n",
    "    post_mean, post_std = posterior_predictive.mean.cpu(), torch.sqrt(torch.diag(posterior_predictive.covariance_matrix.cpu()))\n",
    "    mu_u, mu_l = (post_mean[:cutoff] + mean)/scale, (post_mean[cutoff:] + mean)/scale\n",
    "    std_u, std_l = post_std[:cutoff]/scale, post_std[cutoff:]/scale\n",
    "    \n",
    "    # Plot posterior mean\n",
    "    ax.plot(xc_u, mu_u, color=color, linestyle='-', label = 'DKL GP model', linewidth=2.0)\n",
    "    ax.plot(xc_l, mu_l, color=color, linestyle='--')\n",
    "    ax.fill_between(xc_u, mu_u - 2*std_u, mu_u + 2*std_u, color='lightgray', label = 'Predicted $2\\sigma$', linewidth=2.0)\n",
    "    ax.fill_between(xc_l, mu_l - 2*std_l, mu_l + 2*std_l, color='lightgray', linewidth=2.0)\n",
    "    # ax.set_ylim([-3.0, 1.5])\n",
    "    return (f, ax), (xc_u, xc_l), (mu_u, mu_l), (std_u, std_l)\n",
    "\n",
    "def aggregate_posterior(test_data, weights_list, get_cl=False):\n",
    "    n_ = len(weights_list)\n",
    "    mean = torch.zeros(test_data.shape[0])\n",
    "    covar = torch.zeros(test_data.shape[0], test_data.shape[0]) \n",
    "    for n in range(0, n_):\n",
    "        load_checkpoint('./weights/' + weights_list[n], model, optimizer)\n",
    "        with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "            # print(test_data.get_device())\n",
    "            preds = model(test_data)\n",
    "            mean += preds.mean.cpu()\n",
    "            covar += preds.covariance_matrix.cpu()\n",
    "            \n",
    "            if get_cl:\n",
    "                1 \n",
    "    \n",
    "    mean = mean/n_\n",
    "    covar = covar/n_\n",
    "    agg_posterior_predictive_dist = gpytorch.distributions.MultivariateNormal(mean, covar + torch.eye(mean.shape[0])*1e-4)\n",
    "    \n",
    "    if get_cl:\n",
    "        return 1\n",
    "    elif get_cl == False:\n",
    "        return agg_posterior_predictive_dist \n",
    "\n",
    "def agg_covar(train_data, test_data, weights_list, get_kxx = True):\n",
    "    n_ = len(weights_list)\n",
    "    # mean = torch.zeros(test_data.shape[0])\n",
    "    Kxx = torch.zeros(train_data.shape[0], train_data.shape[0])\n",
    "    Kxs = torch.zeros(train_data.shape[0], test_data.shape[0])\n",
    "    Kss = torch.zeros(test_data.shape[0], test_data.shape[0])\n",
    "    for n in range(0, n_):\n",
    "        load_checkpoint('./weights/' + weights_list[n], model, optimizer)\n",
    "        projected_xtrain = model.feature_extractor(train_data)\n",
    "        projected_xtrain = model.scale_to_bounds(projected_xtrain)\n",
    "        projected_xtest = model.feature_extractor(test_data)\n",
    "        projected_xtest = model.scale_to_bounds(projected_xtest)\n",
    "        \n",
    "        with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "            jitter = 1e-4 * torch.eye(train_x.shape[0])\n",
    "            if get_kxx:\n",
    "                Kxx += (model.covar_module(projected_xtrain).evaluate().cpu() + torch.diag(likelihood.noise.cpu()) + jitter).detach().numpy()\n",
    "            Kxs += (model.covar_module(projected_xtrain, projected_xtest).evaluate().cpu()).detach().numpy()\n",
    "            Kss += (model.covar_module(projected_xtest, projected_xtest).evaluate().cpu()).detach().numpy()\n",
    "    Kxx = Kxx/n_\n",
    "    Kxs = Kxs/n_\n",
    "    Kss = Kss/n_\n",
    "    if get_kxx:\n",
    "        return Kxx, Kxs, Kss\n",
    "    else: \n",
    "        return Kxs, Kss\n",
    "\n",
    "def aggregate_latent_vars(test_data, weights_list):\n",
    "    n_ = len(weights_list)\n",
    "    mean = torch.zeros((test_data.shape[0], nn_dims[-1]))\n",
    "    for n in range(0, n_):\n",
    "        load_checkpoint('./weights/' + weights_list[n], model, optimizer)\n",
    "        with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "            preds = model.feature_extractor(test_data)\n",
    "            preds = model.scale_to_bounds(preds)\n",
    "            mean += preds.cpu()\n",
    "    \n",
    "    mean = mean/n_\n",
    "    return mean \n",
    "\n",
    "def validate_predictions(x_data, y_data, af_df, test_case, num_cases, weights_list = None, save_figs = False):\n",
    "    input_var = []\n",
    "    output_var = []\n",
    "    \n",
    "    af_u = af_df.unique()\n",
    "    subtest_af = [af_u[test_case]]\n",
    "    subtest_x = x_data[af_df.isin(subtest_af).values]\n",
    "    subtest_y = y_data[af_df.isin(subtest_af).values]\n",
    "\n",
    "    # Evaluate\n",
    "    if weights_list is None:\n",
    "        weights_list = []\n",
    "        for n in range(0, 5):\n",
    "            weights_list.append(gen_save_string(save_tag, lr, decay, nn_dims, (history_values['mll'][iters_checkpt-1-n], \n",
    "                                                                    history_values['train_err'][iters_checkpt-1-n], \n",
    "                                                                    history_values['test_err'][iters_checkpt-1-n]), \n",
    "                                    iters_checkpt-1-n))\n",
    "    preds_subtest = aggregate_posterior(subtest_x.to(output_device), weights_list)\n",
    "    print('Evaulating posterior predictive distribution for airfoil ' + subtest_af[0] + '...')\n",
    "    print('Subtest MAE: {}'.format(calc_err(preds_subtest.mean.cpu(), subtest_y.cpu())))\n",
    "\n",
    "    targetAF = af_u[test_case]\n",
    "        \n",
    "    temp_ind_af = np.where(af_df.values == targetAF)[0]\n",
    "    temp_arr = x_data[temp_ind_af]\n",
    "\n",
    "    unique_AM_pair = np.unique(temp_arr[:,-4:-2], axis=0)\n",
    "    if num_cases > unique_AM_pair.shape[0]:\n",
    "        num_cases = unique_AM_pair.shape[0]\n",
    "        \n",
    "    for j in np.arange(0, num_cases):\n",
    "        plt.figure()\n",
    "\n",
    "        # Generate predictor and make predictions\n",
    "        ## Predictor\n",
    "        test_airfoil = gen_test_data(af_u[test_case], unique_AM_pair[j][0], unique_AM_pair[j][1])\n",
    "        input_var.append(test_airfoil)\n",
    "        temp_ind = temp_ind_af[torch.logical_and(temp_arr[:, -4] == unique_AM_pair[j,0], temp_arr[:, -3] == unique_AM_pair[j,1])]\n",
    "        \n",
    "        ## Evaulate Posterior Predictive \n",
    "        sample_airfoil_pred = aggregate_posterior(test_airfoil.to(output_device), weights_list)\n",
    "        output_var.append(sample_airfoil_pred)\n",
    "\n",
    "        # Plot result\n",
    "        ## posterior\n",
    "        (f, ax), (xc_u, xc_l), (mu_u, mu_l), (std_u, std_l) = plot_posterior(test_airfoil[:,-2], sample_airfoil_pred, scale = y_scale, mean = y_mean)\n",
    "        ## experimental validation\n",
    "        temp_plot_x = ((x_data[temp_ind, -2]+1)/2).cpu().detach().numpy()\n",
    "        temp_plot_y = (y_data[temp_ind].cpu()+y_mean)/y_scale\n",
    "        temp_plot_noise = noise[temp_ind]\n",
    "        ax.errorbar(temp_plot_x, temp_plot_y, yerr=2*np.sqrt(temp_plot_noise)/y_scale, fmt='k.', capsize=2, label='Expt., Flemming 1984') #  $C_p \\pm 2\\sigma$\n",
    "        ax.invert_yaxis()\n",
    "        plt.xlabel('x/c')\n",
    "        plt.ylabel('$C_p$')\n",
    "        # plt.legend()\n",
    "        plt.title(targetAF +'\\n' + r'$\\alpha$ = ' + str(np.round(np.rad2deg(unique_AM_pair[j,0]),2)) + r'$^\\circ,$' + r' $M_\\infty = $' + str(unique_AM_pair[j,1]))\n",
    "        if save_figs == True:\n",
    "            plt.savefig(targetAF+'_'+'a'+str(np.round(np.rad2deg(unique_AM_pair[j,0]),2))+'_'+str(unique_AM_pair[j,1])+'.png', bbox_inches='tight')\n",
    "        plt.show() \n",
    "    return input_var, output_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "weights_ = []\n",
    "\n",
    "# If train new \n",
    "if train_new:\n",
    "    weight_sweep_inds = np.lexsort((save_pt_train, save_pt_test, save_pt_mll))\n",
    "    for i in range(0, swa_load_num):\n",
    "        load_str = gen_save_string(save_tag, lr, 0.0, nn_dims, (save_pt_mll[weight_sweep_inds[i]], \n",
    "                                                                        save_pt_train[weight_sweep_inds[i]], \n",
    "                                                                        save_pt_test[weight_sweep_inds[i]]), \n",
    "                                    save_pt_iter[weight_sweep_inds[i]], override_date=None)\n",
    "        weights_.append(load_str)\n",
    "else:\n",
    "    weights_ = ['weights_1', \n",
    "                'weights_2',\n",
    "                'weights_3',] \n",
    "\n",
    "num_figs = 1\n",
    "# 0: NACA 63-415, 1: Supercritical airfoil 9a, 2: SC1095\n",
    "_, _ = validate_predictions(test_x, test_y, test_af, 2, num_figs, weights_list=weights_, save_figs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of data wrt airfoil family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw\n",
    "manual_airfoil_series = ['NACA 4 series', 'NACA 64A Series', 'NASA Supercritical airfoil','Bell family', 'RISO-A family']\n",
    "        # Category vs. Number at varying alpha \n",
    "A_range = np.arange(-16.0, 31.0, 1.0)\n",
    "M_range = np.arange(0.0+0.05, 1.0, 0.1)\n",
    "result = np.zeros((M_range.shape[0], A_range.shape[0]))\n",
    "result_af_family = np.zeros((M_range.shape[0], A_range.shape[0], len(manual_airfoil_series)))\n",
    "results2 = []\n",
    "counter = 0\n",
    "\n",
    "for i in range(0, M_range.shape[0]):\n",
    "    new_ind_M = np.abs(data['M']-M_range[i]) < 0.05\n",
    "    for j in range(0, A_range.shape[0]):\n",
    "        new_ind_A = (data['alpha']-A_range[j] < 0.5) & (data['alpha']-A_range[j] >= -0.5)\n",
    "        new_ind = np.logical_and(new_ind_M, new_ind_A)\n",
    "        result[i, j] = len(data['af'][new_ind].unique())\n",
    "        \n",
    "        # Catergorize for stacked bar plot\n",
    "        if len(data['family'][new_ind]) != 0 :\n",
    "            for ii in range(0, len(manual_airfoil_series)):\n",
    "                if (data['family'][new_ind]).isin([manual_airfoil_series[ii]]).any():\n",
    "                    temp_idx = (data['family'][new_ind]).isin([manual_airfoil_series[ii]])\n",
    "                    result_af_family[i, j, ii] = len(data['af'][new_ind][temp_idx].unique()) \n",
    "        \n",
    "        results2.append([M_range[i], A_range[j], result[i, j]])\n",
    "results2 = np.vstack(results2)\n",
    "# Make figure\n",
    "f = plt.figure(figsize=(24,6))\n",
    "gs = f.add_gridspec(2, 3, width_ratios=(1, 8, 0.65), height_ratios=(1, 2.5),\n",
    "                    left=0.05, right=0.8, bottom=0.1, top=0.9,\n",
    "                    wspace=0.05, hspace=0.05)\n",
    "ax = f.add_subplot(gs[1, 1])\n",
    "ax_histx = f.add_subplot(gs[0, 1], sharex=ax)\n",
    "ax_histy = f.add_subplot(gs[1, 2], sharey=ax)\n",
    "\n",
    "# Plot main\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "# prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors_plt = ['#0072BD', '#D95319', '#EDB120', '#7E2F8E', '#77AC30', '#4DBEEE', '#A2142F']\n",
    "\n",
    "cmap = plt.get_cmap('Reds')\n",
    "colors = cmap(np.linspace(0, 0.7, 256))\n",
    "colors[0, :] = [1,1,1,1]\n",
    "custom_cmap = mcolors.LinearSegmentedColormap.from_list('my_colormap', colors)\n",
    "\n",
    "im = ax.imshow(result, cmap=custom_cmap, aspect='auto', origin='lower')\n",
    "ax.set_yticks(np.arange(0, M_range.shape[0])-0.5, labels=np.round(M_range-0.05, 2))\n",
    "ax.set_xticks(np.arange(0, A_range.shape[0],2), labels=np.round(A_range[::2], 2))\n",
    "\n",
    "for i in range(0, M_range.shape[0]):\n",
    "    for j in range(0, A_range.shape[0]):\n",
    "        text = ax.text(j, i, int(result[i, j]) if not np.isnan(result[i,j]) and result[i,j] != 0 else '',\n",
    "                        ha=\"center\", va=\"center_baseline\", color=\"k\")\n",
    "\n",
    "ax.set_xlabel(r'Angle of attack, $\\alpha$ [deg]', fontsize=16)\n",
    "ax.set_ylabel('Mach number, M', fontsize=16)\n",
    "\n",
    "ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "ax_histx.bar(np.arange(0, A_range.shape[0]), np.sum(result_af_family[:,:,0],axis=0),\n",
    "             label = 'NACA 4-Digit', color=colors_plt[0])\n",
    "ax_histx.bar(np.arange(0, A_range.shape[0]), np.sum(result_af_family[:,:,1],axis=0), bottom=np.sum(result_af_family[:,:,0],axis=0),\n",
    "             label = 'NACA 64A', color=colors_plt[1])\n",
    "ax_histx.bar(np.arange(0, A_range.shape[0]), np.sum(result_af_family[:,:,2],axis=0), bottom=np.sum(result_af_family[:,:,0],axis=0)\n",
    "             +np.sum(result_af_family[:,:,1],axis=0), label='NASA Supercritical airfoil', color=colors_plt[2])\n",
    "ax_histx.bar(np.arange(0, A_range.shape[0]), np.sum(result_af_family[:,:,3],axis=0), bottom=np.sum(result_af_family[:,:,0],axis=0)+np.sum(result_af_family[:,:,1],axis=0)\n",
    "             +np.sum(result_af_family[:,:,2],axis=0), label='DU Series', color=colors_plt[3])\n",
    "ax_histx.bar(np.arange(0, A_range.shape[0]), np.sum(result_af_family[:,:,4],axis=0), bottom=np.sum(result_af_family[:,:,0],axis=0)+np.sum(result_af_family[:,:,1],axis=0)\n",
    "             +np.sum(result_af_family[:,:,2],axis=0)+np.sum(result_af_family[:,:,3],axis=0), label='RISO-A Family', color=colors_plt[4])\n",
    "ax_histx.bar(np.arange(0, A_range.shape[0]), np.sum(result,axis=0),zorder=0, label='Others', color=colors_plt[5])\n",
    "ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result_af_family[:,:,0],axis=1),\n",
    "             label = 'NACA 4-Digit Series', color=colors_plt[0])\n",
    "ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result_af_family[:,:,1],axis=1), left=np.sum(result_af_family[:,:,0],axis=1),\n",
    "             label = 'NACA 64A Series', color=colors_plt[1])\n",
    "ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result_af_family[:,:,2],axis=1), left=np.sum(result_af_family[:,:,0],axis=1)\n",
    "             +np.sum(result_af_family[:,:,1],axis=1), label='NASA Supercritical', color=colors_plt[2])\n",
    "ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result_af_family[:,:,3],axis=1), left=np.sum(result_af_family[:,:,0],axis=1)+np.sum(result_af_family[:,:,1],axis=1)\n",
    "             +np.sum(result_af_family[:,:,2],axis=1), label='Bell', color=colors_plt[3])\n",
    "ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result_af_family[:,:,4],axis=1), left=np.sum(result_af_family[:,:,0],axis=1)+np.sum(result_af_family[:,:,1],axis=1)\n",
    "             +np.sum(result_af_family[:,:,2],axis=1)+np.sum(result_af_family[:,:,3],axis=1), label='RISO-A', color=colors_plt[4])\n",
    "ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result,axis=1),zorder=0, label='Others', color=colors_plt[5])\n",
    "ax_histy.legend(bbox_to_anchor=(-0.35, -0.45), ncol=6)\n",
    "\n",
    "w=1.0\n",
    "h=0.03\n",
    "\n",
    "w1=w*(ax.get_position().x1-ax.get_position().x0)\n",
    "x1=0.135\n",
    "y1=ax.get_position().y0-5*h\n",
    "\n",
    "cax = f.add_axes([x1,y1,w1,h])\n",
    "cbar = plt.colorbar(im, cax=cax, orientation='horizontal', ticks = np.arange(0, 15.1), boundaries=np.arange(0, 15.1))\n",
    "cbar.set_label('Number of airfoils') \n",
    "# plt.subplots_adjust(left=0.15, right=0.95, bottom=0.2, top=0.95)\n",
    "# plt.savefig(\"./Saved Figures/2_avail_data_family.pdf\", dpi=300, bbox_inches='tight') # \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of data wrt airfoil usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw\n",
    "        # Category vs. Number at varying alpha \n",
    "A_range = np.arange(-16.0, 31.0, 1.0)\n",
    "M_range = np.arange(0.0+0.05, 1.0, 0.1)\n",
    "manual_airfoil_type = ['General', 'Rotor', 'Wind Turbine']\n",
    "result = np.zeros((M_range.shape[0], A_range.shape[0]))\n",
    "result_af_type = np.zeros((M_range.shape[0], A_range.shape[0], len(manual_airfoil_type)))\n",
    "results2 = []\n",
    "counter = 0\n",
    "\n",
    "for i in range(0, M_range.shape[0]):\n",
    "    new_ind_M = np.abs(data['M']-M_range[i]) < 0.05\n",
    "    for j in range(0, A_range.shape[0]):\n",
    "        new_ind_A = (data['alpha']-A_range[j] < 0.5) & (data['alpha']-A_range[j] >= -0.5)\n",
    "        new_ind = np.logical_and(new_ind_M, new_ind_A)\n",
    "        result[i, j] = len(data['af'][new_ind].unique())\n",
    "        \n",
    "        # Catergorize for stacked bar plot\n",
    "        if len(data['usage'][new_ind]) != 0 :\n",
    "            for ii in range(0, len(manual_airfoil_type)):\n",
    "                if (data['usage'][new_ind]).isin([manual_airfoil_type[ii]]).any():\n",
    "                    temp_idx = (data['usage'][new_ind]).isin([manual_airfoil_type[ii]])\n",
    "                    result_af_type[i, j, ii] = len(data['af'][new_ind][temp_idx].unique()) \n",
    "        \n",
    "        results2.append([M_range[i], A_range[j], result[i, j]])\n",
    "results2 = np.vstack(results2)\n",
    "# Make figure\n",
    "f = plt.figure(figsize=(24,6))\n",
    "gs = f.add_gridspec(2, 3, width_ratios=(1, 8, 0.65), height_ratios=(1, 2.5),\n",
    "                    left=0.05, right=0.8, bottom=0.1, top=0.9,\n",
    "                    wspace=0.05, hspace=0.05)\n",
    "ax = f.add_subplot(gs[1, 1])\n",
    "ax_histx = f.add_subplot(gs[0, 1], sharex=ax)\n",
    "ax_histy = f.add_subplot(gs[1, 2], sharey=ax)\n",
    "\n",
    "# Plot main\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "# prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "# colors_plt = prop_cycle.by_key()['color']\n",
    "\n",
    "cmap = plt.get_cmap('Reds')\n",
    "colors = cmap(np.linspace(0, 0.7, 256))\n",
    "colors[0, :] = [1, 1, 1, 1,]\n",
    "custom_cmap = mcolors.LinearSegmentedColormap.from_list('my_colormap', colors)\n",
    "im = ax.imshow(result, cmap=custom_cmap, aspect='auto', origin='lower')\n",
    "ax.set_yticks(np.arange(0, M_range.shape[0])-0.5, labels=np.round(M_range-0.05, 2))\n",
    "ax.set_xticks(np.arange(0, A_range.shape[0],2), labels=np.round(A_range[::2], 2))\n",
    "\n",
    "for i in range(0, M_range.shape[0]):\n",
    "    for j in range(0, A_range.shape[0]):\n",
    "        text = ax.text(j, i, int(result[i, j]) if not np.isnan(result[i,j]) and result[i,j] != 0 else '',\n",
    "                        ha=\"center\", va=\"center_baseline\", color=\"k\")\n",
    "\n",
    "ax.set_xlabel(r'Angle of attack, $\\alpha$ [deg]', fontsize=16)\n",
    "ax.set_ylabel('Mach number, M', fontsize=16)\n",
    "\n",
    "ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "ax_histx.bar(np.arange(0, A_range.shape[0]), np.sum(result_af_type[:,:,0],axis=0),\n",
    "             label = 'General', color=colors_plt[0])\n",
    "ax_histx.bar(np.arange(0, A_range.shape[0]), np.sum(result_af_type[:,:,1],axis=0), bottom=np.sum(result_af_type[:,:,0],axis=0),\n",
    "             label = 'Rotor', color=colors_plt[1])\n",
    "ax_histx.bar(np.arange(0, A_range.shape[0]), np.sum(result_af_type[:,:,2],axis=0), bottom=np.sum(result_af_type[:,:,0],axis=0)\n",
    "             +np.sum(result_af_type[:,:,1],axis=0), label='Wind Turbine', color=colors_plt[2])\n",
    "# ax_histx.bar(np.arange(0, A_range.shape[0]), np.sum(result, axis=0), zorder=0, width=1.0)\n",
    "\n",
    "# ax_histx.legend()\n",
    "# ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result,axis=1))\n",
    "ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result_af_type[:,:,0],axis=1),\n",
    "             label = 'General', color=colors_plt[0])\n",
    "ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result_af_type[:,:,1],axis=1), left=np.sum(result_af_type[:,:,0],axis=1),\n",
    "             label = 'Rotor', color=colors_plt[1])\n",
    "ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result_af_type[:,:,2],axis=1), left=np.sum(result_af_type[:,:,0],axis=1)\n",
    "             +np.sum(result_af_type[:,:,1],axis=1), label='Wind Turbine', color=colors_plt[2])\n",
    "\n",
    "# ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result,axis=1), zorder=0, height=1.0)\n",
    "ax_histy.legend(bbox_to_anchor=(-4.0, -0.45), ncol=6)\n",
    "\n",
    "w=1.0\n",
    "h=0.03\n",
    "\n",
    "w1=w*(ax.get_position().x1-ax.get_position().x0)\n",
    "x1=0.137#ax.get_position().x0+w1/2\n",
    "y1=ax.get_position().y0-5*h\n",
    "cax = f.add_axes([x1,y1,w1,h])\n",
    "cbar = plt.colorbar(im, cax=cax, orientation='horizontal', ticks = np.arange(0, 15.1), boundaries=np.arange(0, 15.1))\n",
    "cbar.set_label('Number of airfoils') \n",
    "\n",
    "# plt.savefig(\"./Saved Figures/2_avail_data_usage.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of data wrt airfoil supercriticality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw\n",
    "        # Category vs. Number at varying alpha \n",
    "A_range = np.arange(-16.0, 31.0, 1.0)\n",
    "M_range = np.arange(0.0+0.05, 1.0, 0.1)\n",
    "manual_airfoil_sc = ['-', 'Supercritical']\n",
    "result = np.zeros((M_range.shape[0], A_range.shape[0]))\n",
    "result_af_sc = np.zeros((M_range.shape[0], A_range.shape[0], len(manual_airfoil_sc)))\n",
    "results2 = []\n",
    "counter = 0\n",
    "\n",
    "for i in range(0, M_range.shape[0]):\n",
    "    new_ind_M = np.abs(data['M']-M_range[i]) < 0.05\n",
    "    for j in range(0, A_range.shape[0]):\n",
    "        new_ind_A = (data['alpha']-A_range[j] < 0.5) & (data['alpha']-A_range[j] >= -0.5)\n",
    "        new_ind = np.logical_and(new_ind_M, new_ind_A)\n",
    "        result[i, j] = len(data['af'][new_ind].unique())\n",
    "        \n",
    "        # Catergorize for stacked bar plot\n",
    "        if len(data['supercritical'][new_ind]) != 0 :\n",
    "            for ii in range(0, len(manual_airfoil_sc)):\n",
    "                if (data['supercritical'][new_ind]).isin([manual_airfoil_sc[ii]]).any():\n",
    "                    temp_idx = (data['supercritical'][new_ind]).isin([manual_airfoil_sc[ii]])\n",
    "                    result_af_sc[i, j, ii] = len(data['af'][new_ind][temp_idx].unique()) \n",
    "        \n",
    "        results2.append([M_range[i], A_range[j], result[i, j]])\n",
    "results2 = np.vstack(results2)\n",
    "# Make figure\n",
    "f = plt.figure(figsize=(24,6))\n",
    "gs = f.add_gridspec(2, 3, width_ratios=(1, 8, 0.65), height_ratios=(1, 2.5),\n",
    "                    left=0.05, right=0.8, bottom=0.1, top=0.9,\n",
    "                    wspace=0.05, hspace=0.05)\n",
    "ax = f.add_subplot(gs[1, 1])\n",
    "ax_histx = f.add_subplot(gs[0, 1], sharex=ax)\n",
    "ax_histy = f.add_subplot(gs[1, 2], sharey=ax)\n",
    "\n",
    "# Plot main\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "\n",
    "cmap = plt.get_cmap('Reds')\n",
    "colors = cmap(np.linspace(0, 0.7, 256))\n",
    "colors[0, :] = [1, 1, 1, 1]\n",
    "custom_cmap = mcolors.LinearSegmentedColormap.from_list('my_colormap', colors)\n",
    "im = ax.imshow(result, cmap=custom_cmap, aspect='auto', origin='lower')\n",
    "ax.set_yticks(np.arange(0, M_range.shape[0])-0.5, labels=np.round(M_range-0.05, 2))\n",
    "ax.set_xticks(np.arange(0, A_range.shape[0],2), labels=np.round(A_range[::2], 2))\n",
    "\n",
    "for i in range(0, M_range.shape[0]):\n",
    "    for j in range(0, A_range.shape[0]):\n",
    "        text = ax.text(j, i, int(result[i, j]) if not np.isnan(result[i,j]) and result[i,j] != 0 else '',\n",
    "                        ha=\"center\", va=\"center_baseline\", color=\"k\")\n",
    "\n",
    "ax.set_xlabel(r'Angle of attack, $\\alpha$ [deg]', fontsize=16)\n",
    "ax.set_ylabel('Mach number, M', fontsize=16)\n",
    "\n",
    "ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "ax_histx.bar(np.arange(0, A_range.shape[0]), np.sum(result_af_sc[:,:,0],axis=0),\n",
    "             label = 'General', color=colors_plt[0])\n",
    "ax_histx.bar(np.arange(0, A_range.shape[0]), np.sum(result_af_sc[:,:,1],axis=0), bottom=np.sum(result_af_sc[:,:,0],axis=0),\n",
    "             label = 'Supercritical', color=colors_plt[1]) \n",
    "\n",
    "# ax_histx.legend()\n",
    "# ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result,axis=1))\n",
    "ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result_af_sc[:,:,0],axis=1),\n",
    "             label = 'General', color=colors_plt[0])\n",
    "ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result_af_sc[:,:,1],axis=1), left=np.sum(result_af_sc[:,:,0],axis=1),\n",
    "             label = 'Supercritical', color=colors_plt[1])\n",
    "\n",
    "ax_histy.barh(np.arange(0, M_range.shape[0]), np.sum(result,axis=1),zorder=0)\n",
    "ax_histy.legend(bbox_to_anchor=(-4.7, -0.45), ncol=6)\n",
    "\n",
    "w=1.0\n",
    "h=0.03\n",
    "\n",
    "w1=w*(ax.get_position().x1-ax.get_position().x0)\n",
    "x1=0.1355#ax.get_position().x0+w1/2\n",
    "y1=ax.get_position().y0-5*h\n",
    "\n",
    "cax = f.add_axes([x1,y1,w1,h])\n",
    "cbar = plt.colorbar(im, cax=cax, orientation='horizontal', ticks = np.arange(0, 15.1), boundaries=np.arange(0, 15.1))\n",
    "cbar.set_label('Number of airfoils') \n",
    "# plt.subplots_adjust(left=0.15, right=0.95, bottom=0.2, top=0.95)\n",
    "# plt.savefig(\"./Saved Figures/2_avail_data_supercrit.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust PCA Analysis of Model Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Latent Variables from all unique airfoils (takes some time to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import cm\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "a_sweep_range = np.arange(-4.0, 12.1, 2.0)\n",
    "m_sweep_range = np.array([0.0, 0.3, 0.6, 0.7])\n",
    "x_sweep_range = np.linspace(-1.0, 1.0, 601)\n",
    "\n",
    "af_str_ = af_unique\n",
    "af_arr = []\n",
    "af_type = []\n",
    "latent_vars = []\n",
    "latent_vars_int = []\n",
    "\n",
    "input_vars = []\n",
    "output_cps = []\n",
    "output_cn = []\n",
    "conformal = lambda x: np.sin(np.arccos(x))\n",
    "\n",
    "counter = 0\n",
    "for af_str in af_str_:\n",
    "    for m in m_sweep_range:\n",
    "        for a in a_sweep_range: \n",
    "            test_airfoil = gen_test_data(af_str, np.deg2rad(a), m) #\n",
    "            \n",
    "            xcu = (test_airfoil[0:301, -2].cpu().detach().numpy().flatten()+1)/2\n",
    "            xcl = (test_airfoil[300:601, -2].cpu().detach().numpy().flatten()+1)/2\n",
    "\n",
    "            input_vars.append([a, m])\n",
    "            latent_vars_ = aggregate_latent_vars(test_airfoil.cuda(), weights_).cpu().detach().numpy()\n",
    "            latent_vars.append(latent_vars_)\n",
    "            output_cps_ = aggregate_posterior(test_airfoil.cuda(), weights_).mean.cpu().detach().numpy()\n",
    "            output_cps.append(output_cps_)\n",
    "            af_arr.append(af_str)\n",
    "\n",
    "            latent_vars_int_ = -np.trapz(y=latent_vars_[:301, :], x=xcu, axis=0) + np.trapz(y=np.flip(latent_vars_[300:, :], axis=0), x=np.flip(xcl), axis=0)\n",
    "            latent_vars_int.append(latent_vars_int_)\n",
    "\n",
    "            output_cn_ = -np.trapz(y=output_cps_[:301]*y_scale, x=xcu, axis=0) + np.trapz(y=np.flip(output_cps_[300:]*y_scale, axis=0), x=np.flip(xcl), axis=0)\n",
    "            output_cn.append(output_cn_)\n",
    "            counter += 1\n",
    "\n",
    "input_vars = np.vstack(input_vars)          \n",
    "latent_vars = np.vstack(latent_vars)\n",
    "af_arr = np.vstack(af_arr)\n",
    "output_cps = np.vstack(output_cps)\n",
    "output_cn = np.vstack(output_cn)\n",
    "latent_vars_int = np.vstack(latent_vars_int)\n",
    "\n",
    "import pickle \n",
    "with open('latent_vars.pkl', 'wb') as file:\n",
    "    pickle.dump({'input_vars':input_vars,\n",
    "                 'latent_vars':latent_vars,\n",
    "                 'af_arr': af_arr,\n",
    "                 'output_cps': output_cps,\n",
    "                 'ouput_cn': output_cn,\n",
    "                 'latent_vars_int':latent_vars_int}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted colormap\n",
    "cmap_a = plt.get_cmap('seismic')\n",
    "cmap_m = plt.get_cmap('viridis')\n",
    "norm_a = (a_sweep_range + a_sweep_range[-1])/(2*a_sweep_range[-1])\n",
    "norm_m = (m_sweep_range-m_sweep_range[0])/(m_sweep_range[-1]-m_sweep_range[0])\n",
    "colors_a = cmap_a(norm_a)\n",
    "\n",
    "colors_m = cmap_m(norm_m)\n",
    "colors_a = np.tile(colors_a, (af_unique.shape[0]*len(m_sweep_range), 1)) # random_af_sample.shape[0]\n",
    "colors_m = np.tile(np.repeat(colors_m,len(a_sweep_range), axis=0), (af_unique.shape[0],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vs. Angle of attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_idx = input_vars[:,1]==0.7 # Set M = 0.70\n",
    "m_idx = (input_vars[:,0]==4.0).flatten() & (input_vars[:,1]>0.0).flatten()\n",
    "\n",
    "# Select some airfoils and flow conditions\n",
    "af1_idx = ((af_arr=='NACA 0012').flatten() | (af_arr=='NACA 0011').flatten()) & a_idx & (input_vars[:,0]<10).flatten()\n",
    "af2_idx = ((af_arr=='Supercritical Airfoil 26a').flatten() | (af_arr=='Supercritical Airfoil 12').flatten()) & a_idx & (input_vars[:,0]<10).flatten()\n",
    "af3_idx = ((af_arr=='SSC-A07').flatten() | (af_arr=='SSC-A09').flatten()) & a_idx & (input_vars[:,0]<10).flatten()\n",
    "pca = PCA(n_components=9)\n",
    "\n",
    "rpca = r_pca.R_pca(latent_vars_int - np.mean(latent_vars_int)) # - np.mean(latent_vars, axis=0) # , lmbda=0.0\n",
    "L, S = rpca.fit(max_iter=10000, iter_print=10000, tol=2.0e-2) #  \n",
    "rX_pca = pca.fit_transform(L) #  # 6.5e-4\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Cumulative Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10,6))\n",
    "ax = f.add_axes([0.16, 0.17, 0.68, 0.8])\n",
    "ax.bar(np.arange(1, 10), pca.explained_variance_ratio_, color='b', label = 'Individual Exp. Var.')\n",
    "ax.plot(np.arange(1, 10), pca.explained_variance_ratio_.cumsum(), 'ro-', label = 'Cumulative Exp. Var.',linewidth=2)\n",
    "ax.set_xlabel('Principal Components', fontsize=32)\n",
    "ax.set_ylabel('Explained Variance Ratio', fontsize=32)\n",
    "plt.xticks(fontsize=32)\n",
    "plt.yticks(fontsize=32)\n",
    "plt.xticks(np.arange(1,10))\n",
    "plt.legend(loc='right', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "f = plt.figure(figsize=(16,10))\n",
    "spec = gridspec.GridSpec(ncols=4, nrows=3,\n",
    "                         width_ratios=[4, .01, 2, .1], height_ratios=[1,1,1])\n",
    "ax1 = f.add_subplot(spec[:, 0], projection='3d')\n",
    "ax2 = f.add_subplot(spec[0, 2])\n",
    "ax3 = f.add_subplot(spec[1, 2])\n",
    "ax4 = f.add_subplot(spec[2, 2])\n",
    "cbar_ax = f.add_subplot(spec[:, 3])\n",
    "cbar_ax.axis('off')\n",
    "sort_by_a = np.argsort(input_vars[a_idx][:, 0])\n",
    "colors = colors_a\n",
    "leg_handles = []\n",
    "\n",
    "    # Plot result\n",
    "sct = ax1.scatter(rX_pca[a_idx][sort_by_a, 0], rX_pca[a_idx][sort_by_a, 1], rX_pca[a_idx][sort_by_a, 2], c=colors[a_idx][sort_by_a], marker='o', edgecolors='k', s=80, alpha = 1.0, depthshade=True)\n",
    "\n",
    "ax1.view_init(35, -60)\n",
    "ax1.yaxis.labelpad = 30\n",
    "ax1.zaxis.labelpad = 25\n",
    "ax1.xaxis.labelpad = 20\n",
    "ax1.set_xlabel('PC1', fontsize=32)\n",
    "ax1.set_ylabel('PC2', fontsize=32)\n",
    "ax1.set_zlabel('PC3', fontsize=32)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=24)\n",
    "\n",
    "sort_by_ = np.argsort(rX_pca[a_idx][:, 2])\n",
    "sct = ax2.scatter(rX_pca[a_idx][sort_by_, 0], rX_pca[a_idx][sort_by_, 1], c=colors[a_idx][sort_by_], marker='o', edgecolors='k', s=80)\n",
    "ax2.set_xlabel('PC1', fontsize=24)\n",
    "ax2.set_ylabel('PC2', fontsize=24)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax2.set_xlim([-0.4, 0.55])\n",
    "ax2.set_ylim([-0.3, 0.3])\n",
    "\n",
    "sort_by_ = np.argsort(rX_pca[a_idx][:, 0])\n",
    "sct = ax3.scatter(rX_pca[a_idx][sort_by_, 1], rX_pca[a_idx][sort_by_, 2], c=colors[a_idx][sort_by_], marker='o', edgecolors='k', s=80)\n",
    "ax3.set_xlabel('PC2', fontsize=24)\n",
    "ax3.set_ylabel('PC3', fontsize=24)\n",
    "ax3.set_xlim([-0.3, 0.3])\n",
    "ax3.set_ylim([-0.2, 0.2])\n",
    "ax3.tick_params(axis='both', which='major', labelsize=20)\n",
    "sct.set_cmap('seismic') \n",
    "\n",
    "cbar = plt.colorbar(sct, ax=cbar_ax, ticks=(a_sweep_range+ a_sweep_range[-1])/(2*a_sweep_range[-1]), location='right', fraction=2.5, boundaries=(np.arange(-4.0, 12.1, 0.1)+ a_sweep_range[-1])/(2*a_sweep_range[-1]))\n",
    "cbar.set_label(r'Angle of Attack, $\\alpha$ [deg]', fontsize=24)\n",
    "cbar.set_ticklabels(a_sweep_range)\n",
    "cbar.ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "sort_by_ = np.argsort(rX_pca[a_idx][:, 1])\n",
    "sct = ax4.scatter(rX_pca[a_idx][sort_by_, 0], rX_pca[a_idx][sort_by_, 2], c=colors[a_idx][sort_by_], marker='o', edgecolors='k', s=80) \n",
    "ax4.set_xlabel('PC1', fontsize=24)\n",
    "ax4.set_ylabel('PC3', fontsize=24)\n",
    "ax4.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax4.set_xlim([-0.4, 0.55])\n",
    "ax4.set_ylim([-0.2, 0.2])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./Saved Figures/4_latvar_a2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vs. Mach number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "f = plt.figure(figsize=(16,10))\n",
    "spec = gridspec.GridSpec(ncols=4, nrows=3,\n",
    "                         width_ratios=[4, .1, 2, .1], height_ratios=[1,1,1])\n",
    "ax1 = f.add_subplot(spec[:, 0], projection='3d')\n",
    "ax2 = f.add_subplot(spec[0, 2])\n",
    "ax3 = f.add_subplot(spec[1, 2])\n",
    "ax4 = f.add_subplot(spec[2, 2])\n",
    "cbar_ax = f.add_subplot(spec[:, 3])\n",
    "cbar_ax.axis('off')\n",
    "\n",
    "\n",
    "sort_by_m = np.flip(np.argsort(input_vars[m_idx][:, 1]))\n",
    "\n",
    "colors = colors_m\n",
    "leg_handles = []\n",
    "    # Plot result\n",
    "ax1.view_init(35, -60)\n",
    "sct = ax1.scatter(rX_pca[m_idx][sort_by_m, 0], rX_pca[m_idx][sort_by_m, 1], rX_pca[m_idx][sort_by_m, 2], c=colors[m_idx][sort_by_m], marker='o', edgecolors='k', s=80, depthshade=False)\n",
    "ax1.yaxis.labelpad = 30\n",
    "ax1.zaxis.labelpad = 25\n",
    "ax1.xaxis.labelpad = 20\n",
    "ax1.set_xlabel('PC1', fontsize=32)\n",
    "ax1.set_ylabel('PC2', fontsize=32)\n",
    "ax1.set_zlabel('PC3', fontsize=32)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=24)\n",
    "\n",
    "# \n",
    "\n",
    "sort_by_ = np.argsort(rX_pca[m_idx][:, 2])\n",
    "sct = ax2.scatter(rX_pca[m_idx][sort_by_, 0], rX_pca[m_idx][sort_by_, 1], c=colors[m_idx][sort_by_], marker='o', edgecolors='k', s=80)\n",
    "ax2.set_xlabel('PC1', fontsize=24)\n",
    "ax2.set_ylabel('PC2', fontsize=24)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax2.set_xticks(np.arange(-0.25, 0.25+.1, 0.25))\n",
    "\n",
    "sort_by_ = np.argsort(rX_pca[m_idx][:, 0])\n",
    "sct = ax3.scatter(rX_pca[m_idx][sort_by_, 1], rX_pca[m_idx][sort_by_, 2], c=colors[m_idx][sort_by_], marker='o', edgecolors='k', s=80)\n",
    "ax3.set_xlabel('PC2', fontsize=24)\n",
    "ax3.set_ylabel('PC3', fontsize=24)\n",
    "ax3.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax3.set_ylim([-0.1, 0.2])\n",
    "ax3.set_yticks(np.arange(-0.1, 0.21, 0.1))\n",
    "\n",
    "tick_range = np.arange(0.0, 1.0+0.01, 0.1)\n",
    "cbar = plt.colorbar(sct, ax=cbar_ax, ticks=tick_range, location='right', fraction=2.5)\n",
    "cbar.set_label(r'Mach number, M', fontsize=24)\n",
    "cbar.set_ticklabels(np.round(tick_range*(m_sweep_range[-1]-m_sweep_range[0])+m_sweep_range[0], 2))\n",
    "cbar.ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "sort_by_ = np.argsort(rX_pca[m_idx][:, 1])\n",
    "sct = ax4.scatter(rX_pca[m_idx][sort_by_, 0], rX_pca[m_idx][sort_by_, 2], c=colors[m_idx][sort_by_], marker='o', edgecolors='k', s=80) \n",
    "ax4.set_xlabel('PC1', fontsize=24)\n",
    "ax4.set_ylabel('PC3', fontsize=24)\n",
    "ax4.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax4.set_ylim([-0.1, 0.2])\n",
    "ax4.set_xticks(np.arange(-0.25, 0.25+.1, 0.25))\n",
    "ax4.set_yticks(np.arange(-0.1, 0.21, 0.1))\n",
    "sct.set_cmap('viridis') \n",
    "plt.tight_layout()\n",
    "# plt.savefig('./Saved Figures/4_latvar_m.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aerodynamic Coefficients - A Monte Carlo Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to calculate $C_n$, $C_a$, $C_m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "upper_xc_af = np.flip(np.array(data_raw.values[0,:28], dtype = 'float'))\n",
    "lower_xc_af = np.array(data_raw.values[0, 28:28*2], dtype = 'float')\n",
    "\n",
    "def brute_cn(samples, samples_af):\n",
    "    cn = []\n",
    "    u_bound = samples_af.shape[0]//2+1\n",
    "    l_bound = samples_af.shape[0]//2\n",
    "    samples = samples.detach().numpy()\n",
    "    upper_xc = (samples_af[:u_bound,-2].cpu().detach().numpy()+1)/2\n",
    "    lower_xc = np.flip(samples_af[l_bound:,-2].cpu().detach().numpy()+1)/2\n",
    "    yu = np.interp(x = upper_xc, xp=upper_xc_af, fp=np.flip(samples_af[0,:28].cpu().detach().numpy()/y_scale))\n",
    "    yl = np.interp(x = lower_xc, xp=lower_xc_af         , fp=samples_af[0, 28:28*2].cpu().detach().numpy()/y_scale)\n",
    "\n",
    "    dyudx = np.gradient(yu, upper_xc)\n",
    "    dyldx = np.gradient(yl, lower_xc)\n",
    "    \n",
    "    for i in np.arange(0, samples.shape[0]):\n",
    "        cn.append(-np.trapz(samples[i, :u_bound]/y_scale, x=upper_xc) + np.trapz(np.flip(samples[i, l_bound:])/y_scale, x=lower_xc) ) \n",
    "    return np.mean(cn), np.std(cn)\n",
    "\n",
    "def brute_cm(samples, samples_af, loc=0.25):\n",
    "    cl = []\n",
    "    samples = samples.detach().numpy()\n",
    "    u_bound = samples_af.shape[0]//2+1\n",
    "    l_bound = samples_af.shape[0]//2\n",
    "    upper_xc = (samples_af[:u_bound,-2].cpu().detach().numpy()+1)/2\n",
    "    lower_xc = np.flip(samples_af[l_bound:,-2].cpu().detach().numpy()+1)/2\n",
    "\n",
    "    yucs = scipy.interpolate.CubicSpline(upper_xc_af, np.flip(samples_af[0,:28].cpu().detach().numpy()/y_scale))\n",
    "    yu = yucs(upper_xc)\n",
    "    ylcs = scipy.interpolate.CubicSpline(lower_xc_af, samples_af[0, 28:28*2].cpu().detach().numpy()/y_scale)\n",
    "    yl = ylcs(lower_xc)\n",
    "\n",
    "    dyudx = np.gradient(yu, upper_xc, edge_order=1)\n",
    "    dyldx = np.gradient(yl, lower_xc, edge_order=1)\n",
    "    \n",
    "    # print(dyudx)    \n",
    "    for i in np.arange(0, samples.shape[0]):\n",
    "        term1 = -np.trapz(samples[i, :u_bound]/y_scale*(loc - upper_xc), x=upper_xc) + np.trapz(np.flip(samples[i, l_bound:])/y_scale*(loc - lower_xc), x=lower_xc)\n",
    "        term2 = np.trapz(samples[i, :u_bound]/y_scale * dyudx * yu, x=upper_xc) - np.trapz(samples[i, l_bound:]/y_scale * dyldx * yl, x=lower_xc)\n",
    "        cl.append(term1 + term2)\n",
    "    return np.mean(cl), np.std(cl)\n",
    "\n",
    "def brute_ca(samples, samples_af, manual_coords = None):\n",
    "    ca = []\n",
    "    samples = samples.detach().numpy()\n",
    "    u_bound = samples_af.shape[0]//2+1\n",
    "    l_bound = samples_af.shape[0]//2\n",
    "    upper_xc = (samples_af[:u_bound,-2].cpu().detach().numpy()+1)/2\n",
    "    lower_xc = np.flip(samples_af[l_bound:,-2].cpu().detach().numpy()+1)/2\n",
    "    yucs = scipy.interpolate.CubicSpline(upper_xc_af, np.flip(samples_af[0,:28].cpu().detach().numpy()/y_scale))\n",
    "    yu = yucs(upper_xc)\n",
    "    ylcs = scipy.interpolate.CubicSpline(lower_xc_af, samples_af[0, 28:28*2].cpu().detach().numpy()/y_scale)\n",
    "    yl = ylcs(lower_xc)\n",
    "    if manual_coords is None:\n",
    "        dyudx = np.gradient(yu, upper_xc, edge_order=2)\n",
    "        dyldx = np.gradient(yl, lower_xc, edge_order=2)\n",
    "    else:\n",
    "        dyudx = np.interp(upper_xc, manual_coords[0], np.gradient(manual_coords[1], manual_coords[0], edge_order=1))\n",
    "        dyldx = np.interp(lower_xc, manual_coords[2], np.gradient(manual_coords[3], manual_coords[2], edge_order=1))\n",
    "    for i in np.arange(0, samples.shape[0]):\n",
    "        ca.append(np.trapz(samples[i, :u_bound]/y_scale*dyudx, x=upper_xc) - np.trapz(np.flip(samples[i, l_bound:]/y_scale)*dyldx, x=lower_xc)) \n",
    "    return np.mean(ca), np.std(ca)\n",
    "\n",
    "def brute_cd(samples, samples_af, ang):\n",
    "    cd = []\n",
    "    samples = samples.detach().numpy()\n",
    "    u_bound = samples_af.shape[0]//2+1\n",
    "    l_bound = samples_af.shape[0]//2\n",
    "    upper_xc = (samples_af[:u_bound,-2].cpu().detach().numpy()+1)/2\n",
    "    lower_xc = np.flip(samples_af[l_bound:,-2].cpu().detach().numpy()+1)/2\n",
    "    yucs = scipy.interpolate.CubicSpline(upper_xc_af, np.flip(samples_af[0,:28].cpu().detach().numpy()/y_scale))\n",
    "    yu = yucs(upper_xc)\n",
    "    ylcs = scipy.interpolate.CubicSpline(lower_xc_af, samples_af[0, 28:28*2].cpu().detach().numpy()/y_scale)\n",
    "    yl = ylcs(lower_xc)#np.interp(x = lower_xc,\n",
    "    dyudx = np.gradient(yu, upper_xc, edge_order=2)\n",
    "    dyldx = np.gradient(yl, lower_xc, edge_order=2)\n",
    "    \n",
    "    for i in np.arange(0, samples.shape[0]):\n",
    "        u = np.trapz(y = samples[i, :u_bound]/y_scale*(dyudx - np.tan(ang)), x=upper_xc).flatten()\n",
    "        l = np.trapz(y = samples[i, l_bound:]/y_scale*(dyldx - np.tan(ang)), x=lower_xc).flatten()\n",
    "        cd.append(u - l)\n",
    "    return np.mean(cd), np.std(cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coeffs(airfoil, a, m, num_samples):\n",
    "    from itertools import product \n",
    "    \n",
    "    # Check a and m format\n",
    "    if not (isinstance(a, list) or isinstance(a, np.ndarray)):\n",
    "        a = [a]\n",
    "    if not (isinstance(m, list) or isinstance(m, np.ndarray)):\n",
    "        m = [m]\n",
    "    iter_list = list(product(m, a))\n",
    "    # Set up \n",
    "    CL, CL_2std = [], [] \n",
    "    CD, CD_2std = [], []\n",
    "    CM, CM_2std = [], []\n",
    "\n",
    "    for t in tn.tqdm(range(len(iter_list))):\n",
    "        ang = iter_list[t][1]\n",
    "        mach = iter_list[t][0]\n",
    "        # Set up samples \n",
    "        sample_airfoil = gen_test_data(airfoil, np.deg2rad(ang), mach, manual_point=None)\n",
    "        pred_debug = aggregate_posterior(sample_airfoil.to(output_device), weights_)\n",
    "        pred_debug_samples = pred_debug.sample(sample_shape=torch.Size([num_samples]))\n",
    "        \n",
    "        # Calculate Cn, Ca, Cm\n",
    "        Cn_mu, Cn_std = brute_cn(pred_debug_samples, sample_airfoil) \n",
    "        Ca_mu, Ca_std = brute_ca(pred_debug_samples, sample_airfoil)\n",
    "        Cm_mu, Cm_std = brute_cm(pred_debug_samples, sample_airfoil)\n",
    "        \n",
    "        # Get Cl and std\n",
    "        CL.append(Cn_mu*np.cos(np.deg2rad(ang)) - Ca_mu*np.sin(np.deg2rad(ang)))\n",
    "        # - np.sin(np.deg2rad(ang))*np.tan(np.deg2rad(ang)) this for Eq. from Flemming 1984 (more acc)\n",
    "        CL_2std.append(2*np.sqrt((Cn_std*np.abs(np.cos(np.deg2rad(ang))))**2 + (Ca_std*np.abs(np.sin(np.deg2rad(ang))))**2))\n",
    "        # Get Cd and std\n",
    "        CD.append(Cn_mu*np.sin(np.deg2rad(ang)) + Ca_mu*(np.cos(np.deg2rad(ang))))\n",
    "        # - np.sin(np.deg2rad(ang))*np.tan(np.deg2rad(ang))\n",
    "        CD_2std.append(2*np.sqrt((Cn_std*np.abs(np.sin(np.deg2rad(ang))))**2 + (Ca_std*np.abs(np.cos(np.deg2rad(ang))))**2))\n",
    "        # Get Cm and std\n",
    "        CM.append(Cm_mu)\n",
    "        CM_2std.append(2*Cm_std)\n",
    "\n",
    "    return (np.array(CL), np.array(CL_2std)), (np.array(CD), np.array(CD_2std)), (np.array(CM), np.array(CM_2std)), np.array(iter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: SC1095"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "a_range = np.arange(-4.0, 13.0)\n",
    "# Calculate airfoil info \n",
    "a, b, c, d = calculate_coeffs('SC 1095', a_range, [0.40, 0.60, 0.70], 5000)\n",
    "\n",
    "# M = 0.40 \n",
    "pred_Cl_list_m040, pred_Cl_2std_list_m040 = a[0][d[:,0]==0.4], a[1][d[:,0]==0.4]\n",
    "pred_Cd_list_m040, pred_Cd_2std_list_m040 = b[0][d[:,0]==0.4], b[1][d[:,0]==0.4]\n",
    "pred_Cm_list_m040, pred_Cm_2std_list_m040 = c[0][d[:,0]==0.4], c[1][d[:,0]==0.4]\n",
    "\n",
    "# M = 0.60 \n",
    "pred_Cl_list_m060, pred_Cl_2std_list_m060 = a[0][d[:,0]==0.6], a[1][d[:,0]==0.6]\n",
    "pred_Cd_list_m060, pred_Cd_2std_list_m060 = b[0][d[:,0]==0.6], b[1][d[:,0]==0.6]\n",
    "pred_Cm_list_m060, pred_Cm_2std_list_m060 = c[0][d[:,0]==0.6], c[1][d[:,0]==0.6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expt Data for validation (Brute forcing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sc1095_m060 = np.array([-0.19, -5.30, -3.37, -1.41, -0.30, 3.14, 6.22, 9.17, 10.27, 11.23, 12.24, 13.15, 14.20, 16.15, -0.11, -0.95])\n",
    "a2_sc1095_m060 = np.array([0.03, 3.16, 6.28, 9.31, 11.33, 9.31, 6.28, 3.16, 0.03, -3.11, 0.02]) # run 33\n",
    "cd2_sc1095_m060 = np.array([np.nan, np.nan, 0.0084, 0.0718, 0.1104, 0.0708, 0.0098, np.nan, np.nan, np.nan, np.nan])\n",
    "cl2_sc1095_m060 = np.array([0.12, 0.485, 0.814, 0.937, 0.979, 0.935, 0.811, 0.483, 0.116, -0.286, 0.099])\n",
    "cm2_sc1095_m060 = np.array([-0.019, -0.016, -0.001, -0.012, -0.31, -0.011, 0, -0.015, -0.019, -0.022, -0.019])\n",
    "cl_sc1095_m060 = np.array([0.092, -0.505, -0.300, -0.049, 0.078, 0.489, 0.826, 0.930, 0.966, 0.979, 0.947, 0.951, 0.959, 0.956, 0.09, -0.009])\n",
    "cd_sc1095_m060 = np.array([0.0074, 0.0114, 0.0073, 0.0075, 0.0072, 0.0084, 0.0218, 0.0636, 0.0587, 0.0715, 0.1273, 0.1501, 0.1633, 0.1675, 0.0075, -0.009])\n",
    "cd_sc1095_m060 = np.array([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 0.0316, 0.0822, 0.0931, 0.1129, 0.1322, 0.1491, 0.1713, 0.1976, np.nan, np.nan])\n",
    "cm_sc1095_m060 = np.array([-0.02, -0.029, -0.024, -0.020, -0.020, -0.016, -0.001, -0.013, -0.023, -0.034, -0.048, -0.059, -0.071, -0.092, -0.019, -0.021]) \n",
    "\n",
    "a_sc1095_m040 = np.array([0.05, -5.05, -3.16, -1.24, -0.17, 2.98, 6.13, 9.09, 11.1, 12.06, 12.97]) #  , 13.94, 14.89])\n",
    "cl_sc1095_m040 = np.array([0.110, -.424, -.231, -0.027, 0.092, 0.418, 0.745, 1.041, 1.211, 1.243, 0.874]) #, 0.852, 0.852])\n",
    "cl_sc1095_m040 = np.array([0.132, -0.432, -0.194, -0.003, 0.098, 0.414, 0.759, 1.056, 1.239, 1.293, 0.695]) # Balance\n",
    "cd_sc1095_m040 = np.array([0.0076, 0.0079, 0.0071, 0.0075, 0.0069, 0.008, 0.008, 0.0108, 0.0138, 0.0165, 0.2007])#, 0.2213, 0.228, 0.1042, 0.1264, 0.2253]) \n",
    "cm_sc1095_m040 = np.array([-0.017, -0.02, -0.019, -0.017, -0.017, -0.015, -0.012, -0.005, 0.003, 0.012, -0.107])#, -0.108, -0.0110])\n",
    "a2_sc1095_m040 = np.array([-0.27, -5.01, -3.38, -1.36, -0.32, 2.98, 5.96, 9.03, 11.05, 12.05, 12.85, 13.94]) # Run 16\n",
    "cm2_sc1095_m040 = np.array([-0.01, -0.035, -0.030, -0.022, -0.018, -0.008, -0.002, 0.007, 0.015, 0.01, -0.088, -0.091])\n",
    "cd2_sc1095_m040 = np.array([-0.01, -0.035, -0.030, -0.022, -0.018, -0.008, -0.002, 0.007, 0.015, 0.01, -0.088, -0.091])\n",
    "\n",
    "a_sc1095_m070 = np.array([0.06, -5.17, -3.35, -1.20, -0.17, 2.15, 3.26, 6.31, 8.31, 9.27, 10.26, 11.28])\n",
    "cl_sc1095_m070 = np.array([0.128, -0.595, -.341, -0.063, 0.089, 0.397, 0.55, 0.802, 0.850, 0.883, 0.9, 0.901]) # np.array([0.117, -0.614, -0.347, -0.054, 0.099, 0.421, 0.579, 0.840, 0.852, 0.873, 0.876, 0.870])#\n",
    "cl_sc1095_m070 = np.array([0.117, -0.614, -0.347, -0.054, 0.099, 0.421, 0.579, 0.840, 0.852, 0.873, 0.876, 0.870]) # Balance\n",
    "cd_sc1095_m070 = np.array([0.0074, 0.0335, 0.0143, 0.0077, 0.0074, 0.0087, 0.0144, 0.0529, 0.0699, 0.0729, 0.0778, 0.089])\n",
    "cd_sc1095_m070 = np.array([0.0068, 0.0461, 0.330, 0.0136, 0.0095, np.nan, 0.0145, 0.0513, 0.0785, 0.0955, 0.1094, 0.1201]) # Balance\n",
    "cm_sc1095_m070 = np.array([-0.021, -0.027, -0.029, -0.022, -0.021, -0.014, -0.018, -0.041, -0.051, -0.047, -0.059, -0.067])\n",
    "a2_sc1095_m070 = np.array([-0.32, -5.25, -3.53, -1.34, -0.21, 2.3])\n",
    "cm2_sc1095_m070 = np.array([-0.012, -0.035, -0.024, -0.012, -0.007, 0.004])\n",
    "cd2_sc1095_m070 = np.array([0.0071, 0.0323, 0.0135, .0077, 0.0069, 0.0083])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_range = np.arange(-4.0, 13.0)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(a_sc1095_m040, cl_sc1095_m040, yerr=np.ones_like(cl_sc1095_m040)*0.022*2, fmt='o', color='blue', linewidth=2.0, capsize=2.0, label='M = 0.40, Expt.')\n",
    "plt.errorbar(a_sc1095_m060, cl_sc1095_m060, yerr=np.ones_like(cl_sc1095_m060)*0.007*2, fmt='s', color='orangered', linewidth=2.0, capsize=2.0, label='M = 0.60, Expt.')\n",
    "plt.errorbar(a2_sc1095_m060, cl2_sc1095_m060, yerr=np.ones_like(cl2_sc1095_m060)*0.007*2, fmt='s', color='orangered', linewidth=2.0, capsize=2.0)\n",
    "\n",
    "plt.plot(a_range, pred_Cl_list_m040, label='M = 0.40, LAM', linewidth=2.0)\n",
    "plt.plot(a_range, pred_Cl_list_m060, label='M = 0.60, LAM', linewidth=2.0)\n",
    "plt.fill_between(a_range, pred_Cl_list_m040 - pred_Cl_2std_list_m040, pred_Cl_list_m040 + pred_Cl_2std_list_m040, alpha=0.2)\n",
    "plt.fill_between(a_range, pred_Cl_list_m060 - pred_Cl_2std_list_m060, pred_Cl_list_m060 + pred_Cl_2std_list_m060, alpha=0.2)\n",
    "plt.legend()\n",
    "plt.xlabel(r'$\\alpha$ [deg]', fontsize=36)\n",
    "plt.ylabel('$c_l$', fontsize=36)\n",
    "plt.xticks(fontsize=32)\n",
    "plt.yticks(np.arange(-.75, 1.3, 0.5), fontsize=32)\n",
    "plt.xlim([-4.0, 12.0])\n",
    "plt.ylim([-.75, 1.3])\n",
    "plt.legend(loc='lower right', ncols = 2, columnspacing=0.5, handletextpad=0.2, handlelength=1, fontsize=22)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Saved Figures/4_cl_vs_alpha_sc1095.pdf\", dpi=300, )\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(a_sc1095_m040, cd_sc1095_m040, yerr=np.ones_like(cl_sc1095_m040)*0.017*2, fmt='o', color='blue', linewidth=2.0, capsize=2.0, label='M = 0.40, Expt.')\n",
    "plt.errorbar(a_sc1095_m060, cd_sc1095_m060, yerr=np.ones_like(cl_sc1095_m060)*0.006*2, fmt='s', color='orangered', linewidth=2.0, capsize=2.0, label='M = 0.60, Expt.')\n",
    "plt.errorbar(a2_sc1095_m060, cd2_sc1095_m060, yerr=np.ones_like(cd2_sc1095_m060)*0.006*2, fmt='s', color='orangered', linewidth=2.0, capsize=2.0)\n",
    "plt.plot(a_range, pred_Cd_list_m040, label='M = 0.40, LAM', linewidth=2.0)\n",
    "plt.plot(a_range, pred_Cd_list_m060, label='M = 0.60, LAM', linewidth=2.0)\n",
    "plt.fill_between(a_range, pred_Cd_list_m040 - pred_Cd_2std_list_m040, pred_Cd_list_m040 + pred_Cd_2std_list_m040, alpha=0.2)\n",
    "plt.fill_between(a_range, pred_Cd_list_m060 - pred_Cd_2std_list_m060, pred_Cd_list_m060 + pred_Cd_2std_list_m060, alpha=0.2)\n",
    "plt.legend()\n",
    "plt.xlabel(r'$\\alpha$ [deg]', fontsize=36)\n",
    "plt.ylabel('$c_d$', fontsize=36)\n",
    "plt.legend(loc='upper left', ncols = 2, columnspacing=0.5, handletextpad=0.2, handlelength=1, fontsize=22)\n",
    "plt.xlim([-4.0, 12.0])\n",
    "plt.ylim([-0.05, 0.3])\n",
    "plt.xticks(fontsize=32)\n",
    "plt.yticks(np.arange(-.05, 0.31, 0.05), fontsize=32)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Saved Figures/4_cd_vs_alpha_sc1095.pdf\", dpi=300,)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(a_sc1095_m040, cm_sc1095_m040, yerr=np.ones_like(cm_sc1095_m040)*0.009*2, fmt='o', color='blue', linewidth=2.0, capsize=2.0, label='M = 0.40, Expt.')\n",
    "plt.errorbar(a2_sc1095_m040, cm2_sc1095_m040, yerr=np.ones_like(cm2_sc1095_m040)*0.009*2, fmt='o', color='blue', linewidth=2.0, capsize=2.0)\n",
    "plt.errorbar(a_sc1095_m060, cm_sc1095_m060, yerr=np.ones_like(cm_sc1095_m060)*0.003*2, fmt='s', color='orangered', linewidth=2.0, capsize=2.0, label='M = 0.60, Expt.')\n",
    "plt.errorbar(a2_sc1095_m060, cm2_sc1095_m060, yerr=np.ones_like(cm2_sc1095_m060)*0.003*2, fmt='s', color='orangered', linewidth=2.0, capsize=2.0)\n",
    "plt.plot(a_range, pred_Cm_list_m040, label='M = 0.40, LAM', linewidth=2.0)\n",
    "plt.plot(a_range, pred_Cm_list_m060, label='M = 0.60, LAM', linewidth=2.0)\n",
    "plt.fill_between(a_range, pred_Cm_list_m040 - pred_Cm_2std_list_m040, pred_Cm_list_m040 + pred_Cm_2std_list_m040, alpha=0.2)\n",
    "plt.fill_between(a_range, pred_Cm_list_m060 - pred_Cm_2std_list_m060, pred_Cm_list_m060 + pred_Cm_2std_list_m060, alpha=0.2)\n",
    "plt.ylim([-0.15, 0.05])\n",
    "plt.legend(loc='lower left', ncols = 2, columnspacing=0.5, handletextpad=0.2, handlelength=1, fontsize=22)\n",
    "plt.xlabel(r'$\\alpha$ [deg]', fontsize=36)\n",
    "plt.ylabel('$c_m$', fontsize=36)\n",
    "plt.xlim([-4.0, 12.0])\n",
    "plt.xticks(fontsize=32)\n",
    "plt.yticks(np.arange(-.15, 0.05+.01, 0.05), fontsize=32)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./Saved Figures/4_cm_vs_alpha_sc1095.pdf\", dpi=300,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
